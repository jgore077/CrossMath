A.1	Finding value of $c$ such that the range of the rational function $f(x) = \frac{x^2 + x + c}{x^2 + 2x + c}$ does not contain $[-1, -\frac{1}{3}]$	I am comfortable when I am asked to calculate the range of a rational function, but how do we do the reverse? I came across this problem. If $$f(x)= \frac{x^2 + x + c}{x^2 + 2x + c}$$ then find the value of $c$ for which the range of $f(x)$ does not contain $[-1, -\frac{1}{3}]$.	functions
A.2	Solving differential equations of the form $f'(x)=f(x+1)$	How to solve differential equations of the following form:  $\frac{df}{dx} = f(x+1)$	ordinary-differential-equations
A.3	Approximation to $\sqrt{5}$ correct to an exactitude of $10^{-10}$	I am attempting to resolve the following problem:  Find an approximation to $\sqrt{5}$ correct to an exactitude of $10^{-10}$ using the bisection algorithm.  From what I understand, $\sqrt{5}$ has to be placed in function of $x$ but I am not sure where to go from there. Also, a function in Mathematica are given to do the calculations in which the function $f(x)$, $a$ and $b$ (from the interval $[a, b]$ where $f(a)$ and $f(b)$ have opposite signs), the tolerance and the number of iterations.	numerical-methods,algorithms,bisection
A.4	How to compute this combinatoric sum?	I have the sum $$\sum_{k=0}^{n} \binom{n}{k} k$$ I know the result is $n 2^{n-1}$ but I don't know how you get there. How does one even begin to simplify a sum like this that has binomial coefficients.	combinatorics,number-theory,summation,proof-explanation
A.5	A family has two children. Given that one of the children is a boy, what is the probability that both children are boys?	A family has two children. Given that one of the children is a boy, what is the probability that both children are boys?  I was doing this question using conditional probability formula.  Suppose, (1) is the event, that the first child is a boy, and (2) is the event that the second child is a boy. Then the probability of the second child to be boy given that first child is a boys by formula, $P((2)|(1))=\frac{P((2) \cap (1))}{P((1))}=\frac{P((2))P((1))}{P((1))} = P((2))$ ...since second child to be boy doesn't depend on first child and vice versa. Please provide the detailed solution and correct me if I am wrong.	probability,proof-verification,conditional-probability
A.6	How to calculate mod of number with big exponent	I want to find $$ 5^{133} \mod 8. $$ I have noticed that $5^n \mod 8 = 5$ when $n$ is uneven and 1 otherwise, which would lead me to say that $5^{133} \mod 8 = 5$ But I don't know how to prove this. How can I prove that this is the case (or find another solution if it is not)?	algebra-precalculus,arithmetic
A.7	Finding out the remainder of $\frac{11^\text{10}-1}{100}$ using modulus	If $11^\text{10}-1$ is divided by $100$, then solve for '$x$' of the below term $$11^\text{10}-1 = x \pmod{100}$$  Whatever I tried: $11^\text{2} \equiv 21 \pmod{100}$.....(1) $(11^\text{2})^\text{2} \equiv (21)^\text{2} \pmod{100}$ $11^\text{4} \equiv 441 \pmod{100}$ $11^\text{4} \equiv 41 \pmod{100}$ $(11^\text{4})^\text{2} \equiv (41)^\text{2} \pmod{100}$ $11^\text{8} \equiv 1681 \pmod{100}$ $11^\text{8} \equiv 81 \pmod{100}$ $11^\text{8} × 11^\text{2} \equiv (81×21) \pmod{100}$ ......{from (1)} $11^\text{10} \equiv 1701 \pmod{100} \implies 11^\text{10} \equiv 1 \pmod{100}$ Hence, $11^\text{10} -1 \equiv (1-1) \pmod{100} \implies 11^\text{10} - 1 \equiv 0 \pmod{100}$ and thus we get the value of $x$ and it is $x = 0$ and $11^\text{10}-1$ is divisible by $100$. But this approach take a long time for any competitive exam or any math contest without using calculator. Any easier process on how to determine the remainder of the above problem quickly? That will be very much helpful for me. Thanks in advance.	elementary-number-theory,modular-arithmetic,divisibility,alternative-proof
A.8	finding value of $\lim_{n\rightarrow \infty}\sqrt[n]{\frac{(27)^n(n!)^3}{(3n)!}}$	Finding value of $\lim_{n\rightarrow \infty}\sqrt[n]{\frac{(27)^n(n!)^3}{(3n)!}}$ what i try $\displaystyle l=\lim_{n\rightarrow \infty}\bigg(\frac{(27)^n(n!)^3}{(3n)!}\bigg)^{\frac{1}{n}}$ $\displaystyle \ln(l)=\lim_{n\rightarrow \infty}\frac{1}{n}\bigg[n\ln(27)+3\ln(n!)-\ln((3n)!)\bigg]$ How do i solve it help me please	limits
A.9	Simplifying this series	I need to write the series  $$\sum_{n=0}^N nx^n$$  in a form that does not involve the summation notation, for example $\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$. Does anyone have any idea how to do this? I've attempted multiple ways including using generating functions however no luck	sequences-and-series
A.10	Find the values of a>0 for which the improper integral $\int_{0}^{\infty}\frac{\sin x}{x^{a}} $ converges .	Find the values of a>0 for which the improper integral $\int_{0}^{\infty}\frac{\sin x}{x^{a}} $ converges .  Do  I have to expand integrand using series expansion??	improper-integrals
A.11	What's the cross product in 2 dimensions?	"The math book i'm using states that the cross product for two vectors is defined over $R^3$: $$u = (a,b,c)$$ $$v = (d,e,f)$$ is: $$u \times v = \begin{vmatrix} \hat{i} & \hat{j} & \hat{k} \\ a & b & c \\ d & e & f \\ \end{vmatrix} $$ and the direction of the resultant is determined by curling fingers from vector v to u with thumb pointing in direction of the cross product of u x v.  Out of curiosity, what's the cross product if u and v are defined over $R^2$ instead of $R^3$ instead: $$u = (a,b)$$ $$v = (d,e)$$ Is there a ""degenerate"" case for the cross product of $R^2$ instead $R^3$?  like this is some type of 2x2 determinant instead? for instance if had a parameterization: $$\Phi(u,\ v) = (\ f(u),\ \ g(v)\ )$$ and needed to calculate in $R^2$: $$ D = \Bigg| \frac{\partial{\Phi}}{\partial{u}} \times \frac{\partial{\Phi}}{\partial{v}} \Bigg| $$ There are plenty of examples in the book for calculating the determinate D in $R^3$ but none at all for $R^2$ case. As in: $$ \iint_{V} f(x,y) dx\ dy = \iint_{Q} f(\Phi(u,v) \Bigg| \frac{\partial{\Phi}}{\partial{u}} \times \frac{\partial{\Phi}}{\partial{v}} \Bigg| $$ $$ \Phi(u,v)=(2u \cos v,\ \ u \sin v) $$"	multivariable-calculus,vectors
A.12	Finding the roots of a complex number	I was solving practice problems for my upcoming midterm and however I got stuck with this question type. It is asking me to find all roots and then sketch it. $(1+i\sqrt{3})^{1/2}$ How do we proceed?	linear-algebra,complex-numbers,polar-coordinates
A.13	How to simplify expression $\int_a^b f(x)dx+\int_{f(a)}^{f(b)} f^{-1}(x)dx \ ?$	How to simplify expression $$\int_a^b f(x)dx+\int_{f(a)}^{f(b)} f^{-1}(x)dx \ ?$$ The answer is $bf(b)-af(a)$  but I am wondering how to get the answer.	calculus
A.14	Help solving first-order differential equation	I have first-order differential equation $$y=xy'+ \frac{1}{2}(y')^{2}$$ Maybe, with this someone will find way to solve it $$\frac{1}{2}y'(2x+y')=y$$ I thought I can use $x^2+y=t$ for subtitution and when I derivate, I have $t'=2x+y'\\(t'-2x)t'=2t-2x^2$ which is acctualy the same as previous. I don't have idea how to start..	ordinary-differential-equations
A.15	Derive the sum of $\sum_{i=1}^n ix^{i-1}$	For the series   $$1 + 2x + 3x^2 + 4x^3 + 5x^4 + ... + nx^{n-1}+... $$   and $x \ne 1, |x| < 1$. I need to find partial sums and finally, the sum $S_n$ of series. Here is what I've tried:   We can take a series $S_2 = 1 + x + x^2 + x^3 + x^4 + ...$ so that $\frac{d(S_2)}{dx} = S_1$ (source series). For the $|x| < 1$ the sum of $S_2$ (here is geometric progression): $\frac{1-x^n}{1-x} = \frac{1}{1-x}$ $S_1 = \frac{d(S_2)}{dx} = \frac{d(\frac{1}{1-x})}{dx} = \frac{1}{(1-x)^2}$  But this answer is incorrect. Where is my mistake? Thank you.	sequences-and-series,convergence,summation,power-series
A.16	Finding $ \int_0^1\frac{\ln(1+x)\ln(1-x)}{1+x}dx$	Calculate   $$\int_0^1\frac{\ln(1+x)\ln(1-x)}{1+x}\,dx$$  My try :  Let : $$I(a,b)=\int_0^1\frac{\ln(1-ax)\ln(1+bx)}{1+x}\,dx$$ Then compute $\frac{d^2 I(a,b)}{dadb}$. I'm happy to see ideas in order to kill this integral.	integration,sequences-and-series,definite-integrals,closed-form
A.17	Calculate $\int _{x=0}^{\infty} \frac{\sin(x)}{x}$ with the function $\frac{e^{iz}}{z}$	I want to calculate $\int _{x=0}^{\infty} \frac{\sin(x)}{x}$ with the function $f(z) = \frac{e^{iz}}{z}$. I thought about using the closed path $\Gamma = \gamma _1 + \gamma _R + \gamma _2 + \gamma _{\epsilon}$, when: $\gamma_1 (t) = t, t \in [i\epsilon, iR]$ $\gamma_R (t) = Re^{it}, t \in [-\frac{\pi}{2}, \frac{\pi}{2}]$ $\gamma_2 (t) = t, t \in [-iR, -i\epsilon]$ $\gamma_{\epsilon} (t) = \epsilon e^{it}, t \in [-\frac{\pi}{2}, \frac{\pi}{2}]$ I use the fact that $\frac{\sin(x)}{x}$ is an even function and has an anti derivative, so the integral on a closed path is zero. I managed to show that $\int_{\gamma _{\epsilon}} f = -i\pi$ when $\epsilon \to 0$. However I am struggling to show that $\int_{\gamma _R} f = 0$ when $R \to \infty$ Help would be appreciated	complex-analysis,improper-integrals
A.18	Evaluate $\lim_{n \rightarrow \infty } \frac {[(n+1)(n+2)\cdots(n+n)]^{1/n}}{n}$	Evaluate $$\lim_{n \rightarrow \infty~} \dfrac {[(n+1)(n+2)\cdots(n+n)]^{\dfrac {1}{n}}}{n}$$ using Cesáro-Stolz theorem. I know there are many question like this, but i want to solve it using Cesáro-Stolz method and no others. I took log and applied Cesáro-Stolz, I get $$\log{2}+n\log\cfrac{n}{n+1}$$ Which gives me answer as $\frac{2}{e}$ . But answer is $\frac{4}{e}$. Could someone help?. Edit:  On taking log,  $$\lim_{n \to \infty} \frac{-n\log n + \sum\limits_{k=1}^{n} \log \left(k+n\right)}{n} \\= \lim_{n \to \infty} \left(-(n+1)\log (n+1) + \sum\limits_{k=1}^{n+1} \log \left(k+n\right)\right) - \left(-n\log n + \sum\limits_{k=1}^{n} \log \left(k+n\right)\right) \\ = \lim_{n \to \infty} \log \frac{2n+1}{n+1} - n\log \left(1+\frac{1}{n}\right) = \log 2 - 1$$ Which gives $2/e$	sequences-and-series
A.19	Greatest common factor of $ p^4-1$	I was asked to find the greatest common factor of $p^4-1$ for all primes > 5, First I got the value of $7^4 - 1$ which has divisors of $2^4* 3 *5*2$ and $11^4 - 1$ which has divisors $2^4 *3 * 5*61$ which has a GCF of $2^4*3*5$ I can prove that $p^4 - 1$ is divisible by 3 and 5 by casework and 8  by $(p^2+1)(p-1)(p+1)$ are even integers, but I don't know how to prove divisibility of $2^4$, I do not want to bash it since we must check about 7 numbers to prove its divisibility by assigning $16n + x$ where x <16	divisibility,greatest-common-divisor
A.20	Calculate all $n \in \Bbb N \setminus \{41\}$ such that $\phi(n)=40$?	I'm looking for an $n \in \Bbb N$ for which $\phi(n) = 40$ where $\phi$ is a Euler-Totient Function   I already found one, namely, $n=41$ How the calculate the $n's$?	totient-function
A.21	Finding the last two digits of $9^{9^{9^{…{^9}}}}$ (nine 9s)	I'm continuing on my journey learning about modular arithmetic and got confused with this question: Find the last two digits of $9^{9^{9^{…{^9}}}}$ (nine 9s). The phi function is supposed to be used in this problem and so far this is what I've got: $9^{9^{9^{…{^9}}}} ≡ x (\text{mod } 100)$ Where $0 ≤ x ≤ 100$ $9^{9^{9^{…{^9}}}} \text{ (nine 9s) }= 9^a$ In order to know $9^a (\text{mod } 100)$, we need to know $a (\text{mod } \phi(100))$ As $\phi(100)= 40$, we get $a = b (\text{mod } 40)$ $9^{9^{9^{…{^9}}}} \text{ (eight 9s) }= 9^b$ In order to know $9^b (\text{mod } 40)$, we need to know $b (\text{mod } \phi(40))$ As $\phi(40)= 16$, we get $b = c ( \text{mod }16)$ $9^{9^{9^{…{^9}}}}\text{ (seven 9s) }= 9^c $ In order to know $9^c (mod 16)$, we need to know $c (\text{mod } phi(16))$ as $\phi(16)= 8 $ we need to find $c (\text{mod } 8)$ As $9 = 1 (\text{mod } 8)$ $c = 1 (\text{mod } 8)$ I feel like I might have made a mistake somewhere along the way because I'm having a lot of trouble stitching it all back together in order to get a value for the last two digits. Could anyone please help me with this? Thank you!	number-theory,modular-arithmetic
A.22	Find number of d's that satisfies $d, d+1, d+2... = N$ for an $N$	I have a challenge about a cat in a trip where he can walk in the way of $d, d+1, d+2...$ and the sum of that should give $N$, given an $N$, how many ways of chosing $d$ are posible? Example: $N=30$ -> $Ans=3$ $d_1=4; d_2=6; d_3=8$  For $d_1: 4 + 5 + 6 + 7 + 8 = 30$ Edit: Another way to see it is: How many subsets in the sumation up to N are posible in the way $(\sum (d+n) - \sum(d-1))$=N	sequences-and-series,number-theory
A.23	How do i find the lcm	Qn: If the product of two integers is  $2^7 \cdot 3^8 \cdot 5^2 \cdot 7^{11}$ and their greatest common divisor is $2^3 \cdot 3^4 \cdot 5$, what is their least common multiple? I have issue with this question please help me solve it. I tried assuming that lcm is $x$ =. Then,        Gcd $\cdot x = 2^3  \cdot 3^4 \cdot 5x$. And, product factors /Gcd $x$	prime-numbers,greatest-common-divisor,least-common-multiple
A.24	Is this the only way to evaluate $\sqrt{2i-1}?$	work out the $\sqrt{2i-1}?$ $2i-1=(a+bi)^2$ $a^2+2abi-b^2$ $a^2-b^2=-1$ $2ab=2$  $a^2=b^{-2}$ $b^{-2}-b^2=-1$ $-b^{4}+1=-1$ $b^4=2$ $b=\sqrt[4]{2}$ Can we solve $\sqrt{2i-1}$ in another way?	algebra-precalculus
A.25	What can be P(0), when $P(x^2+1)=(P(x))^2+1$ and P(x) is polynomial?	What can be $P(0)$, when $P(x^2+1)=(P(x))^2+1$ and $P(x)$ is polynomial? Let $P(0)=0$, then $P(1)=1$, $P(2)=2$, $P(5)=5$, $P(26)=26$, $P(677)=677$ ... and so on. Then $P(x)=x$, because all the points on $y=P(x)$ are $y=x$. If $P(0)=2$, then $P(x)=(x^2+1)^2+1$ for the same reason. But when $P(0)=3$, we have that $\lim_{x→∞}\log_xP(x)$ does not converge into an integer. So I think $P(x)$ cannot be a polynomial. Then what are the values of $P(0)$ that makes $P(x)$ polynomial?	polynomials
A.26	How to solve an indefinite integral using the Taylor series?	I am trying to show that the following integral is convergent but not absolutely.  $$\int_0^\infty\frac{\sin x}{x}dx.$$  My attempt:  I first obtained the taylor series of $\int_0^x\frac{sin x}{x}dx$ which is as follows:   $$x-\frac{x^3}{3 \times 3!}+\frac{x^5}{5\times5!}-\frac{x^7}{7 \times 7!}+\cdots = \sum_{n=0}^\infty (-1)^n\frac{x^{(2n+1)}}{(2n+1) \times (2n+1)!} $$  Now $\int_0^\infty\frac{\sin x}{x}dx=\lim_{x\to \infty} \sum_{n=0}^\infty (-1)^n\frac{x^{(2n+1)}}{(2n+1) \times (2n+1)!}$ and I got stuck here! What is the next step?	real-analysis,calculus,integration,taylor-expansion,riemann-integration
A.27	What is the value of $e^{3i \pi /2}$?	When solving for the value, we know that $e^{\pi i}=-1$ . I am confused as to what is the right answer when you evaluate this.I am getting two possible answers: $e^{3\pi i/2}$ = $(e^{\pi i})^{3/2}$ so this could be $(\sqrt{-1})^3=i^3=-i$ or it could be $\sqrt{(-1)^3}=\sqrt{-1}=i$. Which one is the correct answer, and where am I going wrong? Thanks.	complex-numbers,exponentiation
A.28	If $\sin(18^\circ)=\frac{a + \sqrt{b}}{c}$, then what is $a+b+c$?	If $\sin(18)=\frac{a + \sqrt{b}}{c}$ in the simplest form, then what is $a+b+c$? $$ $$ Attempt: $\sin(18)$ in a right triangle with sides $x$ (in front of corner with angle $18$ degrees), $y$, and hypotenuse $z$, is actually just $\frac{x}{z}$, then $x = a + \sqrt{b}, z = c$. We can find $y$ as $$ y = \sqrt{c^{2}- (a + \sqrt{b})^{2}} $$ so we have  $$ \cos(18) = \frac{y}{z} = \frac{\sqrt{c^{2}- (a + \sqrt{b})^{2}}}{c}$$ I also found out that $$b = (c \sin(18) - a)^{2} = c^{2} \sin^{2}(18) - 2ac \sin(18) + a^{2}$$ I got no clue after this.  The solution says that $$ \sin(18) = \frac{-1 + \sqrt{5}}{4} $$ I gotta intuition that we must find $A,B,C$ such that $$ A \sin(18)^{2} + B \sin(18) + C = 0 $$  then $\sin(18)$ is a root iof $Ax^{2} + Bx + C$, and $a = -B, b = B^{2} - 4AC, c = 2A$.  Totally different. This question is not asking to prove that $sin(18)=(-1+\sqrt{5})/4$, that is just part of the solution.	algebra-precalculus,trigonometry,euclidean-geometry,contest-math
A.29	Dividing Complex Numbers by Infinity	My PreCalculus teacher recently reviewed the properties of limits with us before our test and stated that any real number divided by infinity equals zero. This got me thinking and I asked them whether a complex number (i.e. $3+2i$ or $-4i$) divided by infinity would equal zero.  This completely stumped them and I was unable to get an answer. After doing some theoretical calculation, knowing that $i=\sqrt{-1}$, I calculated that a complex number such as $\frac{5i}{\infty}=0$ since  $$\frac{5}{\infty}\cdot \frac{\sqrt{-1}}{\infty} = 0\cdot 0 = 0,$$  using properties utilized with real numbers that would state that $\frac{5x}{\infty} = 0$ since $$\frac{5}{\infty}\cdot \frac{x}{\infty} = 0\cdot 0 = 0.$$ Is this theoretical calculation correct or is there more to the concept than this?	algebra-precalculus,limits,complex-numbers,infinity
A.30	Find $a^3+b^3+c^3-3abc$ (binomial theorem)	$$a=\sum_{n=0}^\infty\frac{x^{3n}}{(3n)!}\\b=\sum_{n=1}^\infty\frac{x^{3n-2}}{(3n-2)!}\\c=\sum_{n=1}^\infty\frac{x^{3n-1}}{(3n-1)!}$$Find $a^3+b^3+c^3-3abc$: $(a)\ 1$ $(b)\ 0$ $(c)-1$ $(d)-2$  Please help me solve this question. I added $a,b$ and $c$. It gives me the expansion of $e^x$. But i dont know how to use it.	binomial-theorem
A.32	Are definitions axioms?	"I just want to ask a very elementary question. When we introduce a ""definition"" in a first order logical system. For example when we say  Define: $Empty(x) \iff \not \exists y (y \in x) $  Isn't that definition itself an ""axiom"", call it a definitional axiom. I'm asking this because the one place predicate symbol Empty() is actually new, it is not among the listed primitives of say Zermelo, which has only identity and membership as primitive symbols.  So when we are stating definitions are we in effect stating axioms? but instead of being about characterizing a primitive, they are definitional axioms giving a complete reference to a specified set of symbols in the system. Is that correct? Now if that is the case, then why we don't call it axiom when we state it, I mean why we don't say for example: Definitional axiom 1) $Empty(x) \iff \not \exists y (y \in x)$ Zuhair"	terminology,definition,first-order-logic,axioms
A.33	Physical meaning and significance of third derivative of a function	Given a physical quantity represented by a function $f(t,x)$ what is (if there is any) the actual meaning of the third derivative of $f$, $\frac{\partial^3 f}{\partial t^3}$ or $\frac{\partial^3 f}{\partial x^3}$	physics
A.34	Extending Knuth up-arrow/hyperoperations to non-positive values	"So... I had the silly idea to extend Knuth's up-arrow notation so that it included zero and negative arrows. It is normally defined as $$\begin{align*} a \uparrow b & = a^b \\ a \uparrow^n b & = \underbrace{a \uparrow^{n - 1} (a \uparrow^{n - 1} (\dots(a \uparrow^{n - 1} a) \dots ))}_{b\text{ copies of } a} \end{align*}$$ so, basically the hyperoperation sequence starting from exponentiation. For now, I will only consider $a,b > 0$. If we try to go backwards from $a \uparrow b$, the ""trivial"" extension (letting down arrows represent negative up arrows, because why the heck not) is: $$\begin{align*} a \;b & = a \cdot b \\ a \downarrow b & = a + b \\ a \downarrow \downarrow b & = \text{see below} \end{align*} \\ \vdots$$ But I had trouble coming up with an expression for $a \downarrow \downarrow \downarrow b$. Maybe it doesn't exist. Alternatively, maybe there is a way of defining $a \; b$ (zero arrows) such that it does exist. So my question is: Is there an extension of Knuth's up-arrow notation such that $a \downarrow^n b$ exists for all $n \geq 3$?  Edit: Welp, I messed this question up. I initially thought $a \downarrow \downarrow b = a + 1$ was correct, but it is actually $b + 1$. So I thought I had an example of an extension when I did not. I have modified the question accordingly.  An extension would define $a \uparrow^n b$ for each $n \leq 0$ which satisfies the recursive definition of the notation.  Edit 2: Okay, turns out $a \downarrow \downarrow b = b + 1$ isn't correct either, as this would imply $a \downarrow b = a + b - 1$. For example, $4 \downarrow 3 = 4 \downarrow \downarrow (4 \downarrow \downarrow 4) = 4 \downarrow \downarrow (4 + 1) = (4 + 1) + 1 = 6 = 4 + 3 - 1$. But it is really close; perhaps we need an exception, such as $$\begin{align*} a \downarrow \downarrow b = \begin{cases}b + 1 & \text{if } a < b \\ b + 2 & \text{if } a = b \end{cases}\end{align*}.$$ The case $a > b$ does not show up when evaluating $a \downarrow b$, but it will be need to be defined if we try to extend further to $a \downarrow \downarrow \downarrow b$. For instance, we could abuse the fact that the case $a > b$ is allowed to be anything, and let $$a \downarrow \downarrow b = b + 1 + \left\lfloor \frac{a}{b} \right\rfloor,$$ but finding $a \downarrow \downarrow \downarrow b$ may be intractable as a result."	hyperoperation,ackermann-function
A.35	When does a function NOT have an antiderivative?	I know this question may sound naïve but why can't we write $\int e^{x^2} dx$ as $\int e^{2x} dx$? The former does not have an antiderivative, while the latter has. In light of this question, what are sufficient conditions for a function NOT to have an antiderivative. That is, do we need careful examination of a function to say it does not have an antiderivative or is there any way that once you see the function, you can right away say it does not have an antiderivative?	integration
A.36	Proof by contradiction, status of initial assumption after the proof is complete.	"First of all I'd like to say that I have looked for the answers to my specific question and have not found it in the existing topics. The question is fairly simple. Say, we need to  prove statement P by the method of contradiction. Assuming that $\lnot P$ holds, using the list of statements proven earlier to hold or derived by us during the proof, we arrive to P being $true$. $$\lnot P  \to A_1 \to\ ... \ \to A_n \to P$$ $$\lnot P  \to  P \iff \lnot(\lnot P) \lor P \iff P $$ We can therefore add P to the list of our proven statements, because it was derived. Most of the proofs contain something in the lines of ""the obtained contradiction proves that our initial assumption ($\lnot P$) was wrong and so $P$ holds"". What I don't understand is, if the initial assumption ($\lnot P$) is thus proven to be false, then why can we be sure that anything derived from it holds (in particular, that P holds)? On the other hand, if it cannot be derived then the assumption ($\lnot P$) can in fact be true.  Can someone explain why this type of argument cannot be used?"	logic,proof-writing
A.37	Non trivial examples of $f\circ g = g \circ f$ but $f^{-1} \neq g$ and $f\neq\mathrm{id}\neq g$.	"Are there real-valued functions $f$ and $g$ which are neither each other's inverses, the identity, nor linear, yet exhibit the behaviour $$f\circ g = g \circ f?$$ Examples such as $f(x) = 2x$ and $g(x)=3x$ are ""trivial"" in this sense.  Moreover, given a function $f$, can one go about obtaining an example of a function $g$ which commutes with $f$?  I suppose this would be similar to fixed point iteration? E.g. if $f\colon\mathbb R\smallsetminus\{1\}\to\mathbb R$ is defined by $f(x) = 2x/(1-x)$, I would need a function $g$ such that  $$g(x) = f^{-1}\circ g\circ f =\frac{g(\frac{2x}{1-x})}{2+g(\frac{2x}{1-x})},$$ so maybe choosing an appropriate ""starting function"" $g_0$ and finding a fixed point of $g_{n+1} \mapsto f^{-1}\circ g_n\circ f$ would be a possible strategy, but I can't seem to find a suitable $g_0$."	real-analysis,functional-analysis,functions
A.38	Uses of Axiom of Choice	I am a first-year maths student but I occasionally drift away from our taught material. Some years ago I saw the ZFC axioms for the first time, but now that I am in college, and although the stuff I've been taught so far is nowhere near ZFC (in terms of difficulty), it happened to me that we use the axiom of choice all the time in every module, even if we don't know it by name yet. For example, in the proof that, for every non-negative integers $a, b$, there exist integers $q, r: a = bq + r$ (with the known restrictions on r), and the proof starts like this: $Choose$ the largest integer $q : qb <= a$... blah blah blah. Is it the axiom of choice that allows us to execute this simple yet so important step?  And a couple more questions: Can you name some other simple proofs, theorems, results etc for which the axiom of choice is essential? Also, I've read that AOC has long been a topic of dispute for mathematicians, and that even today, some people do not accept it. Are there any alternative axiomatic systems that work equally well without needing AOC? Thanks!	set-theory
A.39	How to know which value is bigger?	Which is bigger between $2018^{2019}$ or $\ 2019^{2018}\ $? When taking logs of both sides and  I get:  $2019\log(2018)\ $ and $\ 2018 \log(2019)$ I know $\log 2019\gt \log 2018$ so does this mean that $2019^{2018}$ is the biggest one? And did I do it properly?	algebra-precalculus,logarithms
A.40	"What is the meaning of the term ""linear"""	"$a_1x_1+a_2x_2+a_3x_3+...+a_nx_n=$ is called a linear equation because it represents the equation of a line in an n dimensional space. So ""linear"" comes from the word ""line"".Basically there should not be any higher power of x failing which the graph of the function will not be a straight line. simillarly $a(x)y+b(x)y'+c(x)y""+d(x)y'''+...+q(x)=0$ is also called linear differential equation because all the derivatives have power equal to 1 which is similar to the above definition of a linear equation. A function f is called linear if: $f(x+y)=f(x)+f(y)$ and $f(cx)=cf(x)$. Here c is a constant. In this definition of linearity of function ""$f$"" what does the word linear means? How does it relate to a straight line? Finally what does the term linear means in case of linear vector spaces? Where is the reference to a straight line? So, whether linear is just a word used in different contexts? Does it have different meaning in different situation? Or linearity refers to some relation to a straight line? At Least please explain how the linearity of function f and linear vector space relate to the equation of a line."	linear-algebra
A.41	Confusion in how to find number of onto functions if two sets are given	In the book it is given if A and B are two finite sets containing $m$ and $n$ elements, respectively, then the number of onto functions from A to B will be if  $n \leq m$  $$\sum_{r=1}^n (-1)^{(n-r)} {n \choose r}(r)^m $$ well I can't understand it and I am aware with combinations.	combinatorics,functions,combinations
A.42	What is a simple, physical situation where complex numbers emerge naturally?	I'm trying to teach middle schoolers about the emergence of complex numbers and I want to motivate this organically. By this, I mean some sort of real world problem that people were trying to solve that led them to realize that we needed to extend the real numbers to the complex.  For instance, the Greeks were forced to recognize irrational numbers not for pure mathematical reasons, but because the length of the diagonal of a square with unit length really is irrational, and this is the kind of geometrical situation they were already dealing with. What similar situation would lead to complex numbers in terms that kids could appreciate? I could just say, try to solve the equation $x^2 + 1 = 0$, but that's not something from the physical world. I could also give an abstract sort of answer, like that $\sqrt{-1}$ is just an object that we define to have certain properties that turn out to be consistent and important, but I think that won't be entirely satisfying to kids either.	complex-numbers,physics,applications
A.43	Prove $\sum_{n\geq1}\frac1{n^2+1}=\frac{\pi\coth\pi-1}2$	I am trying to prove  $$\sum_{n\geq1}\frac1{n^2+1}=\frac{\pi\coth\pi-1}2$$ Letting $$S=\sum_{n\geq1}\frac1{n^2+1}$$ we recall the Fourier series for the exponential function $$e^x=\frac{\sinh\pi}\pi+\frac{2\sinh\pi}\pi\sum_{n\geq1}\frac{(-1)^n}{n^2+1}(\cos nx-n\sin nx)$$ Plugging in $x=\pi$ $$e^\pi=\frac{\sinh\pi}\pi+\frac{2\sinh\pi}\pi\sum_{n\geq1}\frac{(-1)^n}{n^2+1}(\cos n\pi-n\sin n\pi)$$ $$e^\pi=\frac{\sinh\pi}\pi+\frac{2\sinh\pi}\pi\sum_{n\geq1}\frac{(-1)^n}{n^2+1}((-1)^n-n\cdot0)$$ $$e^\pi=\frac{\sinh\pi}\pi+\frac{2\sinh\pi}\pi S$$ $$S=\frac{\pi e^\pi}{2\sinh\pi}-\frac12$$ But that is nowhere near to correct. What did I do wrong, and how do can I prove the identity? Thanks.	real-analysis,sequences-and-series
A.44	For $A,B \in \mathscr{M}_{2\times2}(\mathbb{Q}) $ of finite order, show that $AB$ has infinite order	Let $G$ be the group $ ( \mathscr{M}_{2\times2}(\mathbb{Q}) , \times ) $ of nonsingular matrices. Let $ A = \left ( \begin{matrix}  0 & -1 \\   1 & 0  \end{matrix} \right ) $, the order of $A$ is $4$; Let $ B = \left ( \begin{matrix}  0 & 1 \\   -1 & -1  \end{matrix} \right ) $, the order of $B$ is $3$. Show that $AB$ has infinite order.  The only reasoning possible here is by contradiction as $G$ is not abelian. And so I tried, but I got stuck before any concrete development. Any hints are welcome, Thanks.	matrices,group-theory,cyclic-groups
A.45	How to prove that {$\sin(x) , \sin(2x) , \sin(3x) ,...,\sin(nx)$} is independent in $\mathbb{R}$?	Prove that {$\sin(x) , \sin(2x) , \sin(3x) ,...,\sin(nx)$} is independent in $\mathbb{R}$  my trial : we know that the Wronsekian shouldn't be $0$ to get the trivial solution and thus they are independent. its not trivial to show that $ W \not = 0$ W =  $\begin{vmatrix} (1)\sin(x) & (1)\sin(2x) & (1)\sin(3x) &  ... &   (1)\sin(nx) \\ (1)\cos(x) & (2)\cos(2x) & (3)\cos(3x) &  ... &   (n)\cos(nx) \\  -(1)^2\sin(x) & -(2)^2\sin(2x) & -(3)^2\sin(3x) &  ... &   -(n)^2\sin(nx) \\ -(1)^3\cos(x) & -(2)^3\cos(2x) & -(3)^3\cos(3x) &  ... &   -(n)^3\cos(nx) \\ \end{vmatrix}$ and so on. it looks like Vandermonde matrix but i cant prove that and so we conclude that its $W\not =0$	ordinary-differential-equations
A.46	Suppose $f$ is a Lebesgue integrable function on$[0,1]$ which satisfies $\int x^k f(x) dx=0$ , prove $f=0 \text{ a.e.}$	Suppose $f$ is an indefinitely differentiable real valued function on$[0,1]$ which satisfies $\int_0^1 x^k f(x)\, dx=0$ for $k=\{0,1,2,3,.. .\}$, prove $f=0$ . My attempt : To prove this assertion , it suffice to prove $$\int_0^1 f^2 (x) \,dx=0$$  Then by approximation theorem , we can find a polynomial such that for every $\epsilon \gt 0$ $$\int_0^1 f^2 (x) \,dx= \int_0^1 (f(x)-\sum_{n=0}^N a_n x^n)f(x) ,dx+\int_0^1 \sum_{n=0}^N f(x) a_n x^n\,dx \le \epsilon M$$ where $M=\sup_{x \in [0,1]}|f(x)|$ . It seems that we do not need the condition that $f$ is indefinitely differentiable , if $f$ is continuous then the conclution may hold . My question : a) Suppose $f$ is a Lebesgue integrable real valued function on$[0,1]$ which satisfies $\int_0^1 x^k f(x)\ dx=0$ for $k=\{0,1,2,3,.. .\}$, can we prove $f=0$ except on a set of measure $0$ ?      b) If a) is not true , suppose $f$ is a Riemann integrable real valued function on$[0,1]$ which satisfies $\int_0^1 x^k f(x)\ dx=0$ for $k=\{0,1,2,3,.. .\}$, can we prove $f=0$ except on a set of measure $0$ ?   EDIT: To prove $f=0 \text{  a.e.}$ , it suffice to prove all the fourier coefficient of $f$ equal to $0$ , then  $$\int_0^1 f(x) \cos(2 \pi nx) \,dx \le \int_0^1 |f(x)||\cos(2 \pi nx)-\sum_{n=0}^{N}a_n x^n| \le \epsilon||f||_{L^1}$$ and the proof is complete.	real-analysis
A.47	Prove that for a given prime $p$ and each $0 < r < p-1$, there exists a $q$ such that $rq \equiv 1 \bmod p$	Prove that for a given prime $p$ and each $0 < r < p-1$, there exists a $q$ such that  $$rq \equiv 1 \bmod p$$ I've only taken one intro number theory course (years ago), and this just popped up in a computer science class (homework). I was assuming that this proof would be elementary since my current class in an algorithm cours, but after the few basic attempts I've tried it didn't look promising. Here's a couple approaches I thought of:  (reverse engineer) To arrive at the conclusion we would need $$rq - 1 = kp$$ for some $k$. A little manipulation: $$qr - kp = 1$$ That looks familiar, but I can't see anything from it.  (sum on $r$) $$\sum_{r=1}^{p-2} r = \frac{(p-2)(p-1)}{2} = p\frac{p - 3}{2} + 1 \equiv 1 \bmod p$$ which looks good but I don't know how to incorporate $r$ int0 the final equality.    (Wilson's Theorem—proved by Lagrange)   I vaguely recall this theorem, but I was looking at it in an old book and it wasn't easy to see how we arrived there. Anyways, $p$ is prime iff $$(p-1)! \equiv -1 \bmod p$$ Here the $r$ multiplier is built in to the factorial expression so I was thinking of adding $2$ to either side $$(p-1)! + 2 \equiv 1 \bmod p$$ which is a dead end (pretty sure). But then I was thinking, maybe multiplying Wilson't Thm by $(p+1)$? Then getting $$(p+1)(p-1)! = -(p+1) \bmod p$$ which I think results in $$(p+1)(p-1)! = 1 \bmod p$$ of which $r$ is a multiple and $q$ is obvious. But I'm not sure if that's valid.	elementary-number-theory,proof-verification
A.48	Hints for showing that if $x,y \geq 0$, then $(x+y)^k \geq x^k + y^k$ for all $k \geq 1$	I'm trying to show the following:   If $x,y \geq 0$, then $(x+y)^k \geq x^k + y^k$ for all $k \in \mathbb{R_{\geq 1}}$   So, far I've tried a few things, but nothing seems to stick. Clearly $x \leq x+ y$ and since both sides of the inequality are positive, the inequality $x^k \leq (x + y)^k$ will hold for $k \geq 1$. Similarly, $y^k \leq (x+y)^k$. Adding both inequalities together, we obtain: $x^k + y^k \leq 2(x+y)^k$. While this is close, of course, it is not what we want to show. Alternatively, I was thinking that if we fix $x,y \geq 0$, let $f(k) = (x+y)^k$, and let $g(k) = x^k + y^k$, we can show using the binomial theorem that $(x+y)^r \geq x^r + y^r$ for all $r \in \mathbb{Z^+}$. Then, if we can show both $f$ and $g$ intersect only when $k =1 $, we might have better luck proving the statement since then we would have $(x+y)^r > x^r + y^r$ for all positive integers $r \geq 2$. A real number $m \notin \mathbb{Z}$ with the property that $f(m) = g(m)$ could not then exist since then that would violate the fact that $k=1$ is the only intersection. We could then invoke the continuity of $f$ and $g$ together with the fact that $(x+y)^r > x^r + y^r$ for all positive integers $r \geq 2$ to obtain our result. Of course, all of this is dependent on rigorously showing that $f$ and $g$ intersect only when $k=1$. Otherwise, I'm running low on ideas. Any hints would be greatly appreciated.	real-analysis,functions,inequality
A.49	Is there a simple combinatoric interpretation of this identity?	I came across an exercise in which we are asked to prove the identity: $${2n\choose n}=\sum_{k=0}^n{n\choose k}^2$$ The exercise gives the hint: $$\left(1+x\right)^{2n}=\left[(1+x)^n\right]^2$$ It's not too difficult to use the hint to prove the identity (the expressions in the identity are the coefficients of $x^n$ in the respective expansions of the expressions in the hint, which of course must be the same number), but I was wondering whether there is a neater equivalent-counting interpretation... It's clear that ${2n \choose n}$ is the number of ways in which we can choose half the elements in a set (where this is possible): how can we interpret $\sum_{k=0}^n{n\choose k}^2$ equivalently?	combinatorics,binomial-coefficients
A.50	Divergent series $\sum{\frac{1}{n^{2+\cos{n}}}}$	Bonjour.  Show that  $$\sum{\frac{1}{n^{2+\cos{n}}}}$$ is a divergent serie. $$\\$$ My main problem is: If $\epsilon$ is “infinitely small positive real number” define $A_{\epsilon}$ as the set of all $n, |2+\cos n|\leq 1+\epsilon$ $(n \in A_{\epsilon}\iff-1\leq \cos n \leq -1+\epsilon)$. The divergence should come from the sum over $A_{\epsilon}$ but I have no idea to how to handle this. $$\\$$	real-analysis,integration,sequences-and-series,analysis
A.51	Sum of series having binomial coefficients	Prove that $\displaystyle \sum_{r=0}^n {n+r\choose r} \frac{1}{2^{r}}= 2^{n}$ what i try $$\binom{n}{n}+\binom{n+1}{1}\frac{1}{2}+\binom{n+2}{2}\frac{1}{2^2}+\binom{n+3}{3}\frac{1}{2^3}+\cdots +\binom{n+n}{n}\frac{1}{2^n}$$ $$\binom{n}{n}+\binom{n+1}{n}\frac{1}{2}+\binom{n+2}{n}\frac{1}{2^2}+\binom{n+3}{n}\frac{1}{2^3}+\cdots +\binom{n+n}{n}\frac{1}{2^n}.$$ coefficient of $x^n$ in  $$(1+x)^n+(1+x)^{n+1}\frac{1}{2}+(1+x)^{n+2}\frac{1}{2^2}+\cdots\cdots +(1+x)^{2n}\frac{1}{2^n}.$$ How do i solve ithelp me plesse	binomial-coefficients
A.52	Prove $\forall n\in\mathbb{N}$, $\exists m\in\mathbb{N}$ s.t. $m>n$ and $m$ is prime	There are two parts I am having trouble getting started. A. Prove that $n_1, n_2,...,n_k\in\mathbb{N}$ are each at least $2$ then $n=n_1n_2...n_k+1$ is not divisible by any numbers $n_1, n_2,...,n_k$.  B. Prove that the truth of the negation leads to a contradiction. (Use theorem: For all $a,b\in\mathbb{N}$ there exist a unique quotient $q$ and remainder $r$ in $\mathbb{Z^+}$ such that we have both $a=qb+r$ and $0\leq r<q$.) For part A, I started with, given $k\in\mathbb{N}$ and $n_1, n_2,...,n_k\geq1$, I'll show that $\forall i$, $n_i \nmid n=n_1n_2...n_k+1$ to set it up, but I'm not sure how to actually go about starting it. For part B, I know that the negation is $\exists n\in\mathbb{N}$ s.t. $\forall m\in\mathbb{N}$ either $m\leq n$ or $m$ is not prime, but again I'm not sure what I should do to start the proof or exactly how to incorporate that theorem.	proof-writing,prime-numbers
A.53	Show that one-sided inverse of a square matrix is a true inverse	We know that for a group element $g\in G$, $gh=1$ does not necessarily mean that $hg = 1$. In the case for matrices (linear maps between vector spaces), it is also true that $AB = 1 \nRightarrow BA = 1$. This happens when the $A$ and $B$ are not square matrices (in which case they do not even form a group under multiplication). However if we restrict the square matrices, $AB = 1 \Rightarrow BA = 1$. What is simple proof of this that avoids chasing the entries, and makes use simply the vector space structure of linear transformations? (In fact if we could prove this, I think this might imply that for a group to have one-sided(but not two-sided) inverses, it has to be infinite, since every finite group admits a finite dimensional representation.	linear-algebra,group-theory
A.54	By using a diagonal argument, show that the powerset $P(N) = (S|S ⊆ N)$ is uncountable.	Any tips or solutions for this one? By using a diagonal argument, show that the powerset $P(N) = (S|S ⊆ N)$ is uncountable.	discrete-mathematics,elementary-set-theory
A.55	$\frac{1}{\sqrt{-1}}=\sqrt{-1}$?	I have trouble to comprehend what my mistake is in the following calculation: If we set $\sqrt{-1}$ to be the new number with the property that $(\sqrt{-1})^2 = -1$ then I can write $$\frac{1}{\sqrt{-1}}=\sqrt{\frac{1}{-1}}=\sqrt{-1}.$$ But we also have (and I know this is the correct result) $$ \frac{1}{\sqrt{-1}}\cdot\frac{\sqrt{-1}}{\sqrt{-1}}=\frac{\sqrt{-1}}{-1}=-\sqrt{-1}$$ What am I missing? Thanks.	complex-numbers,definition
A.56	A curious logical formula involving prime numbers	Let $S$ be a nonempty set of natural numbers. Is the following formula $$ \exists p\ \bigl(\text{$p$ is prime } \rightarrow \forall x  \text{ ($x$ is prime)}\bigr)  $$ true or false on $S$? I know the answer to this question, but what would be the shortest way to arrive to the conclusion using some deduction system?	logic,first-order-logic
A.57	Preimage of continuous one-to-one function on connected domain is not continuous.	I know that given $B$, a compact subset of $\mathbb{R}^n$, and $f : B \to \mathbb{R}^m$, a continuous injective (one-to-one) function, $f^{-1}$ is continuous on $f(B)$. (This true). I also know that image $f(X)$ of a connected subset $X$ is connected under a continuous function. Now let $X$ be a connected (non-compact) subset of $\mathbb{R}^n$, and $f : X \to \mathbb{R}^m$ be a continuous injective (one-to-one) function. I am trying (and struggling) to provide a counterexample in which mapping $f^{-1} : f(X) \mapsto X$ is not continuous on $f(X)$. (A rigorous, parametrized example). Thank you in advance!	real-analysis,general-topology,analysis,continuity,metric-spaces
A.58	Prove that $3\arcsin \frac{1}{4} + \arccos \frac {11}{16} = \frac {\pi}{2}$	Can someone help me with this exercise? I honestly don't know where to start and how to prove it. You don't have to answer it fully, just give me a hint or something. Thank you in advance.  Exercise 1. Prove that  $3\arcsin \frac{1}{4} + \arccos \frac {11}{16} = \frac {\pi}{2}$  Thanks.	trigonometry
A.59	Multiple proofs of $\sum_{d|n}{\phi(d)}=n$	I am looking for multiple proofs of that statement: here $\phi(n)$ denotes the Euler’s totient  $$\sum_{d|n}{\phi(d)}=n$$  Here’s one:  By unique factorisation theorem: $n=\prod_{k=1}^{m}{p_k^{\alpha_k}}$ and $d=\prod_{k=1}^{m}{p_k^{\beta_k}}$ where $0\leq \beta_k\leq \alpha_k$ so:  $\begin{align} \sum_{d|n}{\phi(d)}&=\sum_{0\leq \beta_k\leq \alpha_k}{\phi\left(\prod_{k=1}^{m}{p_k^{\beta_k}}\right)}\\ &= \sum_{0\leq \beta_k\leq \alpha_k}{\prod_{k=1}^{m}\phi({p_k^{\beta_k})}}\\ &=\sum_{0\leq \beta_k\leq \alpha_k}{\prod_{k=1}^{m}{(p_k^{\beta_k}-p_k^{\beta_k-1}})}\\ &=\prod_{k=1}^{m}{\sum_{0\leq \beta_k\leq \alpha_k}{(p_k^{\beta_k}-p_k^{\beta_k-1}}})\\ &= \prod_{k=1}^{m}{p_k^{\alpha_k}}\\ &=n. \end{align}$	group-theory,number-theory,alternative-proof,big-list
A.60	Limiting value of a sequence when n tends to infinity	Q) Let, $a_{n} \;=\; \left ( 1-\frac{1}{\sqrt{2}} \right ) ... \left ( 1- \frac{1}{\sqrt{n+1}} \right )$ , $n \geq 1$. Then $\lim_{n\rightarrow \infty } a_{n}$ (A) equals $1$ (B) does not exist (C) equals $\frac{1}{\sqrt{\pi }}$ (D) equals $0$  My Approach :- I am not getting a particular direction or any procedure to simplify $a_{n}$ and find its value when n tends to infinity. So, I tried like this simple way to substitute values and trying to find the limiting value :- $\left ( 1-\frac{1}{\sqrt{1+1}} \right ) * \left ( 1-\frac{1}{\sqrt{2+1}} \right )*\left ( 1-\frac{1}{\sqrt{3+1}} \right )*\left ( 1-\frac{1}{\sqrt{4+1}} \right )*\left ( 1-\frac{1}{\sqrt{5+1}} \right )*\left ( 1-\frac{1}{\sqrt{6+1}} \right )*\left ( 1-\frac{1}{\sqrt{7+1}} \right )*\left ( 1-\frac{1}{\sqrt{8+1}} \right )*.........*\left ( 1-\frac{1}{\sqrt{n+1}}    \right )$  =$(0.293)*(0.423)*(0.5)*(0.553)*(0.622)*(0.647)*(0.667)* ....$ =0.009*... So, here value is tending to zero. I think option $(D)$ is correct. I have tried like this $\left ( \frac{\sqrt{2}-1}{\sqrt{2}} \right )*\left ( \frac{\sqrt{3}-1}{\sqrt{3}} \right )*\left ( \frac{\sqrt{4}-1}{\sqrt{4}} \right )*.......\left ( \frac{\sqrt{(n+1)}-1}{\sqrt{n+1}} \right )$ = $\left ( \frac{(\sqrt{2}-1)*(\sqrt{3}-1)*(\sqrt{4}-1)*.......*(\sqrt{n+1}-1)}{{\sqrt{(n+1)!}}} \right )$ Now, again I stuck how to simplify further and find the value for which $a_{n}$ converges when $n$ tends to infinity . Please help if there is any procedure to solve this question.	calculus,sequences-and-series,limits,products
A.61	There exists $i, j \in \mathbb{N}$ such that $n=3i+5j$ for $n\ge 8$	Prove that there exists $i, j \in \mathbb{N}$ such that $n=3i+5j$ for $n\ge 8$  I'm having a hard time with this exercise, I'm trying to prove it by induction: Basis step:  $n=8 \implies 8=3\cdot1+5\cdot 1$ $n=9 \implies 9=3\cdot3+5\cdot0$ $n=10 \implies 10=3\cdot0+5\cdot2$  Induction step: If it's true for $n=h$ then it must be true for $n=h+1$. So now, I don't know how to begin proving that $k+1=3i+5j$.	elementary-number-theory,discrete-mathematics,induction,diophantine-equations
A.62	Prove that the cardinality of the set of rational numbers and the set of integers is equal	I just learned about cardinality in my discrete class a few days ago, and this is in the homework. This is all fairly confusing to me, and I'm not entirely sure where to even start. Here's the full question: Let $\mathbb{Q}$ denote the set of rational numbers and $\mathbb{Z}$ denote the set of integers. Prove that $|\mathbb{Q}| = |\mathbb{Z}|$. I thought about saying that every element in $\mathbb{Q}$ can be written as some element in $\mathbb{Z} \times \mathbb{Z}$, but I still don't know how to prove that that is a bijection, or even how to prove that $|\mathbb{Z} \times \mathbb{Z}| = |\mathbb{Z}|$. Any help would be greatly appreciated.	discrete-mathematics
A.63	$\gcd$ and $\text{lcm}$ of more than $2$ positive integers	For any two positive integers ${n_1,n_2}$, the relationship between their greatest common divisor and their least common multiple is given by $$\text{lcm}(n_1,n_2)=\frac{n_1 n_2}{\gcd(n_1,n_2)}$$ If I have a set of $r$ positive integers ${n_1,n_2,n_3,...,n_r}$, does the same relationship hold? Is it true that $$\text{lcm}(n_1,n_2,n_3,...,n_r)=\frac{\prod_{i=1}^r n_i}{gcd(n_1,n2,n_3,...,n_r)}$$ I feel like this should be easy to prove, but I'm struggling to get a handle on it.	proof-explanation,greatest-common-divisor,least-common-multiple
A.64	Suppose that f : [a, b] → R is continuous and that f([a, b]) ⊂ [a, b]. Prove that there exists a point c ∈ [a, b] satisfying f(c) = c.	Suppose that $f : [a, b] \to \mathbb{R}$ is continuous and that $f([a, b]) \subset [a, b]$. Prove that there exists a point $c \in [a, b]$ satisfying $f(c) = c$.  (If either $f(a) = a$ or $f(b) = b$ there is nothing left to show, so you might as well assume that $f(a) = a$ and $f(b) = b$. Since $f$ takes its values in $[a, b]$ this is the same as assuming that $f(a) > a$ and $f(b) < b$.) So far, I have: Pf. Assume $f(a)>a$ and $f(b)< b$.  Let $x, y \in [a,b]$ such that $f(a)=x$ and $f(b)=y$ which means $f[a,b]=[x,y]$. Notice $[x,y]\subset [a, b]$. Since f is continuous on $[x,y]$, there exists some $c \in [a,b]$ such that $x$ is less than or equal to $c$ is less than or equal to $y$... This is where I am stuck because I don't think I can just assume by Intermediate Value Theorem that some $f(c)=c$?	real-analysis,continuity,proof-explanation
A.65	How can we show that $e^{-2\lambda t}\lambda^2\le\frac1{e^2t^2}$ for all $\lambda,t\ge0$?	How can we show that $$e^{-2\lambda t}\lambda^2\le\frac1{e^2t^2}\tag1$$ for all $\lambda,t\ge0$? Applying $\ln$ to both sides yields that $(1)$ should be equivalent to $$t\lambda\le e^{t\lambda-1}\tag2.$$ So, if I did no mistake, it should suffice to show $x\le e^{x-1}$ for all $x\ge0$. How can we do this?	calculus,inequality,exponential-function
A.66	if $x,h \in \mathbb{R}^d$ and $A \in \mathbb{R}^{d\times d}$ is it possible to justify that $(x^TAh)^T = h^TA^Tx$?	if $x,h \in \mathbb{R}^d$ and $A \in \mathbb{R}^{d\times d}$ is it possible to justify that $(x^TAh)^T = h^TA^Tx$?	linear-algebra,transpose
A.67	Combination of matrixes	If A is a $k\times k$ matrix,B is a $k\times l$ matrix and C is a $l\times l$ matrix prove that: $\det{\begin{bmatrix}A&B\\O&C\end{bmatrix}}=\det(A)\det(C)$ O is the matrix that all it's elements are equal to zero. I know some rules for calculating determinants but I don't know how to begin in this question.	calculus,determinant
A.68	Prove $a^n+1$ is divisible by $a + 1$ if $n$ is odd	Prove $a^n+1$ is divisible by $a + 1$ if $n$ is odd: We know $a$ cannot be $-1$ and the $n \in \mathbb{N}$. Since $n$ must be odd, we can rewrite $n$ as $2k+1$. Now we assume it holds for prove that it holds for the next term. $$a^{2(k+1)+1}+1$$ $$=a^{2k+3}+1$$ $$=a^3\cdot a^{2k}+1$$ $$=(a^3+1)\cdot a^{2k} -a^{2k}+1$$ Im not sure on what to do next. Since $a^{2k}$ means that the exponential term will be even and thus you cant use the fact that $a^n+1$ is divisible by $a + 1$ if $n$ is odd.	polynomials,induction,divisibility
A.69	Induction with two variable parameters	"So I was assigned this homework problem: $$\ {s \choose s} + {s+1 \choose s} +...+ {n \choose s} = {n+1 \choose s+1}$$ for all s and all $n \geq s$ I've tried to email both my professor and my TA and their explanations seem contradictory. My professor responded saying the statement I need to prove is ""The formula is correct for $0 \leq s \leq n$."" Whereas my TA told me I need to use induction on both variables and I'm not sure how to do that. Any help is appreciated!"	combinatorics
A.70	Proving $\sum_{j=0}^{N-1}\cos\frac{\left(2j+1\right)\pi}{2N}=0$	Let $l\in\mathbb{Z}$ and $N\in\mathbb{N}$. I need to prove the following: $\begin{equation} \sum_{j=0}^{N-1}\cos\left(l\frac{\left(2j+1\right)\pi}{2N} \right)=0 \end{equation}$ I tried to use Euler formula and then sum the first $N$ terms of the geometric serie I get, but it didn't work. Any ideas?	trigonometry,summation
A.71	Show, with induction that $1^2 + 2^2 + .... + n^2 = \frac{n(n+1)(2n+1)}{6}$	Show, with induction that $1^2 + 2^2 + .... + n^2 = \frac{n(n+1)(2n+1)}{6}$ My attempt Case 1: n = 1 $LHS = 1^2$  $RHS = \frac{(1+1)(2+1)}{6} = \frac{2*3}{6} = 1$ Case 2: n = p $LHS_{p} = 1^2 + 2^2 + ... + p^2$ $RHS_{p} = \frac{p(p+1)(2p+1)}{6}$ Case 3: n = p + 1 $LHS_{p+1} = 1^2+2^2+....+p^2+(p+1)^2$ $RHS_{p+1} = \frac{(p+1)((p+1)+1)(2(p+1)+1)}{6}$ Now to show this with induction I think i need to show that $RHS_{p+1} = RHS_{p} + (p+1)^2$ $RHS_{p+1} = \frac{p(p+1)(2p+1)}{6} + (p+1)^2$ So I need to rewrite  $RHS_{p+1} = \frac{(p+1)((p+1)+1)(2(p+1)+1)}{6} $to be equal to $\frac{p(p+1)(2p+1)}{6} + (p+1)^2$  Anyone see how I can do that? Or got any other solution?	induction
A.72	Is it possible that $\mathcal{X} = \mathcal{Y}$, yet $\mathcal{X} \in \mathcal{Y}$?	Is it possible for a set to equal another set, yet the former set be an element in the latter set?  I.e.: $\mathcal{X} = \mathcal{Y}$, yet $\mathcal{X} \in \mathcal{Y}$	set-theory,axioms
A.73	Help on proof of $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2 = \binom{2n}{n}$	The proof is required to be made through the binomial theorem. I will expose the demonstration I was tought, and forward my questions after exposing it. You'll see question marks like this one (?-n) on points I don't quite understand, where $n$ is the numeration of the mark. This are the doubts I have about the demonstration, the which I hope someone can clarify. Prove that $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2 = \binom{2n}{n}$. We will use the following equality, and call it $P$:  $(1+x)^n(1+x)^n=(1+x)^{2n}$ (?-1) The result will be proved finding the $x^n$ coefficient of both terms of this equality (?-2). According to the binomial theorem, the left-hand side of this equation is the product of two factors, both equal to $\binom{n}{0}1+\binom{n}{1}x+...+\binom{n}{r}x^r+...+\binom{n}{n}x^n$ When both factors multiply, a term on $x^n$ is obtained when a term of the first factor has some $x^i$ and the term of the second factor has some $x^{n-i}$. Therefor the coefficients of $x^n$ are $\binom{n}{0}\binom{n}{n}+\binom{n}{1}\binom{n}{n-1}+\binom{n}{2}\binom{n}{n-2}+...\binom{n}{n}\binom{n}{0}$ . Since $\binom{n}{n-r}=\binom{n}{r}$, the previous summation is equal to $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2$. So the left hand side of the equation we are asked to proove is a coefficient of $x^n$. When we expand the right-hand side of the equation $P$, we find that $\binom{2n}{n}$ is a coefficient of $x^n$. Therefore (?-3) the left-hand side of the equation we were asked to prove is in deed equal to $\binom{2n}{n}$. In conclussion, $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2 = \binom{2n}{n}$. This was all the demonstration. My doubt one (?-1) goes about where the heck does this equation come from? How would I know what equation to come up with if requested to prove a different equality? Doubt two (?-2) goes about why would the solution of the first equation would have anything to do with finding the $x^n$ coefficients of the one I just made up (see doubt one). Doubt three (?-3) goes about why demonstrating that $a$ is a coefficient of $x^n$ on the left hand side of the equation I made up, and that $b$ is a coefficient of $x^n$ on the right-hand side of this equation as well, would prove my original equation, the one I was supposed to prove on the first place? I know there are many doubts here, I hope you guys can help me. Sorry for the long post, it's a long demonstration.	discrete-mathematics,binomial-coefficients,binomial-theorem
A.74	Show that the image of the function $f:(0,\infty)\rightarrow \mathbb{R}$, $f(x)=x+\dfrac{1}{x}$ is the interval $[2,\infty)$.	Show that the image of the function $f:(0,\infty)\rightarrow \mathbb{R}$, $f(x)=x+\dfrac{1}{x}$ is the interval $[2,\infty)$.  If $x=1$, then $f(1)=2$. So how can I show that the mage of the function is the interval $[2,\infty)$?	functions,elementary-set-theory
A.75	Prove that for each integer $m$, $ \lim_{u\to \infty} \frac{u^m}{e^u} = 0 $	I'm unsure how to show  that for each integer $m$, $ \lim_{u\to \infty} \frac{u^m}{e^u} = 0 $.   Looking at the solutions it starts with $e^u$ $>$ $\frac{u^{m+1}}{(m+1)!}$ but not sure how this is a logical step.	real-analysis,calculus,limits
A.76	Covering $\mathbb{Z}$ by arithmetic progressions	I am solving problems from an old exam (in topology, but I've translated the problem into more algebraic terms). The problem is the following:  Let $a+b\mathbb{Z}=\{z\in \mathbb{Z}\mid z = a+bk \text{ for some  }k\in \mathbb{Z}\}$ where $a\in \mathbb{Z}$ and $b\in  \mathbb{Z}-\{0\}$. Suppose we have a collection of such sets   $\{a_i+b_i\mathbb{Z}\mid i \in \mathbb{N}\}$   satisfying: $$\bigcup_{i \in \mathbb{N}}(a_i+b_i\mathbb{Z})=\mathbb{Z}$$ Show whether it is always possible to extract a finite   $I\subset \mathbb{N}$ s.t. $$\bigcup_{i \in I}(a_i+b_i\mathbb{Z})=\mathbb{Z}$$  Unfortunately, I seem to have forgotten a lot of my elementary algebra... Nevertheless, I have attempted something: Let $\{p_k\}=\{2,3,5,\dots\}$ be the set of primes. We can construct: $$\left(\bigcup_{k\in \mathbb{N}}(0+p_k\mathbb{Z})\right)\cup (-1+\ell_1 \mathbb{Z})\cup (1+\ell_2\mathbb{Z})=\mathbb{Z}$$ for some appropriate non-negative integers $\ell_1,\ell_2$. We could for instance pick $\ell_1=\ell_2=5$. Suppose there is a finite sub-collection $\{0+p_{k_j}\}$, $j=1,\dots,n$ s.t.  $$\left(\bigcup_{1\leq j\leq n}(0+p_{k_j}\mathbb{Z})\right)\cup (-1+5 \mathbb{Z})\cup (1+5\mathbb{Z})$$ Now, assume $p$ is some prime s.t. $p>\max\{p_{k_1},\dots,p_{k_n}\}$, then clearly $p\notin \bigcup_{1\leq j\leq n}(0+p_{k_j}\mathbb{Z})$. But here I run into a problem. I want $p\notin(-1+5 \mathbb{Z})\cup (1+5\mathbb{Z})$. That is, I want $5\nmid p-1$ and $5\nmid p+1$. This is of course possible if $p$ is a prime with a $7$ as its last digit. However, this approach means I have to prove that there are infinitely many primes ending on a $7$, which seems like a silly thing to prove for a simple problem like this. Surely, there is a nicer way of solving this? EDIT: I am particularily interested in a solution not relying on topology, and whether a solution like my attempted solution works.	general-topology,elementary-number-theory
A.77	Show that the relation $(- 1) (- 1) = 1$ is a consequence of the distributive law	"Show that the relation $(- 1) (- 1) =  1$ is a consequence of the distributive law.  This question is the first problem from 'Number Theory for Beginners"" by Andre Weil. I cannot get the point from where to begin. I tried using $1\cdot 1 = 1$ and $ 1\cdot x = x $, but couldn't get somewhere. Can you help me just with a hint? I would be willing to work up from there."	elementary-number-theory
A.79	Inequality with complex exponential	Rudin in Real and Complex Analysis uses this in a proof near the beginning of chapter 9:  $\displaystyle \left \vert{ \frac {e^{-ixu}-1}{u}}\right\vert \le \vert x \vert$ for all real $u \ne 0$  Why is this true? Edit: I believe $x$ is real	inequality,exponential-function,fourier-transform
A.80	Why does this proof that the set of all finite subsets of N is a countable set not work for the set of all subsets of N?	"I found this proof in a StackExchange thread and found it pretty understandable and simple: ""The other answers give some sort of formula, like you were trying to do.  But, the simplest way to see that the set of all finite subsets of $\mathbb{N}$ is countable is probably the following. If you can list out the elements of a set, with one coming first, then the next, and so on, then that shows the set is countable.  There is an easy pattern to see here.  Just start out with the least elements. $$\emptyset, \{1\}, \{2\}, \{1, 2\}, \{3\}, \{1, 3\}, \{2, 3\}, \{1, 2, 3\}, \{4\}, \ldots$$ In other words, first comes $\{1\}$, then comes $\{2\}$.  Each time you introduce a new integer, $n$, list all the subsets of $[n] = \{1, 2, \ldots, n\}$ that contain $n$ (the ones that don't contain $n$ have already showed up).  Therefore, all subsets of $[n]$ show up in the first $2^{n}$ elements of this sequence."" I understand how it applies for finite subsets of N, but I cant really pinpoint of why it would not apply to a set of all subsets of N. We could continue this scheme for ever, couldnt we?  I assume that I think in a wrong way about infinity but I am not quite sure. Any help is greatly appreciated!"	analysis,elementary-set-theory,proof-explanation
A.81	Any infinite set contains a countable subset. Why is my proof wrong? (Axiom of Choice)	Let $M$ be an infinite set.    Proposition 1:   For any $n \in \mathbb{N}$, there exists an injection from $\{1, \cdots, n\}$ to $M$.      (1) Since $M \neq \emptyset$, there exists $x \in M$. Define $f(1)$ as $f(1) := x$. $f$ is an injection from $\{1\}$ to $M$. (2) Suppose that there exists an injection $f$ from $\{1, \cdots, n\}$ to $M$. Since $M$ is an infinite set, $M - \{f(1), \cdots, f(n)\}$ is not an empty set. So, there exists $x \in M - \{f(1), \cdots, f(n)\}$. Define $g(1), \cdots, g(n+1)$ as $g(1) := f(1), \cdots, g(n):=f(n)$ and $g(n+1) := x$. Obviously, $g$ is an injection from $\{1, \cdots, n+1\}$ to $M$.   Let $n_1$ be an arbitrary natural number. If I wanna calculate $h(n_1)$, then I get an injection $g$ from $\{1, \cdots, n_1\}$ to $M$ by Proposition 1. And I return $g(n_1)$ as the value of $h(n_1)$. And I store the pairs $(1, g(1)), \cdots, (n_1, g(n_1))$ to my database.   If I wanna calculate $h(n_2)$ for $n_2 \leq n_1$, then I search my database and I get the value $g(n_2)$ from my database and I return $g(n_2)$ as the value of $h(n_2)$. If I wanna calculate $h(n_3)$ for $n_3 > n_1$, then I add the pairs $(n_1 + 1, g(n_1+1)), \cdots, (n_3, g(n_3))$ to my database by Proposition 1 and I return $g(n_3)$ as the value of $h(n_3)$.   I can calculate $h(n)$ for any $n \in \mathbb{N}$.   From above, we get an injection $h : \mathbb{N} \to M$.   Why is my proof wrong? By the way. Suppose that a man wanna know if I have an injection $h : \mathbb{N} \to M$ or not. Then how can the man know if I have  an injection $h : \mathbb{N} \to M$ or not?	elementary-set-theory,axiom-of-choice
A.82	All definite integrals evaluate to 0 using periodic functions.	I know that my reasoning is incorrect, I just don't know where I went wrong. I did discuss this with my Maths teacher, and even she could not find what I did wrong. Let us begin by assuming a function, $f(x)$ that is continuous and has an antiderivative in the interval $[0, 2\pi]$. Let $A$ be the area under the curve for $f(x)$ in the interval $[0, 2\pi]$ $A = \displaystyle \int_{0}^{2\pi}{f(x)\space\mathrm{d}x}$ Now there must exist a function, $g(x)$ such that:   $f(x) = g(x)\cdot \cos(x)$ Substituting the value of $f(x)$: $A = \displaystyle\int_{0}^{2\pi}{g(x)\cdot \cos(x)\space\mathrm{d}x}$ Using t substitution: Let $t = \sin(x)$ Then: $\mathrm{d}t = \cos(x)\space\mathrm{d}x$ And:  $x = \arcsin(t)$ Changing the limits: $t = \sin(x)$ $0$ becomes $\sin(0) = 0$ $2\pi$ becomes $\sin(2\pi) = 0$ Substituting in the definite integral: $A = \displaystyle \int_{0}^{0}{g(\arcsin(t))\space\mathrm{d}t}$ But Definite Integral where the lower and upper bounds are the same is $0$. So: $A = 0$, which is not possible. Thanks for the help.	calculus,integration,trigonometry,definite-integrals,inverse-function
A.83	Is the sequence of sums of inverse of natural numbers bounded?	I'm reading through Spivak Ch.22 (Infinite Sequences) right now. He mentioned in the written portion that it's often not a trivial matter to determine the boundedness of sequences. With that in mind, he gave us a sequence to chew on before we learn more about boundedness. That sequence is: $$1, 1+\frac{1}{2}, 1+\frac{1}{2}+\frac{1}{3}, 1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}, . . .$$ I know that a sequence is bounded above if there is a number $M$ such that $a_n\leq M$ for all $n$. Any hints here?	calculus,sequences-and-series,harmonic-numbers
A.84	Is the ideal generated by ${4,x}$ a principal ideal in $Z[x]$?	I've : $I=<p,x>$ is not a principal ideal in $Z[x]$ where p is prime. My question :   Is $I=<p,x>$ a principal ideal in $Z[x]$ where p is not a prime? More particularly, is the ideal generated by ${4,x}$ a principal ideal in $Z[x]$ ?	abstract-algebra,ring-theory,ideals,principal-ideal-domains
A.85	Expected number of steps for a bug to reach position $N$	A bug starts at time $0$ at position $0$. At each step, the bug either moves to the right by $1$ step $(+1)$ with probability $1/2$, or returns to the origin with probability $1/2$. What is the expected number of steps for this bug to reach position $N$? I tried to first find the possibility that this bug reaches $N$ as the number of steps goes to infinity. The recurrence equation I find is $$p_n = \frac{1}{2}p_{n-1}$$, where $p_n$ is the possibility for the bug starting at position $n$ to reach $N$. We also have the boundary condition $p_N = 1$. Then we see that $p_{N-1}=2$, and that $p_0 = 2^N$, which doesn't make sense at all because it is greater than $1$. I think I should sort out the value of probability first, and think about the number of expected steps later. I'm sure there is something wrong with the recurrence equation, but what's wrong about it?	markov-chains,random-walk
A.86	Is it true that $\sum_{k=0}^{n}k\cdot \left(\begin{array}{l}{n}\\{k}\end{array}\right)=O\left(2 ^ {n\log _{3}n}\right)?$	Problem: Is it true that $\sum_{k=0}^{n}k\cdot \left(\begin{array}{l}{n}\\{k}\end{array}\right)=O\left( 2 ^ {n\log _{3}n}\right)?$ My start of solution: $$\sum_{k=0}^{n}k\cdot \left(\begin{array}{l}{n}\\{k}\end{array}\right)\leq \sum_{k=0}^{n}k\cdot \left(\begin{array}{l}{n}\\{\lfloor \frac{n}{2}\rfloor}\end{array}\right)\leq \frac{n\cdot(n+1)}{2}\cdot \left(\begin{array}{l}{n}\\{\lfloor \frac{n}{2}\rfloor}\end{array}\right)\leq n(n+1)! \leq nn^n \leq n^{n+1}$$ I think this upper bound is way too large and I can't seem to find a solution.	combinatorics,elementary-number-theory,discrete-mathematics
A.87	Is it true that $\forall n \in \Bbb{N} : (\sum_{i=1}^{n} a_{i} ) (\sum_{i=1}^{n} \frac{1}{a_{i}} ) \ge n^2$ , if all $a_{i}$ are positive?	If  $\forall i \in \Bbb{N}: a_{i} \in \Bbb{R}^+$ , is it true that $\forall n \in \Bbb{N} : \big(\sum_{i=1}^{n}a_{i}\big) \big(\sum_{i=1}^{n}  \frac{1}{a_{i}}\big) \ge n^2$ ?  I have been able to prove that this holds for $n=1$ , $n=2$, and $n=3$ using the following lemma:  Lemma 1:  Let $a,b \in \Bbb{R}^+$. If $ab =1$ then $a+b \ge 2$  For example, the case for $n=3$ can be proven like this: Let $a,b,c \in \Bbb{R}^+$. Then we have: $(a+b+c)\big(\frac{1}{a} + \frac{1}{b} + \frac{1}{c}\big) = 1 + \frac{a}{b} + \frac{a}{c} + \frac{b}{a} + 1 + \frac{b}{c} + \frac{c}{a} + \frac{c}{b}  + 1 $ $= 3 + \big(\frac{a}{b}  + \frac{b}{a}\big) + \big(\frac{a}{c} + \frac{c}{a}\big) + \big(\frac{b}{c} + \frac{c}{b}\big) $ By lemma 1, $\big(\frac{a}{b}  + \frac{b}{a}\big) \ge 2$,  $ \big(\frac{a}{c} + \frac{c}{a}\big) \ge 2$ and  $\big(\frac{b}{c} + \frac{c}{b}\big) \ge 2$ , therefore: $3 + \big(\frac{a}{b}  + \frac{b}{a}\big) + \big(\frac{a}{c} + \frac{c}{a}\big) + \big(\frac{b}{c} + \frac{c}{b}\big) \ge 3 + 2 + 2 +2 = 9 = 3^2 \ \blacksquare $ However I'm not sure the generalized version for all natural $n$ is true. I can't come up with a counterexample and when I try to prove it by induction I get stuck. Here is my attempt: Let $P(n)::\big(\sum_{i=1}^{n}a_{i}\big) \big(\sum_{i=1}^{n} \frac{1}{a_{i}}\big) \ge n^2$ Base case: $\big(\sum_{i=1}^{1}a_{i}\big) \big(\sum_{i=1}^{1} \frac{1}{a_{i}}\big) = a_{1} \frac{1}{a_{1}} = 1 = 1^2$ , so $P(1)$ is true. Inductive hypothesis:  I assume $P(n)$ is true. Inductive step: $$\left(\sum_{i=1}^{n+1}a_{i}\right) \left(\sum_{i=1}^{n+1} \frac{1}{a_{i}}\right) = \left[\left(\sum_{i=1}^{n}a_{i}\right) + a_{n+1}\right] \left[\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) + \frac{1}{a_{n+1}}\right]$$ $$=\left(\sum_{i=1}^{n}a_{i}\right) \left[\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) + \frac{1}{a_{n+1}}\right] + a_{n+1} \left[\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) + \frac{1}{a_{n+1}}\right]$$ $$=\left(\sum_{i=1}^{n}a_{i}\right)\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) +\left(\sum_{i=1}^{n}a_{i}\right) \frac{1}{a_{n+1}} + a_{n+1} \left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) +a_{n+1} \frac{1}{a_{n+1}}$$ $$=\left(\sum_{i=1}^{n}a_{i}\right)\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) +\left(\sum_{i=1}^{n}a_{i}\right) \frac{1}{a_{n+1}} + a_{n+1} \left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) +1 $$ $$\underbrace{\ge}_{IH} n^2 + \left(\sum_{i=1}^{n}a_{i}\right) \frac{1}{a_{n+1}} + a_{n+1} \left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) + 1$$ And here I don't know what to do with the $\big( \sum_{i=1}^{n}a_{i} \big) \frac{1}{a_{n+1}} + a_{n+1} \big(\sum_{i=1}^{n} \frac{1}{a_{i}}\big)$ term. Is this inequality true? If it is, how can I prove it? If it isn't, can anyone show me a counterexample?	algebra-precalculus,inequality
A.88	Is the polynomial $x^4+10x^2+1$ reducible over $\mathbb{Z}[x]$?	Is the polynomial  $x^4+10x^2+1$  reducible over  $\mathbb{Z}[x]$?	abstract-algebra,ring-theory,field-theory,irreducible-polynomials
A.89	Parametrization of pythagorean-like equation	Is there any known complete parametrization of the Diophantine equation $$ A^{2} + B^{2} = C^{2} + D^{2} $$ where $A, B, C, D$ are (positive) rational numbers, or equivalently, integers?	number-theory,diophantine-equations
A.90	Question on the definition of an Inverse matrix	By definition, if $A$ is a $ n \times n $ matrix, an inverse of $A$ is an $ n \times n $ matrix $A^{-1}$ with the property that: $$ A^{-1}A=\mathbb I_n \ \ \land \ \ AA^{-1}=\mathbb I_n \ \ \ \ (1)$$  where $ \mathbb I_n $ is the $ n \times n $ identity matrix. Are there any cases where $ A^{-1}A=\mathbb I_n$ but $AA^{-1} \neq \mathbb I_n$ or the other way around (and thus making (1) a false statement) ?	linear-algebra,matrices
A.91	Continuous function that reaches each value on its range exactly 2 times.	Is there a continuous function from $\mathbb{R}\to\mathbb{R}$ that reaches all of its possible values (each value in it's range) exactly $2$ times (for example, $x^2$ would be perfect if it wasn't for $0$..). Also, the same question but $3$ times. I'm almost certain that there aren't such functions but who knows haha maybe there are a bunch...	real-analysis,calculus
A.92	Is there a principal maximal ideal in $\mathbb F_q[X,Y]$?	Given an infinite field $K$, one can prove that any maximal ideal of $K[X,Y]$ can't be principal. In fact, every non-principal prime ideal is a maximal ideal, and can be generated by two polynomials. I am wondering whether the same result holds in $\mathbb F_q[X,Y]$. Can we find a principal ideal $I = (P(X,Y))$ for some irreducible polynomial $P$ that is a maximal ideal ?    Such a polynomial $P$ must have positive degrees in both $X$ and $Y$. Indeed, given an irreducible polynomial $Q(X)$ in only one variable, the quotient  $$\mathbb F_q[X,Y]/(Q(X))\cong (\mathbb F_q[X]/(Q(X)))[Y]$$ is a ring of polynomials over a field and as such, can never be a field.   Moreover, such a polynomial $P$ must have the form  $$P(X,Y) = \sum_{i=0}^n P_i(X)Y^i$$ where $d\geq 1$, the $P_i$'s are polynomials in $X$ and $P_n$ is a nonzero polynomial that vanishes identically on $\mathbb F_q$, thus $P_n$ must be divisible by $\prod_{\alpha \in \mathbb F_q} (X-\alpha)$. Indeed, if there were some $\alpha \in \mathbb F_q$ such that $P_n(\alpha) \not = 0$, then the ideal $I:=(P,X-\alpha)$ would contain $(P)$ strictly. If $(P)$ were to be maximal, $I$ would be the whole of $\mathbb F_q[X,Y]$. Writing down the fact that $1\in I$ and evaluating $X=\alpha$ would leads us to the conclusion $n=\deg_Y(P)=0$, which is absurd.   This is all I could infer so far. With respect to the above, I tried looking at $P(X,Y) = (X^{p-1}-1)XY - 1$ or $P(X,Y) = (X^{p-1}-1)XY - X - 1$ in $\mathbb F_p[X,Y]$ for some prime number $p$ but I have trouble determining whether the quotient is a field or not.   Would somebody know the answer of the problem, and according to it, give a proof or a counter-example ? Thank you very much in advance.	polynomials,commutative-algebra,field-theory,finite-fields
A.93	Characteristic Polynomial $AB =$ characteristic polynomial $ BA$?	Let $A,B$ matrix on $\mathbb{R}$ size $nxn$. How can I prove that $det(xI - AB) = det(xI - BA)$ if $A$ and $B$ are singular matrix	linear-algebra,polynomials
A.94	Are there $2^{\aleph_{0} }$ sets of natural numbers such that each two have finite intersection	Question: Are there $2^{\aleph_{0} }$ sets of natural numbers such that each two have finite intersection. From what I've read about infinite families, I need to ignore those who have the properpty $P$. Property $P$: the family is threadless, but whenever we take finitely many sets from the family, those sets have infinite intersection. and probably find ones with property $T$. Property $T$: the family is threadless, and it is an “almost-tower”: Whenever you pick two sets in the family, one of them is almost-contained in the other. - probably meaning that we can find such an intersection which is finite, because members are contained in each other. Then I thought.. To find them, I should think of rational numbers instead of natural numbers, remembering that rational numbers can be paired up with natural ones, so solving this problem for families of rational numbers is the same as solving it for families of natural numbers. Now, I need to consider various ways of defining the real numbers. And I'm stuck.. any help is appreciated.	cardinals,rational-numbers,natural-numbers
A.95	Length of convex paths and bounding $\sin x$	"The problem:  Defining $\sin x$ as the leg $b$ of a right triangle with $\angle B=x$ (in radians) and hypotenuse $1$, prove that $$\lim_{x\to 0}\frac{\sin x}x=1$$  (The motivation is to find the derivative of $\sin x$ in a elementary, ""pre-Taylor"" and ""pre-series"" context). I have seen many times a proof that it is based in the fact that the length of the arc $x$ satisfies $$\sin x<x<\tan x$$ or sometimes $$\sin x<x<\sin x+1-\cos x$$ The lower bound is clear because the $\sin x$ is the length of a straight segment and $x$ is the length of a curved segment with the same endpoints. But I find that the upper bound is based on this intuitive fact:  If $F$ and $G$ are two convex subsets of $\Bbb R^2$ and   $F\subset G$, then $|\partial F|<|\partial G|$.  ($|\partial F|$ is the length of the boundary of $F$). But I haven't ever seen a proof of that. I tried it myself but I get stuck trying to bound $$\int_s^t\sqrt{1+y'(u)^2}du$$ provided for example that $y''$ is negative and $y(s)=y(t)$. I realize that $y'$ can't be bounded (note for example $y=\sqrt{1-x^2}$, $-1\le x\le 1$). Question Is it possible to justify the derivative of $\sin x$ or the inequality about the lengths of bounds? Is there any calculus text with this approach (or similar) to define trigonometical functions?"	real-analysis,derivatives,trigonometry,reference-request,arc-length
A.96	Let $\sum_i a_i$ be a convergent sum with positive $a_i$. Does $\sum_i \frac{a_i}{a_i+a_{i+1}+a_{i+2}+\cdots}$ always diverge?	Let $\sum_i a_i$ be a convergent sum, with all $a_i$ positive.  Let $s_n=\sum_{i=n}^{\infty}a_i$.   Does $\sum_i  \frac{a_i}{s_i}$ always diverge?  I've tried a few examples such as $a_i= r^i$ (geometric series) and $a_i=1/i^2$ and it seems to always diverge.	sequences-and-series,analysis
A.97	what is the dimension of $\mathbb{R}$ at a vector space over there field $\mathbb{Q}$?	If we look at $\mathbb{C}$ as a vector space over $\mathbb{R}$ it's dimension will be $2$, because $\mathbb{C} = span\{1,i\}$.  A question I thought of is what would be the dimension of $\mathbb{R}$ as a vector space over $\mathbb{Q}$?  I feel like the answer should be infinity, because if the dimension was finite, say $n$, then for every $m := n+1$ real numbers  $x_1,...x_m$ there was a linear combination with rational coefficients that gives $0$: $\frac{a_1}{b_1}x_1+...+\frac{a_m}{b_m}x_m = 0$. Multiplying by $lcm(b_1,...,b_m)$ we get that for every $m$ real numbers there is a linear combination with natural coefficients that gives $0$. That feels false, how do you prove it?	linear-algebra,vector-spaces
A.98	If $R:S^{1}\rightarrow S^{1}$ is a irrational rotation, $\{R^{n}([x])\}$ is dense in $S^{1}$ for all points.	Let $\alpha$ a irrational number, and $R:S^{1}\rightarrow S^{1}$ the irrational rotation, i.e., $[x]\rightarrow[x+\alpha]$. I need to prove that, for all $[x]\in S^{1}$, the set $\{R^{n}([x])\}$ is dense in $S^{1}$. First, I can write $R$ by $$R(e^{2\pi ix})=e^{2\pi i (x+\alpha)}.$$ So, I can write $R^{n}(x)$, $n\in\mathbb{Z}$ by $$R^{n}(e^{2\pi i x})=e^{2\pi i(x+n\alpha)} $$ I need to prove that, for all $e^{2\pi i x}\in S^{1}$ and for all $[y]=e^{2\pi i y}\in S^{1}$, every neighborhood $V$ of $[y]$ contains a point $[z_{y}]=e^{2\pi i z}$ such that $[z_y]=R^{n}([x])$ for some $n\in\mathbb{Z}$. That is, $e^{2\pi i z}=e^{2\pi i (x+\alpha n)}\Rightarrow 2\pi iz=2\pi i(x+\alpha n)+2ik\pi\;\textrm{for some}\;k\in\mathbb{Z}\Rightarrow z=x+\alpha n+k.$ Is my way correct? If it does, how can I proceed now? If it doesn't, what I need to do? I don't think this question is duplicate. I'm showing my attempt to proof, that is different from other proofs.	general-topology,circles,rotations,irrational-numbers
A.99	Rationals can be the set of continuity of a function?	Most of the functions that I have seen have their discontinuities on rationals and continuities on irrationals! I am wondering if there is any exampe of some function whose continuities are rationals? Or is other words   The set of continuities of a function $f:\mathbb{R}\to\mathbb{R}$ can be $\mathbb{Q}$?	real-analysis,calculus,continuity
A.100	"Is this set ""not closed""?"	Is it correct to say that this set $E=(0,1]$ where $E\subseteq R$ (Where $R$ is the set of real numbers) is not closed?	real-analysis
A.201	Matrix over division ring having one sided inverse is invertible	I want to see if there is any elementary way to prove the following assertion about matrices over division rings (such as not using Wedderburn's theory or tensoring techniques). If an $n\times n$ matrix over a division ring has left inverse, then it also has right inverse. The assertion has elementary proof for matrices over fields, but I am considering over division rings.  One can give some hints also.	abstract-algebra,matrices,ring-theory
A.202	Rings Trapped Between Fields	Some Background and Motivation: In this question, it is shown that an integral domain $D$ such that $F \subset D \subset E$, $E$ and $F$ fields with $[E:F]$ finite, is itself a field.  However, a significantly more general result holds and seems worthy, of independent address; hence, Let $F \subset E \tag 1$ be fields with $[E:F] < \infty; \tag 2$ if $R$ is a ring such that $F \subset R \subset E, \tag 3$ show that $R$ is in fact a field.	abstract-algebra,ring-theory,field-theory,extension-field
A.203	Why does the subtraction symbol go away? $-(-x)= x$	"$+(+x) = +x$ but why is $-(-x) = +x$??? What's the reason behind the rule, it's really basic and ""obvious"" because a no turns a no to a yes But I don't want to reason like that, lol. So how would you explain it? Do I just say on a real number line $-x$ makes a turnaround and $-(-x)$ would turn it positive again? Also if I say for example: $-x = 5$ then I do have $-(-x) = -5$ Is it correct? It wouldn't matter, right? (Btw: I don't know if the tag ""elementary-number-theory"" is correct) **The question is different to $(-x)*(-x)=x$"	abstract-algebra
A.204	subscheme where two morphisms agree is points where they agree on residue fields	"Let $X, Y, Z$ be schemes, where $X, Y$ are $Z$ schemes.  I know the definition of ""the locally closed subscheme of $X$ where two $Z$-  morphisms $\pi, \pi': X\rightarrow Y$ agree"" from its universal property.  Also I can define it as the fiber product of the diagonal $$\delta :  Y\rightarrow Y\times_Z Y$$ with $$(\pi, \pi'): X\rightarrow Y\times_Z Y.$$ My question:  how to prove that the underlying set of ""the locally closed subscheme where the two morphisms agree"" is the same as the set of points where the two morphism agree on the residue field. It is probably clear thatthe former is contained in the latter, but why is it all of them?  That is, why is a point where $\pi, \pi'$ agree on the residue field necessarily contained in ""the subscheme where $\pi, \pi'$ agree""?"	algebraic-geometry,schemes
A.205	How can we find x for x^n = n^x	Find values of x such that $x^n=n^x$ Here, n $\in$ I.  One solution will remain x=n But i want to find if any more solutions can exist $$x^n=n^x$$	algebra-precalculus,logarithms
A.206	I'm confused on the limit of $\left(1+\frac{1}{n}\right)^n$	Okay so I read Richard Rusczyk's AoPS Volume 2 Book, and I stumbled upon the part where he informs very briefly that $\lim_{n \rightarrow \infty} \left(1+\frac{1}{n}\right)^n=e$. But he doesn't really provide a rigorous proof as to why that's true (not criticizing him or anything).. It would really help if someone could provide me with the simplest proof possible as to why $\lim_{n \rightarrow \infty} \left(1+\frac{1}{n}\right)^n=e$. Thank you in advance!	algebra-precalculus,limits,exponential-function
A.207	What function is asymptotically eqyivalent to $\sum_{k \geq 0}k!/N^k$?	I am working on this problem to find a function $f(N)$ s.t. $$ f(N) \sim \sum_{k \geq 0}\frac{k!}{N^k} $$ where $\sim$ means that given functions $f$ and $g$, we have $f \sim g \implies f = O(g) \text{ and } f=\Omega(g)$. For instance, given the right hand side of the equation above, on input $N$ we have the following (it's a divergent series) $$ f(N) \sim 1 + \frac{1}{N} + \frac{1}{2N^2} + \frac{1}{6N^3} + O\bigg(\frac{1}{N^4}\bigg) $$ The closest function I can think of are the binomials where: $$ (N \text{ choose } r) \sim \frac{N^r}{r!} $$ But it doesnt really equal the first equation above. Any help?	algorithms,asymptotics,approximation,factorial
A.208	Where does this asymptote for $H_n^{(k)}$ come from?	@Claude Leibovici's answer to this Math Stack Exchange question (it's the second answer) gives an asymptote for the generalised harmonic number $H_n^{(k)}=\sum_{i=1}^n \frac{1}{i^k}$: $$H_n^{(k)}=n^{-k}    \left(-\frac{n}{k-1}+\frac{1}{2}-\frac{k}{12    n}+O\left(\frac{1}{n^3}\right)\right)    +\zeta (k)$$ Heuristically, this is an excellent fit. But can someone please tell me if this is a published result, and more importantly how it is derived?	asymptotics,harmonic-functions,upper-lower-bounds,harmonic-numbers
A.209	Evaluate the definite integral: $\int_0^\infty e^{-hx^2}\;\mathrm{d}x$	where $h>0$. Could someone explain to me how to solve it? I searched the internet and I found the result is $\frac{\sqrt{\pi}}{2\sqrt{h}}$ but I couldn't undersand Gauss error function - that is involved in solving.	calculus,integration,definite-integrals
A.210	what's an elegant way to show that $x(1-x) \leq \frac14$?	for $x \in \mathbb{R}$, consider $f(x) = x(1-x)$, using traditional methods of finding global extremas, we can show that the derivative has a unique zero at $x= \frac12$ and $f''(\frac12) < 0$, thus $x(1-x) \leq \frac14 = f(\frac12)$ is there a more elegant way ?	calculus,inequality
A.211	$\int\sqrt{x^2\sqrt{x^3\sqrt{x^4\sqrt{x^5\sqrt{x^6\sqrt{x^7\sqrt{x^8\ldots}}}}}}}\,dx$	I was attempting to solve an MIT integration bee problem (1) when I misread the integral and wrote (2) instead.  $$\int\sqrt{x\cdot \sqrt[3]{x\cdot \sqrt[4]{x\cdot\sqrt[5]{x\ldots } }}}\,dx\tag{1}$$ $$\int\sqrt{x^2\sqrt{x^3\sqrt{x^4\sqrt{x^5\sqrt{x^6\sqrt{x^7\sqrt{x^8\ldots}}}}}}}\,dx\tag{2}$$ I was able to solve (1), as the integrand simplifies to $x^{e-2}$, however, I'm struggling with solving (2).  If we rewrite the roots as powers, we get: $$\int x^\frac{2}{2}\cdot x^\frac{3}{4}\cdot x^\frac{4}{8}\cdot x^\frac{5}{16}\ldots\,dx$$ combining the powers we get: $$\int x^{\frac{2}{2}+\frac{3}{4}+\frac{4}{8}+\frac{5}{16}+\ldots}$$ the exponent is the infinite sum $$\sum^{\infty}_{n=1}\frac{n+1}{2^n}\tag{3} $$ we can split this into:  $$\sum^{\infty}_{n=1}\frac{n}{2^n}+\sum^{\infty}_{n=1}\frac{1}{2^n} $$ The right sum is well known except here the sum begins at $n=1$, meaning that the right sum evaluates to 1. Messing around with desmos, the integrand appears to be $x^3,x>0$ implying that (3) converges to 3 and the $\sum^{\infty}_{n=1}\frac{n}{2^n}$ converges to 2. Which is part I'm struggling with. Any ideas?  $$\sum^{\infty}_{n=1}\frac{n}{2^n}$$	calculus,sequences-and-series
A.212	Evaluating an infinite series	I've been given the function $$f(x)=\sum_{n=0}^{\infty}(2n+1)(2x)^{2n}$$ And I have to evaluate $f(1/4)$ so find the value of $$f(1/4)=\sum_{n=0}^{\infty}\frac{2n+1}{2^{2n}}$$ I would appreciate any help with this as I am pretty lost.	calculus,sequences-and-series,power-series,taylor-expansion
A.213	Calculate $\sum_{x=1}^{\infty} \frac{(x-1)}{2^{x}}$	I can prove it converges but I don't know at what value it converges. $\sum_{x=1}^{\infty} \frac{(x-1)}{2^{x}}$	calculus,power-series
A.214	Show that $\int\limits_{-\infty}^\infty e^{-\pi x^2}dx = 1$	I want to show that $\int\limits_{-\infty}^\infty e^{-\pi x^2}dx = 1$. By definition $$\int\limits_{-\infty}^\infty e^{-\pi x^2}dx = \lim\limits_{t\to\infty}\int\limits_{-t}^t e^{-\pi x^2}dx$$ and since the integrand $e^{-\pi x^2}$ is an even function $$\int\limits_{-\infty}^\infty e^{-\pi x^2}dx = \lim\limits_{t\to\infty}\int\limits_{-t}^t e^{-\pi x^2}dx = 2\lim\limits_{t\to\infty}\int\limits_0^t e^{-\pi x^2}dx$$ i.e. we can equivalently show that $\lim\limits_{t\to\infty}\int\limits_0^t e^{-\pi x^2}dx=\frac{1}{2}$. Since the antiderivative of $e^{-x^2}$ is given by the error function we can't straightforwardly evaluate the integral, so I tried to use the power series expansion, hoping to be able to see that the resulting series will converge to $\frac{1}{2}$: $$|\int\limits_0^t e^{-\pi x^2}dx-\frac{1}{2}| = |\int\limits_0^t\sum\limits_{n=0}^\infty\frac{\pi^n\cdot x^{2n}}{n!}dx - \frac{1}{2}| = |\sum\limits_{n=0}^\infty\frac{\pi^n\cdot t^{2n+1}}{n!\cdot(n+1)}-\frac{1}{2}|$$ However, I'm in a doubt that it converges and a quick check in Wolfram Mathematica shows indeed that with $t\to\infty$ the resulting series will diverge. What am I doing wrong? Can anybody help me with a proof for this problem? Any help will be really appreciated.	calculus,integration,improper-integrals
A.215	Set Of Discontinuities Of A Derivative	Prove that the set of discontinuities of a derivative of an everywhere differentiable function $f(x)$ is of 1st category. Let $f'(x)$ be a derivative of an everywhere differentiable function $f(x)$. Now as the set of discontinuities of any arbitrary functions can be written as a countable union of closed sets. So let $A$ be the set of discontinuities of $f'(x)$, then we can write $$A=\bigcup_{n=1}^{\infty}A_n$$ where all the $A_n$ are closed set. Now suppose that for $n=n_0$ the set $A_{n_0}$ is not nowhere dense then there exists an open interval $(p, q)$ such that for any interval $I$ in that open interval $(p, q)$ we have $$I \cap A_{n_0} \neq \phi$$ and hence $A_{n_0}$ is dense in the open interval $(p, q)$ and as $A_{n_0}$ is closed so it contains the interval $(p, q)$ and hence $f'(x)$ is entirely discontinuous on the open interval $(p, q)$, but as the derivative of an everywhere differentiable function cannot be entirely discontinuous on an interval, so a contradiction. Is My Proof Correct??	calculus
A.216	Compute the limit $\lim\limits_{t \to + \infty} \int_0^{+ \infty} \frac{ \mathrm d x}{e^x+ \sin tx} $	I have been working on this limit for days, but I am not getting it. The question is  Compute the limit $$\lim_{t \to + \infty} \int_0^{+ \infty} \frac{ \mathrm d x}{e^x+ \sin (tx)}$$  Note that the integral is well defined and convergent for every $t >0$. Indeed the integrand function is a positive function for every $t >0$ since $$e^x + \sin tx > e^x-1 > x>0$$ And as $x \to + \infty$ the integrand function behaves like $e^{-x}$. WHAT I TRIED: I consider $t=2n \pi$ a multiple of $2 \pi$, and see what happens: $$\int_0^{+ \infty} \frac{ \mathrm d x}{e^x+ \sin (2n \pi x)} = \sum_{k=0}^\infty \int_{k /n}^{(k+1) /n} \frac{ \mathrm d x}{e^x+ \sin (2n \pi x)}$$ Making the change of variables $u = 2n \pi x$ I get \begin{align}\sum_{k=0}^\infty \frac{1}{2n \pi} \int_{2k \pi}^{(2k+2) \pi} \frac{ \mathrm d u}{e^{u/2n \pi}+ \sin (u)} &\ge \sum_{k=0}^\infty \frac{1}{2n \pi} \int_{2k \pi}^{(2k+2) \pi} \frac{ \mathrm d u}{e^{(2k+2) \pi/2n \pi}+ \sin (u)} \\&= \sum_{k=0}^\infty \frac{1}{2n \pi} \int_{2k \pi}^{(2k+2) \pi} \frac{ \mathrm d u}{e^{(k+1)/n}+ \sin (u)}\end{align} where I write the lower bound with the minimum of the function at $u=(2k+2) \pi$. Now I use the fact that the integrand function does is integrated over a period of $2 \pi$, and using the result for $C>1$ $$\int_0^{2 \pi} \frac{ \mathrm d u}{C+ \sin (u)} = \frac{2 \pi}{\sqrt{C^2-1}}$$ I get the estimate \begin{align}\sum_{k=0}^\infty \frac{1}{2n \pi} \int_{2k \pi}^{(2k+2) \pi} \frac{ \mathrm d u}{e^{(k+1)/n}+ \sin (u)} &= \sum_{k=0}^\infty \frac{1}{2n \pi} \frac{2 \pi}{\sqrt{e^{2(k+1)/n} -1 }} \\&= \frac{1}{n} \sum_{k=0}^\infty \frac{1}{\sqrt{e^{2(k+1)/n} -1 }}\end{align} Summing all up, I got that $$\int_0^{+ \infty} \frac{ \mathrm d x}{e^x+ \sin (2n \pi x)} \ge \frac{1}{n} \sum_{k=0}^\infty \frac{1}{\sqrt{e^{2(k+1)/n} -1 }}$$ As $n \to \infty$ the series converges to the Riemann integral $$\int_0^{+ \infty} \frac{\mathrm d y}{\sqrt{e^{2y}-1}} = \frac{\pi}{2}$$ Hence the limit should be a number larger than $\pi/2$, or $+ \infty$. Using WA I got for large values of $t$ that the integral is between $1$ and $2$, thus $\pi/2$ could be the actual limit.	calculus,integration,limits,definite-integrals
A.217	An easy Calculus Problem	Here's the question- Find the maximum area of an isosceles triangle inscribed in the ellipse $x^2/a^2 + y^2/b^2 = 1$. My teacher solved it by considering two arbitrary points on the ellipse to be vertices of the triangle, being $(a\cos\theta, b\sin \theta)$ and $(a\cos\theta, -b\sin \theta)$. (Let's just say $\theta$ is theta) and then proceeded with the derivative tests(which i understood) But, he didn't indicate what our $\theta$ was,and declared that these points always lie on an ellipse. Why so? And even if they do, what's the guarantee that points of such a form will be our required vertices? One more thing, I'd appreciate it if you could suggest another way of solving this problem. Thank you!	calculus,derivatives
A.218	Problem involving recursion of binomial coefficients	Wrt Ramsey numbers I have the following identity given to me: $ R(m, n) \leq R(m-1, n)+R(m, n-1) $ And i have the following bases cases: $R(m,2)=m$ and $R(2,n)=n$. One has to prove that: $R(m, n) \leq\left(\begin{array}{c}{m+n-2} \\ {m-1}\end{array}\right)$ It is obvious that we have to go on splitting the two terms on the RHS into pairs of terms decrementing the indices by 1 each time. But the appearance of the combination is non-obvious.	combinatorics,combinations,binomial-coefficients,ramsey-theory
A.219	How to prove a combinatorial identity with a combinatorial argument	I am trying to prove the following identity using a a combinatorial argument: $$\dbinom{n+r+1}{r}=\sum_{k=0}^{r}\dbinom{n+k}{k}$$	combinatorics,discrete-mathematics,combinations,combinatorial-proofs
A.220	How can i prove the identity $\sum_{k=0}^{n} \binom{x+k}{k}=\binom{x+n+1}{n}$	I'm having a difficult time understanding how to give a combinatorics proof of the identity $$\sum_{k=0}^{n} \binom{x+k}{k}=\binom{x+n+1}{n}$$	combinatorics,summation,binomial-coefficients
A.221	What's the minimum number of $2$s needed to write a positive integer?	This is just for fun and inspired by Estimating pi, using only 2s. For a positive integer $n$, let $f(n)$ denote the minimum number of $2$s needed to express $n$ using addition, subtraction, multiplication, division, and exponentiation, together with the ability to concatenate $2$s, so for example $2 \times 22^2 + \frac{222}{2}$ is a valid expression. Other variants involving different sets of allowed operations are possible, of course. This function is very far from monotonic, so to smooth it out let's also consider $$g(n) = \text{max}_{1 \le m \le n} f(m).$$ For example,  $f(1) = 2$ ($1 = \frac 22$) $f(11) = 3$ ($11 = \frac{22}{2}$)   Question: What can you say about $f(n)$ and $g(n)$? Can you give exact values for small values of $n$? Can you give (asymptotic or exact) upper bounds? Lower bounds?  As a simple example we can write any positive integer $n$ in the form $2^k + n'$ where $n' < 2^k$ ($2^k$ is just the leading digit in the binary expansion of $n$), which gives $f(n) \le f(k) + 1 + f(n')$. If we write $\ell(n) = \lfloor \log_2 n \rfloor$ then iterating this gives something like $$g(n) \le \sum_{k=1}^{\ell(n)} \left( g(k) + 1 \right).$$ This gives an upper bound growing something like $\ell(n) \ell^2(n) \ell^3(n) \dots$ which I think is pessimistic. For example, in my answer to the linked question I show that $$f(14885392687) \le 36$$ and $\ell(14885392687) = 33$ so maybe we can expect something as good as $g(n) = O(\log n)$ for an upper bound. I have no idea about a lower bound, other than to write down an upper bound on the number of possible expressions that can be made with a given number of $2$s. Edit: A related question involving $4$s and more allowed operations: How many fours are needed to represent numbers up to $N$?	combinatorics,optimization,recreational-mathematics
A.222	A company hires $11$ new employees, and they will be assigned to four different departments, A, B, C, D	A company hires $11$ new employees, and they will be assigned to four different departments, A, B, C, D. Each department has at least one new employee. In how many ways can these assignments be done?  I know that for each section (A,B,C,D) I should add a () and as long as every section must get a new employee we should start like this: $$(x+x^2/2!+x^3/3!+...)^4$$ then if we look  it's $(e^x-1)^4$. After this step I don't know what to do.	combinatorics,generating-functions
A.223	combinatorial proof that $\sum_{i=0}^n {n+i\choose i}\frac{1}{2^i} = 2^n$	Give a combinatorial proof that $\displaystyle\sum_{i=0}^n {n+i\choose i}\frac{1}{2^i} = 2^n$.  I'm not sure if Pascal's identity is useful here. Or perhaps there is a way involving binary strings? $2^n$ is the number of binary strings of length $n$, so if there was some way to decompose these strings into disjoint sets $B_i$ with cardinality ${n+i\choose i}\frac{1}{2^i}$, a proof using that method might work. ${n+i\choose i}$ is the number of binary strings of length $n+i$ with exactly $i$ ones, since one can choose $i$ of the $n+i$ positions to be ones in ${n+i\choose i}$ ways and make the rest zeroes in one way. Then we divide by the number of binary strings of length $i$, though I'm not sure how to deduce the combinatorial significance of this. I'm most likely thinking of this the wrong way.	combinatorics,elementary-set-theory,summation,combinatorial-proofs
A.224	I think I found a flaw in the $\varepsilon$-$\delta$ definition of continuity.	If I have a function $f(x)$ defined as follows.  $f(x) = 1$ for all $x<1$ and $x>2$; $f(x) = 100$ for $x = 1.5$; $f(x)$ is undefined anywhere else.  According to the $\varepsilon$-$\delta$ definition of continuity, if I take $\delta$ as any positive number smaller than $0.5$, then $f(x)$ by definition is continuous at $x = 1.5$ because within the $\delta$-neighborhood there is only one point defined, but $f(x)$ is obviously not continuous at $x = 1.5$. Below is the $\varepsilon$-$\delta$ definition of continuity: The function $f(x)$ is continuous at a point $x_0$ of its domain if for every positive $\varepsilon$ we can find a positive number $\delta$ such that $$|f(x) - f(x_0)|<\varepsilon$$ for all values $x$ in the domain of $f$ for which $|x-x_0|<\delta$.	continuity
A.225	Finding the sum of non-unique roots of cubic equations	The real numbers $\alpha,\beta$ satisfy $$\alpha^3-3\alpha^2+5\alpha-17=0\tag{1}$$ $$\beta^3-3\beta^2+5\beta+11=0\tag{2}$$ Find $\alpha+\beta$  Are the three roots of both cubic equations unique, or is there only one root? How can you prove it? What's the best approach to this problem?  I tried using Vieta's formulas, where the sum of three roots of (1) and (2) are: $$\alpha_1+\alpha_2+\alpha_3=3$$ $$\beta_1+\beta_2+\beta_3=3$$ Summing both, $$\alpha_1+\alpha_2+\alpha_3+\beta_1+\beta_2+\beta_3=6$$ Assuming there is only one root for each of (1) and (2), we are done, but what if there isn't?	cubic-equations
A.226	Parametrization of the curve $x^{x^y}=y$	I was looking at the graph of the equation $x^{x^y}=y$ (Desmos link). This graph has two components that cross at the point $(1/e^e,1/e)=(e^{-e},e^{-1})$. Component 1 (as I'll call it) is the component $x^y=y$ which has the simple parametrization $$(x,y)=\left(t^{1/t},t\right),\qquad0 Component 2 is a path between the points $(0,0)$ and $(0,1)$.  Does component 2 also admit a parameterization?  To clarify: Component 2 is a path so of course it abstractly admits a parameterization, but I'm asking if there is a parametrization that we can actually write down algebraically in terms of elementary functions.  My motivation for this question is from the limiting behavior of the sequence $0,1,x,x^x,x^{x^x},x^{x^{x^x}},\ldots$, whose behavior is closely related to the solutions to $x^{x^y}=y$. In particular, if $x$ is less than $e^{-e}$ then this sequence alternates between the upper and lower parts of component 2.	curves,parametric,plane-curves,parametrization
A.227	$1+2+3... =-\frac{1}{12}$ - Question regarding this	So, I'm not a big expert in this subject but I know $1+2+3...=-\dfrac{1}{12}$ isn't to do with 'real' maths but it's all to do with the zeta function; however I was watching a maths video and the equation: $$ \frac{x(x+1)}{2} $$ ... is actually a perfect equation for the series $1+2+3...$ etc. where $x$ represents $n$ in a series and $y$ is the sum of the series up to $n$. So, you can conclude that: $$ \sum^{n}_{i=1}1+2+3...=\frac{x(x+1)}{2} $$ However, this is where it gets weird; as you have probably guessed, the roots of the equation is $x=0,-1$  but if I want to find the integral of the roots from $-1$ to $0$ which is under the $x$ axis, I get the following: $$ \int_{-1}^{0} \frac{x(x+1)}{2}\:dx=-\frac{1}{12} $$ So, my question is why is this the case; what connection is there between the value of the integral under the $x$ axis that this graph has compared to the summation of the series? Link to Desmos graph for more clarity	definite-integrals,summation,quadratics,zeta-functions
A.228	Terrible integral with parameter	Let be   $\quad f(x)=\int_{0 }^{+\infty}cos\left(\frac{t^3}{3}+xt\right) d t$  Find the integral $$F(x, y)=\int_{-\infty}^{+\infty} f(t+x) f(t+y) d t$$ I tried exploring f(x), took it in parts, got that it converges. F(x,y) is difficult to investigate, since the product of integrals is there, I don't know what to do with it.	definite-integrals,improper-integrals
A.229	Finding integral of function involving fractional part of x	The integral given is: $$\int_{0}^{1}\big\lbrace\frac{1}{x}\big\rbrace \big\lbrace\frac{1}{1-x}\big\rbrace \big\lbrace1-\frac{1}{x}\big\rbrace dx$$ where  $\big\lbrace x\big\rbrace$ represents the fractional part of $x$ I first tried breaking it using a piecewise definition but I couldn't figure out how to do it as there wasn't any consistent pattern that I could spot.  I tried graphing it to get an idea but that also didn't get me anywhere. Finally, I tried using the property of definite integral that $\int_{0}^{a}f(x)dx=\int_{0}^{a}f(a-x)dx$ but the first and seccond terms remained the same and the last term changed but not it did not lead to any noticeable changes. I am stuck now. Any help would be appreciated.	definite-integrals,fractional-part
A.230	Question about definition of Ramsey number	So I went through the definition of Ramsey number and I have a basic question. Definition: For any given number of colours, $c$, and any given integers $n_1, …, n_c$, there is a number, $R(n_1, …, n_c)$, such that if the edges of a complete graph of order $R(n_1, ..., n_c)$ are coloured with c different colours, then for some i between 1 and c, it must contain a complete subgraph of order ni whose edges are all colour i. Question: Is the multicolour Ramsey number $R(n_1,n_2,..n_c) $same as $R(n_2,n_1,..n_c) $or any other permutation of$ {n_1,n_2,..n_c}$? The definition seems to imply so, I just want to verify if I'm thinking right. I have read that $R(m,n)=R(n,m)$ but nothing about symmetricity of multicolour Ramsey numbers.	definition,ramsey-theory
A.231	Why is 1 divided by aleph null undefined?	So recently I have been thinking about infinity, and one of the things that I thought of was if you were able to get a defined value for the reciprocal of a transfinite (cardinal) number. So, I plugged $\frac{1}{א_0}$ into WolframAlpha and it said the following: img1  Why is this the case? Shouldn't this be similar to this case? $$\lim_{x\to\infty} \frac{1}{x} =0$$ Aren't $\infty$ and $א_0$ equal to the same value in this context? What am I missing here?	definition
A.232	Definition of Induced Matrix Norm	I'm getting confused between 2 variants of definition of induced matrix norm. Given a norm $||\cdot||$ on $\mathbb{R}^n$, the induced matrix norm is defined by $$ \left\lVert A \right\rVert = \max_{\mathbf v\not =0}\frac{\left\lVert A\mathbf v \right\rVert}{\left\lVert \mathbf v \right\rVert} \qquad for \quad A \in \mathbb{R}^{n\times n}.$$  I'm trying to deduce the second variant from this definition i.e. $$\left\lVert A \right\rVert =\max_{\Vert \mathbf w\Vert = 1}\Vert A \mathbf w\Vert.$$ Consider $\mathbf v= \frac{\left\Vert\mathbf v\right\rVert\mathbf v}{\left\Vert\mathbf v\right\rVert}=\mathbf w\left\Vert\mathbf v\right\rVert $ where $\mathbf w= \frac{\mathbf v}{||\mathbf v||}$ and $||\mathbf w||=1$. Therefore, $$ \left\lVert A \right\rVert = \max_{v\not =0}\frac{\left\lVert A\mathbf v \right\rVert}{\left\lVert \mathbf v \right\rVert}= \max_{v\not =0}\frac{\left\lVert A(\mathbf w\left\Vert\mathbf v\right\rVert )\right\rVert}{\left\lVert \mathbf v \right\rVert}=\max_{\mathbf v\not=0}\frac{||\mathbf v||}{||\mathbf v||}\Vert A \mathbf w\Vert.$$ I don't know how to continue from here on.  Also, I 'm reading textbooks where they say : $$\left\lVert A \right\rVert =\max_{\Vert \mathbf w\Vert = 1}\Vert A \mathbf w\Vert \quad for \quad \mathbf w \in \mathbb{R}^n .$$ My question is shouldn't it be  $$\left\lVert A \right\rVert =\max_{\Vert \mathbf w\Vert = 1}\Vert A \mathbf w\Vert \quad for \quad \mathbf w \in \mathbb{R}^n \setminus \{\mathbf 0\} .$$ Because $\mathbf w=\mathbf 0$ means $||\mathbf w||=0$ which is ruled out because $||\mathbf w||=1.$	definition,norm
A.233	Motivation for defining tangent vectors with derivations and why they should act on $f\in C^\infty(M)$	I'm revisiting the definition for tangent spaces in Lee's Introduction to Smooth Manifolds and I'm trying to convince myself why we might define tangent vectors as derivations at a point $p\in M$:  Let $M$ be a smooth manifold, and let $p\in M$. A linear map $v:C^\infty(M)\to \mathbb{R}$ is called a derivation at $p$ if \begin{align*} v(fg) = f(p)vg + g(p)vf \end{align*} for all $f,g\in C^\infty(M)$.  So far, I know that if $M=\mathbb{R}^n$, then each derivation can be given as a directional derivative in some direction in $\mathbb{R}^n$. After reading the parts on the differential and its computation in coordinates, I'm still wondering why we would be interested in defining a tangent vector as a map that acts on functions on the manifold and the benefits from acting on smooth functions. The main reason that I can think of is that the collection of derivations at a point forms a vector space, which is we what want for a tangent space. I have also looked at the approach of defining tangent vectors with equivalence classes of curves, but it seems that there's also an action on $f\in C^\infty(M)$ going on; we call curves $\gamma:J\to M$ the tangent vectors, and they have a directional-derivative-like operators that act on $f\in C^\infty(M)$ by \begin{align*} \left.\frac{d}{dt}(f\circ \gamma)(t)\right|_{t=0}. \end{align*} This seems really similar to how a vector in $\mathbb{R}^n$ defines its own directional derivative, but again, I'm not sure why the action on $f\in C^\infty(M)$ would be useful/significant.	differential-geometry,differential-topology,smooth-manifolds,tangent-spaces
A.234	Sards theorem for polynomial	I'm having some struggles with an aspect about something apparently trivial about Sard's theorem, but couldn't find anything online. Let $f$ be a polynomial. According to Sard's theorem, the image $f(Z)$ of the set of critical values  $$Z = \{a \in X : f'(a) = 0\}$$ has measure zero. What if I want to show that the set $Z$ itself has measure zero in the domain of $f$? I feel like it's so simple but i just can't get behind it.	differential-topology
A.235	Method for solving Diophantine equation $ax^2 + bx + c = y^2$	How do I solve the Diophantine equation $ax^2 + bx + c = y^2$? The approach I have so far is to use the transformation $X = 2ax + b$ and $Y = 2y$. Applying this, we get, $X^2 - dY^2 = n$, where $n = b^2 - 4ac$ and $d = a$. $X^2 - dY^2 = n$ is a Pell equation. Questions:  Is there any other method? What is the complexity of the algorithm for finding the solution to the Pell equation?	diophantine-equations,computational-complexity,pell-type-equations
A.236	Normalizing constant in Dirichlet distribution	According to references (e.g. Wikipedia and elsewhere), the Dirichlet distribution, parametrized by $\boldsymbol{\alpha}=(\alpha_1,\ldots,\alpha_K)$, is $$ D(x_1, \ldots, x_K) = \frac{1}{\mathrm{B}(\boldsymbol\alpha)} \prod_{i=1}^K x_i^{\alpha_i - 1} $$ where $$ \mathrm{B}(\boldsymbol\alpha) = \frac{\prod_{i=1}^K \Gamma(\alpha_i)}{\Gamma\left(\sum_{i=1}^K \alpha_i\right)}. $$ So, if $K = 2$ and $\alpha_1 = \alpha_2 = 1$ then this gives $ D(x_1, x_2) = 1/\mathrm{B(\boldsymbol\alpha)} $ where $$ \mathrm{B}(\boldsymbol\alpha) = \Gamma(1)^2 / \Gamma(2) = 1 $$ so, $D(x_1, x_2) = 1$ for all $x_1, x_2$.  However, $D(x_1, x_2)$ is defined on the standard $1$-simplex defined in $R^2$ by $x_i \ge 0$ and $x_1 + x_2 = 1$.  This is the span (or affine hull) of the two points $(0, 1)$ and $(1, 0)$.  Since this is a line segment of length $\sqrt{2}$, the integral of the Dirichlet distribution over this simplex is $\sqrt{2}$, not $1$ as expected.  What am I missing here? The same problem comes in higher dimensions.  For instance, for $K=3$, the simplex is a triangle with side $\sqrt{2}$, but the normalization constant becomes $B(\boldsymbol\alpha) = 1/\Gamma(3) = 1/2$, which is not the area of this triangle. What is wrong here?	dirichlet-series
A.237	Rewrite the propositions without implication	I want to rewrite the following types of propositions without the simple or double implication:  $p \land \lnot q \to r$ $p \land \lnot q \to r \land q$ $(p \to r) \leftrightarrow (q \to r)$  So we have to write these propositions without any implication, for example the first proposition like $p \land \lnot q \land r$ or is something else meant?	discrete-mathematics,logic
A.238	Definition of Equivalence Relation	"I was going through the text ""Discrete Mathematics and its Application"" by Kenneth Rosen (5th Edition) where I am across the definition of equivalence relation and felt that it is one sided.  Definition: A relation on a set A is called an equivalence relation if it is reflexive, symmetric, and transitive.  Now let us analyze the situation of what equivalence is meant to us intuitively. Let there be a binary relation $R$ defined on a set $A$. Now we suppose that $R$ be reflexive, symmetric and transitive. So we have for $a,b,c \in A$   $a R a$ (by the reflexive property of R) if $a R b$ then $b R a$  (by the symmetric property of R) if $a R b$ and $b R c$ then $aRc$ (by the transitive property of R)  Intuitively we can satisfy ourselves with the fact that the above are the necessary conditions for $R$ to be equivalent. So ""if $R$ is reflexive, symmetric and transitive, then $R$ is an equivalence relation"" Now working our intuition for equivalence relation $\sim$ we note the following. Let $\sim$ be an equivalence relation on a set A, then for $a,b,c \in A$ we have,  $a \sim a$ (by the intuitive knowledge of what $\sim$ means) if $a\sim b$ then $b \sim a$ (by the intuitive knowledge of what $\sim$ means) if $a\sim b$ and $b \sim c$ then $a\sim c$ (by the intuitive knowledge of what $\sim$ means)  Now we see that (1) implies $\sim$ is reflexive, (2) implies that $\sim$ is symmetric and (3) implies that $\sim$ is transitive. So we have ""if $\sim$ is an equivalent relation then $\sim$ is reflexive, symmetric and transitive"" From the two intuitive implications we can conclude that A relation on a set A is called an equivalence relation if and only if it is reflexive, symmetric, and transitive. and not what the book says. This definition makes quite sense unlike the book definition which says that if $R$ fails to be either reflexive or symmetric or transitive then $R$ may or may not be an equivalence relation, which after all gives a weird feeling. Correct me if my logic is wrong."	discrete-mathematics,elementary-set-theory,proof-writing,definition,relations
A.239	Fake proof, symmetric and transitive relation is already reflexive	Let $R$ be a symmetric, transitive relation. If $(x, y) \in R$ then the symmetric property implies that $(y, x) \in R$. Using the the transitive property upon $(x, y)$ and $(y, x)$ we can conclude $(x, x) \in R$. Is this fair logic or is it flawed?	discrete-mathematics,relations,fake-proofs
A.240	"What do the constants ""4"" and ""2"" in Bhaskara mean and where did they come from?"	In bhaskar, the way to get the result, is to get the $\Delta  = b^2 – 4ac$, and then the $X = (–b \pm \sqrt\Delta)/2a)$. But from where come these constants?	education
A.241	Divisibility of $n^3 +6n^2-7n$	Let $n = 2, 3, 4, ...$ be an integer. Show that $n^3 +6n^2-7n$ is divisible by $6$.  How should one approach this? Using modular arithmetic or some other approach?	elementary-number-theory
A.242	Show that for all prime numbers $p$ greater than $3$, $24$ divides $p^2-1$ evenly.	Show that for all prime numbers $p$ greater than $3$, $24$ divides $p^2-1$ evenly.  Since $(p+1)(p-1) = p^2-1$ we have that $\frac{(p+1)(p-1)}{24}=k$, where $k \in \Bbb Z.$ Now since $24 = 2^3 \cdot 3$ and the numerator contains always at least one even factor(?) we have that $24=2^3\cdot3\vert(p+1)(p-1).$ Is my reasoning here correct or am I missing something here?	elementary-number-theory
A.243	If $2^{2k}-x^2\bigm|2^{2k}-1$ then $x=1$	This is the $y=2^k$ case of this question. Suppose that $k\geq1$ and $0 and $2^{2k}-x^2\bigm|2^{2k}-1$. Is it necessarily the case that $x=1$? Equivalently: Suppose that there are two positive divisors of $2^{2k}-1$ which average to $2^k$. Is it necessarily the case that these two divisors are $2^k-1$ and $2^k+1$?	elementary-number-theory,divisibility
A.244	Compute the Cardinality of a quotient set	Let $R$ be a an Equivalence relation on $\Bbb{R}$ defined by: $$aRb \Leftrightarrow (a-b)\in \Bbb{Z}$$ What is the cardinality of $|\Bbb{R}/R|$?   where $\Bbb{R}/R$ is the quotient set of $\Bbb{R}$ under $R$.	elementary-set-theory
A.245	Is there a known set of closed form solutions to the functional equation f(f(z)) = sin z?	That is $$ f(f(z)) = \sin z $$ where $$ z \in \mathbb{Z} $$	functional-equations
A.246	Finding $n$ such that in a regular $n$-gon $A_1A_2\ldots A_n$ we have $\frac1{A_1A_2}=\frac1{A_1A_3}+\frac1{A_1A_4}$	INMO '92 Question 9:  Find $n$ such that in a regular $n$-gon $A_1A_2 ...A_n$ we have $$\frac{1}{A_1A_2}=\frac{1}{A_1A_3}+\frac{1}{A_1A_4}$$  I tried the following Assume it is inscribed in a circle. Then length of chord is $2\sin(\theta)$ where $\theta$ is half the angle subtended at the center between consecutive points. So, $\theta=\frac{180^\circ}{n}$. Then we get $$\csc(\theta)=\csc(2\theta)+\csc(3\theta)$$ Not sure quite how to proceed from there- using double and triple angle formulae doesn't seem to work	geometry,trigonometry,contest-math,polygons
A.247	Finding the endpoints of the maximal arc of circle $x^2+(y-8)^2=25$ visible from $(0,-5)$.	"The equation of circle $C$ is $x^2 + (y − 8)^2 = 25$. The eye is located at $E = (0, −5)$. The maximal circular arc visible to the eye is $AB$, which is then being projected on to the one-dimensional ""screen"" as $A'B'$. What are the co-ordinates of points $A$ and $B$?  I came this far: point $P$ on circle $C$ has the coordinates $x = 5 \cos\theta$, $y = 8 + 5 \sin \theta$. Now I should use this to find points $A$ and $B$, but I don't know how to proceed."	geometry,analytic-geometry
A.248	product of elements $\ne e$ implies only one element with order $2$	Let $G$ be an abelian group with even order and $M:=\{g\in G:g^2=e\}$. It is easy to show that $M$ is a sugroup of $G$ and the number of elements of $M$ must be a power of $2$.  I want to prove that the product of the elements of $G$ (which in this case is equal to the product of the elements of $M$) is not the identity element $e$, if and only if #$M=2$ (this means that there is only one element with order $2$). I found this claim when I studied the Wilson criterion for the primality of a number.  The case #$M=4$ is easy. If $a,b\in M$, we can show that $ab$ is not $a,b,e$, hence must be some other element $c$ and we get $abc=c^2=e$. But what about #$M=8$? If we have the elements $e,a,b,c,d,f,g,h$ and $ab=c$ and $df=g$, the product would be $h$ which is impossible considering my claim.  Who can complete my proof ?	group-theory,finite-groups
A.249	An abelian group proof with $g*g=e$ for all $g$.	I have to show that the following group $$ (G, * , e) $$ with its operation $*$, which is defined through $ g*g = e$  for every $g \in G $ is an abelian group. In order to do that one have only to show that the group is commutative. How can one prove it whereas the operation is defined always between an Element and itself? I reckon it is not so simple as it seems Thanks in advance for your help :)	group-theory,abelian-groups,monoid
A.250	How to show $(a_1a_2\ldots a_n)^{\frac{1}{n}}\leq \frac{\sum_{i=1}^{n}a_i}{n}$	How to show $(a_1a_2\ldots a_n)^{\frac{1}{n}}\leq \frac{\sum_{i=1}^{n}a_i}{n}$ with $a_i$ positive. Well, I tried by induction: with $n=2$ then $\sqrt{ab}\leq \frac{a+b}{2}$ is equivalent to say (elevate square in both side) $4ab\leq a^2 +2ab + b^2$ and this is equivalent $0\leq(a-b)^2$ and this is true. I suppose it is true for some $n$. But with $n+1$, I don't know how to do, Please can help me with a hint or other way, thank you so much.	inequality,a.m.-g.m.-inequality
A.251	How to prove that $(k+1)^{\frac{1}{k+1}} < k^{\frac{1}{k}} (k >= 3)$ using mathematical induction?	I've been trying to solve this for hours, but I just can't seem to do it. And yes, I know I have to show that $(k+2)^{\frac{1}{k+2}} < ... < (k + 1)^{\frac{1}{k+1}}$ from the starting assumption that $(k+1)^{\frac{1}{k+1}} < k^{\frac{1}{k}}$. Any hints would be highly welcome.	inequality,induction
A.252	Unexpected appearances of $\pi^2 /~6$.	"""The number $\frac 16 \pi^2$ turns up surprisingly often and frequently in unexpected places."" - Julian Havil, Gamma: Exploring Euler's Constant.   It is well-known, especially in 'pop math,' that $$\zeta(2)=\frac1{1^2}+\frac1{2^2}+\frac1{3^2}+\cdots = \frac{\pi^2}{6}.$$ Euler's proof of which is quite nice. I would like to know where else this constant appears non-trivially. This is a bit broad, so here are the specifics of my question:  We can fiddle with the zeta function at arbitrary even integer values to eek out a $\zeta(2)$. I would consider these 'appearances' of $\frac 16 \pi^2$ to be redundant and ask that they not be mentioned unless you have some wickedly compelling reason to include it. By 'non-trivially,' I mean that I do not want converging series, integrals, etc. where it is obvious that $c\pi$ or $c\pi^2$ with $c \in \mathbb{Q}$ can simply be 'factored out' in some way such that it looks like $c\pi^2$ was included after-the-fact so that said series, integral, etc. would equal $\frac 16 \pi^2$. For instance, $\sum \frac{\pi^2}{6\cdot2^n} = \frac 16 \pi^2$, but clearly the appearance of $\frac 16\pi^2$ here is contrived. (But, if you have an answer that seems very interesting but you're unsure if it fits the 'non-trivial' bill, keep in mind that nobody will actually stop you from posting it.)  I hope this is specific enough. This was my attempt at formally saying 'I want to see all the interesting ways we can make $\frac 16 \pi^2$.' With all that being said, I will give my favorite example as an answer below! :$)$  There used to be a chunk of text explaining why this question should be reopened here. It was reopened, so I removed it."	integration,sequences-and-series,riemann-zeta,big-list,pi
A.253	How would I show the result below using contour integration?	How would I show the result below using contour integration? $$\int_{-\infty}^{\infty} \frac{\cos bx - \cos ax}{x^2} dx = \pi (a-b)$$ where a>b>0 using contour integration. Any help would be greatly appreciated, thanks!	integration,complex-analysis,trigonometry,fourier-analysis,contour-integration
A.254	Integration question hard	Can I get some hints on how to solve this integral? $$ I=\int_0^\pi \frac{x \ dx}{1-sinx \ cosx} $$	integration,definite-integrals
A.255	Integral of $e^{-\frac{u^2}{2}}$	This is the first time I came across the problem of finding integral of $\propto$. I have a joint distribution $$f_{X,Y}(x,y) \propto \exp\left(13xy - 94x^2 - \frac{1}{2}y^2\right)$$  where $ -\infty< x <\infty, -\infty< y <\infty $ I attempted to find $f_X(x)$ as follows:  \begin{align*} f_X(x) &\propto \int_{-\infty}^\infty e^{13xy - 94x^2 - \frac{1}{2}y^2}{\rm d}y\\ &\propto \int_{-\infty}^\infty e^{-\frac{1}{2}(y - 13x)^2 - \frac{19x^2}{2}}{\rm d}y\\ &\propto \frac{1}{e^{\frac{19x^2}{2}}} \int_{-\infty}^\infty e^{-\frac{u^2}{2}}{\rm d}u \end{align*} where $ u = (y - 13x)^2 $ Similarly, I derived $$ f_Y(y) \propto \frac{1}{e^{\frac{19x^2}{376}}} \int_{-\infty}^\infty e^{-u^2}{\rm d}u $$ where $ u = \sqrt{94}x - \frac{13y}{2\sqrt{94}} $ Could you please show me how to proceed to the destination solutions? Thanks in advance.	integration,probability-distributions,definite-integrals
A.256	Why do we have to factorize the function before taking its limit	I am learning about limits and there is something that I cant quite understand: If we have the function:  $$ f(x) =\frac{x^2-1}{x-1} $$  Let's say that we want to see which value for y (image) the function approaches as x (domain) gets closer to 1. On a nutshell, we have to take this following limit:  $$ \lim_{x\to1}\frac{x^2-1}{x-1} $$  As soon as we look to this function, we realize that the function is not continuous at x = 1 (By the way, can I say that?).  I know the algorithm to figure out the solution of the limit: First, there is the need of eliminating the function discontinuity. Usually, it is just a matter of factorizing the function into a new function which the exactly same image as the one before with one crucial difference: The function is continuous for all real numbers  My doubts:Is my way to think about it correct? Can I think like that? Take the example above:    $$ f(x) = \frac{x^2-1}{x-1} $$  After factorizing, we get: $$ f(x) = {x+1} $$ If we plot both functions, they are the same, although the second one has its continuity all along the real numbers domain Thanks in advance	limits
A.257	"Is ""taking a limit"" a function? Is it a procedure? A ternary operation?"	"I was sitting in analysis yesterday and, naturally, we took the limit of some expression. It occurred to me that ""taking the limit"" of some expression abides the rules of a linear transformation $$\lim_{x \rightarrow k}\ c(f(x)+g(x)) = c \lim_{x \rightarrow k} f(x) + c\ \lim_{x \rightarrow k} g(x),$$ and (my group theory is virtually non existent) appears also to be a homomorphism: $$\lim_{x \rightarrow k} (fg)(x) = \lim_{x \rightarrow k} f(x)g(x), $$ etc. Anyway, my real question is, what mathematical construct is the limit?"	limits,terminology
A.258	How to prove $\lim_{x \to \infty} \frac{x^k}{e^x}=0$ ? (k is any positive number)	Originally, i was trying to find the value of $$\lim_{h \to 0} \frac{e^{-\frac{1}{h^2}}}{h}$$ to find out differentiabiltiy of  $f(x) = \begin{cases} e^{-1/x^2} & \text{ if } x \ne 0 \\ 0 & \text{ if } x = 0 \end{cases}$  at 0. In this case,  $$\lim_{h \to 0} \frac{e^{-\frac{1}{h^2}}}{h}=\lim_{x \to \infty} \frac{x}{e^{x^2}}=\lim_{x \to -\infty} \frac{x}{e^{x^2}}$$ So when I applied L'Hospital's rule, then its value was 0. And I thought that No matter how big $k$ is,  $$\lim_{x \to \infty} \frac{x^k}{e^x}$$ will be equal to zero because exponential's increase speed is much faster than polynomial's. Definitely, we can also apply L'hospital's rule at here, but I think it is not proper qualitative explanation for the fact that exponential is much bigger than polynomial. Is there any other approach which explains why  about this problem? (In fact, I tried to use $\epsilon-\delta$ but how can i show that there exist some $M$ s.t. for every $M then  $x^k<\epsilon e^x$?) (And I also tried to use inequality like $e^x>x+1$  for all $x>0$ but it only worked for $k<1$)	limits,derivatives,exponential-function,epsilon-delta,differential
A.259	Why does $n^c$ grow faster than $2^n$?	For every finite case, I can find a $c$ where $2^n = n^c$, so why is this true? $$\lim_{n \rightarrow \infty} \frac{2^n}{n^c} = 0$$ From the finite cases it seems like $2^n$ grows faster because we can find a $c$ to match it at any $n$.	limits
A.260	limit with two variables	The limit I need to calculate is $\lim_{(x,y)\rightarrow (0,0)}\frac{xy^{2}}{x^{4}+y^{2}}$. Using polar coordinates I get: $lim_{r\rightarrow 0}\frac{r\cos(\theta)\sin^{2}(\theta)}{r^{2}\cos^{4}(\theta)+\sin^{2}(\theta)}$. Now if $\sin(\theta)\neq 0$ then the limit is $0$. How do I handle the case where $\theta=0$ or $\theta = \pi$? And is there a better way to approach this limit?	limits,multivariable-calculus
A.261	Show that $\det(T_n)=\sum_{k=0}^{n}\alpha^{n-k}\beta^k$	"Let the matrix $T_n\in M(n\times n,\mathbb{F})$, where $\mathbb{F}$ denotes a field, be defined by $T_n=(t_{ij})$ with $$t_{ij}= \begin{cases}        \alpha\beta &  1\leq i\leq n-1,\;j=i+1 \\       \alpha+\beta & 1\leq i\leq n,\; j=i \\       1 & 2\leq i\leq n,\; j=i-1 \\       0 &\textrm{otherwise}    \end{cases} $$ Show that  $$\det(T_n)=\sum_{k=0}^{n}\alpha^{n-k}\beta^k$$  My approach: I tried a proof via induction and while the basis step is trivial, I can't seem to solve the induction step since the matrix is never in upper or lower triangular form but always a block matrix, which makes this seemingly difficult as when calculating the determinant of block matrices, one usually calculates the product of all the ""diagonal blocks"". I would very much appreciate help, thank you very much."	linear-algebra,matrices,induction,determinant
A.262	Positve part and negative part of a real number	Let $a$ be a real number . The positive part of $a$, denoted by $a^+$ is given by expression $$a^+ = \text{if } a\geq 0 \text{ then $a$ else } 0$$ The negative part of $a$, denoted by $a^-$ is given by expression $$a^- = \text{if } a\geq 0 \text{ then $0$ else } -a$$ Both $a^+$ and $a^-$  are non negative and the following relationship hold $$ a = a^+ - a^-$$ Above is the text from my compiler optimization book and I cannot understand the relationship explained. How can $a$ be a real number and have positive and negative parts?	linear-programming
A.263	Formal relationship between rules of inference and the material conditional	I am not $100\%$ clear as to what constitutes the difference between a rule of inference and the material conditional, at least in classical logic. I am using the truth-functional definition of the material conditional, commonly visualised through its truth table, but I'm not entirely sure what the formal definition of a rule of inference is. The wikipedia article defines it to be a particular kind of logical form, which seems to be a term from philosophical logic that I'm not familiar with, but reading that article didn't really answer my question. It pertains more to the mathematical side of things, and I am specifically interested in the interplay between the concepts on the syntactic and semantic level. As far as I can tell, any rule of inference can be 'captured' by a corresponding material conditional: if we take modus ponens as a well-known example, what is the difference between $$(a\land (a\to b))\to b$$ and $${a\to b,\text{ } a \over b}?$$ On a functional level, both statements seem to be expressing the same thing. What determines the need to use two separate terms and notations, and what, if anything, separates them?	logic,soft-question,formal-systems
A.264	Modulo Power Arithmetic	While performing some arithmetic operations, i am stuck at one point.I want to know is it possible to write ($a^b)\%p$ as ($a^{(b\%p)})\%p$?	modular-arithmetic
A.265	How to show that $(a/b)^2$ in this situation cannot be an integer	Let $a$,$b$ be relatively prime integers which are both greater than or equal to $2$ . Show that $(a/b)^2$ cannot be an integer. I have tried to use contradiction to prove this result, but I am not sure that my idea is correct or not. Below is my idea (not a complete proof). Suppose $(a/b)^2$ be an integer. We have $a/b$ is an integer. Then, we have $a=bk$ where $k$ is an integer since $b$ divides $a$. Since $a,b$ are relatively prime, therefore $b$ must be $1$. Hence, contradiction arises. I am not sure that my idea is correct or not, could anyone help me to check it? If my idea is wrong, could you give me a idea to do this question?	number-theory,gcd-and-lcm
A.266	What happens when we (incorrectly) make improper fractions proper again?	"Many folks avoid the ""mixed number"" notation such as $4\frac{2}{3}$ due to its ambiguity. The example could mean ""$4$ and two thirds"", i.e. $4+\frac{2}{3}$, but one may also be tempted to multiply, resulting in $\frac{8}{3}$. My questions pertain to what happens when we iterate this process -- alternating between changing a fraction to a mixed number, then ""incorrectly"" multiplying the mixed fraction. The iteration terminates when you arrive at a proper fraction (numerator $\leq$ denominator) or an integer. I'll ""define"" this process via sufficiently-complicated example: $$\frac{14}{3} \rightarrow 4 \frac{2}{3} \rightarrow \frac{8}{3} \rightarrow 2 \frac{2}{3} \rightarrow \frac{4}{3} \rightarrow 1\frac{1}{3}\rightarrow \frac{1}{3}.$$  Does this process always terminate?  For which $(p,q)\in\mathbb{N}\times(\mathbb{N}\setminus\{0\})$ does this process, with initial iterate $\frac{p}{q}$, terminate at $\frac{p \mod q}{q}$?"	number-theory,elementary-number-theory,recreational-mathematics,fractions
A.267	Dual of Lagrange Dual	For linear programming, it's well known that the dual of the dual is the primal. I'm wondering if it is the case for Lagrange duality, and I'm having a hard time showing this. Notationally, let the primal problem be: $$\text{minimize } \quad f_0(x)$$ $$\text{subject to } \quad f_i(x) \leq 0, \quad i = 1, \dots, m$$ And the dual be: $$\text{minimize } \quad -g(\lambda) = - \inf_x L(x, \lambda)$$ $$\text{subject to } \quad -\lambda \leq 0$$ Where $L(x,\lambda) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x)$ is the Lagrangian. I suspect it isn't true in general that the dual of dual is the primal. However, intuitively when I hear the term dual I assume that the dual of the dual should be the primal, so this got me confused.	optimization,convex-optimization,linear-programming,duality-theorems
A.268	If $Z\thicksim\text{Poisson}(\lambda)$, find the expected value of $\frac{1}{1+Z}.$	The Problem: Suppose that $Z\thicksim\text{Poisson}(\lambda)$. Find the expected value of $\dfrac{1}{1+Z}.$ We have that \begin{align*} E\left[\frac{1}{1+Z}\right]&=\sum_{k=0}^\infty\frac{1}{1+k}\frac{e^{-\lambda}\lambda^k}{k!}=e^{-\lambda}\sum_{k=0}^\infty\frac{\lambda^k}{(k+1)!}=\frac{e^{-\lambda}}{\lambda}\left[\sum_{k=0}^\infty\frac{\lambda^k}{k!}-1\right]\\ &=\frac{e^{-\lambda}}{\lambda}[e^\lambda-1]=\frac{1}{\lambda}-\frac{e^{-\lambda}}{\lambda}, \end{align*} where we used the Taylor series for the exponential function.  Do you agree with my approach above? Any feedback is most welcomed. Thank you very much for your time.	probability,solution-verification,expected-value,poisson-distribution
A.269	conditional probability in a family	Let a family have two children. It is known that one of the children is a boy. What is the probability that both the children are boys. So for this we build the sample space $S=\{(b,b)(b,g)(g,g)\}$ Let our event E be the case where both children are boys $E=\{ (b,b) \}$ Let the conditional be F $F=\{(b,g),(b,b)\}$ Hence $P(E|F)=\frac{P(E\cap F)}{P(F)}=\frac{1/3}{2/3}=\frac{1}{2}$ But the answer in my book is given as $\frac{1}{3}$ and I can't seem to understand why.	probability,conditional-probability
A.270	Expected number of heads before it turns up tails five times	A fair coin is flipped repeatedly until it turns up tails five times. What is the expected number of heads before that happens? Based on the link given in the comment, I have found it can be solved using the recursion $E(n)=\frac{1}{2}(E(n)+1)+\frac{1}{2}(E(n-1))$ which is equivalent to $E(n)=E(n-1)+1$. Is it correct?	probability,expected-value
A.271	Probability of $\limsup_{n\to \infty} \{X_n X_{n+1}>0\}$ where $\{X_n\}$ are independent Gaussian r.v.'s with mean 0	Let $\{X_n\}$ be a sequence of independent Gaussian random variables with $\mathbb{E}\, X_n = 0$ for all $n \geq 1$. Find the probability of the event $$ \limsup_{n\to \infty} \big\{ X_n X_{n+1}> 0 \big\} $$ My first thought is that it should be 1 since Gaussians are always positive for a finite value. I was thinking of applying Borel-Cantelli and was trying something along the lines of \begin{align*} \mathbb{P} \big( \limsup_{n\to \infty} \big\{ X_n X_{n+1}> 0 \big\}\big) &= \mathbb{P}\big( X_n X_{n+1} > 0 \,\,\, i.o. \big) \\ &\leq \mathbb{P}\big( \big\{ X_n X_{n+1}> 0 \,\,\, i.o \big\} \cap \big\{ X_{n+1} > 0 \,\,\, i.o\big\} \big)\\ &= \mathbb{P}\big( \big\{ X_n X_{n+1}> 0 \,\,\, i.o \big\}\big) \,\,\mathbb{P}\big( \big\{ X_{n+1} > 0 \,\,\, i.o\big\} \big) \,\,\,\, \text{(by independence)} \end{align*} I'm not sure I'm thinking of this problem right, though.	probability-theory,random-variables,borel-cantelli-lemmas
A.272	Why is $\sqrt{ab}$ = $\sqrt{a}\sqrt{b}$ not true when a and b are both negative?	Apparently $\sqrt{ab}$ = $\sqrt{a}\sqrt{b}$ is only true if a and b are both positive or if a is negative and b is positive or if a is positive and b is negative. In other words, a and b can't both be negative. Is it possible to algebraically prove this? Or is it just a result of the way the square root function is defined?  I know of 1 way to prove this radical property, but I'm still not sure why it won't work for negative numbers. Let x = $\sqrt{ab}$. Let y = $\sqrt{a}\sqrt{b}$ Square both sides for both equations. $x^2 = (\sqrt{ab})^2 = ab$ $y^2 = (\sqrt{a}\sqrt{b})^2 = (\sqrt{a}\sqrt{b})(\sqrt{a}\sqrt{b}) = (\sqrt{a})^2(\sqrt{b})^2 = ab$ $\therefore x^2 = y^2$ $x^2-y^2=0$ $(x+y)(x-y)=0$ $\therefore x = y$ or $x = -y$ Or $\therefore y = x$ or $y = -x$  A lot of people will go through this line of reasoning (shown below) in order to justify why a and b can't both be negative.  Considering that mathematicians define $i^2=-1$ or $i = \sqrt{-1}$ $1 = \sqrt{(-1)(-1)} = \sqrt{-1}\sqrt{-1} = (i)(i) = i^2 = -1  $ But this is only a specific instance where this property fails us. This isn't a  rigorous or at least satisfying proof of why $\sqrt{ab}$ = $\sqrt{a}\sqrt{b}$ can only be true if a and b are not both negative. Note: I just started learning about complex and imaginary numbers and I am no means an expert in  mathematical proofs, so if you do know the answer to this question please try (if possible) your best to answer the question without using too much complex or high-order math that I won't be able to understand.	radicals
A.273	Can fractional/decimal radicals/roots exist?	"For questions like ""What is the 1/2th root of x would the answer be $x^2$? My logic is that since $$ \sqrt[\cfrac{1}{2}]{x}=x^{1/{(\cfrac{1}{2}})} $$ Which simplifies to $x^2$. So as a general rule it could be $$ \sqrt[\cfrac{1}{a}]{x}=x^{1/{(\cfrac{1}{a}})} =x^a $$ And with a different denominator $$\sqrt[\cfrac{b}{a}]{x}=x^{1/{(\cfrac{b}{a}})} =x^{\cfrac{a}{b}}$$ This corresponds to how decimal/fractional exponents denote radicals (their inverse) while fractional radicals are easier shown with exponents. Example : (2/3rd root of 4) $$\sqrt[\cfrac{2}{3}]{4}=4^{1/{(\cfrac{2}{3}})} =4^{\cfrac{3}{2}}= 8$$ Example (22/7th root of π) : $$\sqrt[\cfrac{22}{7}]{π}=π^{1/{(\cfrac{22}{7}})} =π^{\cfrac{7}{22}} \approx 1.439$$ Example (1/2th root of 1/4) : $$\sqrt[\cfrac{1}{2}]{\cfrac{1}{4}}=\cfrac{1}{4}^{1/(\cfrac{2}{1})} =\cfrac{1}{4}^{(\cfrac{2}{1})} =\cfrac{1}{4}^{2} =\cfrac{1}{16} $$"	radicals,decimal-expansion,radical-equations
A.274	Why the Heaviside distribution $H$ doesn't belong to any Sobolev space $H^{s}(\mathbb{R})$	I prooved that the Dirac distribution $\delta_{0}$ is in the Sobolev space $H^{s}\left(\mathbf{R}^{n}\right)=\left\{f \in \mathcal{S}^{\prime}\left(\mathbf{R}^{n}\right)\left|\left(1+|\xi|^{2}\right)^{s / 2} \mathcal{F} f \in L^{2}\left(\mathbf{R}^{n}\right)\right\}\right.$ for every  $s<-n / 2$  but I steel  wrestling to proof that the  Heaviside distribution $H$  $\forall x \in \mathbb{R}, H(x)=\left\{\begin{array}{lll}{0} & {\text { si }} & {x<0} \\ {1} & {\text { si }} & {x \geq 0}\end{array}\right.$ Doesn't belong to any Sobolev space $H^{s}(\mathbb{R})$, could you elaborate on that? Thanks in advance!	real-analysis,functional-analysis,fourier-analysis,sobolev-spaces,distribution-theory
A.275	Proving an Integral Identity with Increasing Bounds	How can I show that  $$ \lim_{A\rightarrow \infty} \int_0^A \frac{\sin(x)}{x}dx\;=\;\frac{\pi}{2}?$$ I know that can use the fact that, for $x>0$,  $$x^{-1}\;=\;\int_0^\infty e^{-xt}dt$$ but I'm not sure how to begin.	real-analysis,integration,analysis
A.276	Let $k\in\mathbb{N}$ and $a>1$. Show that $\lim_{n\to\infty} \frac{n^k}{a^n}=0$.	I think what I need to do is find the value of $n$ where $n^k. I know that this value occurs whenever $n>k\log_an$, however I don't understand how to interpret this result into a general $N$ to pick as a maximum for the sequence convergence. What am I missing here?	real-analysis
A.277	Is the AM-GM inequality the only obstruction for getting a specific sum and product?	This might be silly, but here it goes. Let $P,S>0$ be positive real numbers that satisfy $\frac{S}{n} \ge \sqrt[n]{P}$.  Does there exist a sequence of positive real numbers $a_1,\dots,a_n$ such that $S=\sum a_i,P=\prod a_i$?  Clearly, $\frac{S}{n} \ge \sqrt[n]{P}$ is a necessary condition, due to the AM-GM inequality. But is it sufficient? For $n=2$, the answer is positive, as can be seen by analysing the discriminant of the associated quadratic equation. (In fact, the solvability criterion for the quadratic, namely- the non-negativity of the discriminant, is equivalent to the AM-GM inequality for the sum and the product). What about $n>3$?	real-analysis,polynomials,systems-of-equations,a.m.-g.m.-inequality
A.278	Is there a differentiable function such that $f(\mathbb Q) \subseteq \mathbb Q$ but $f'(\mathbb Q) \not \subseteq \mathbb Q$?	Is there a differentiable function $f:\mathbb R \rightarrow \mathbb R$ such that $f(\mathbb Q) \subseteq \mathbb Q$, but $f'(\mathbb Q) \not \subseteq \mathbb Q$? A friend of mine asserted this without giving any examples. I seriously doubt it, but I had hard time trying to disprove it since analysis isn't really my thing. I can't even think of any class of differentiable functions with $f(\mathbb Q) \subseteq \mathbb Q$ other than the rational functions.	real-analysis
A.279	If $\lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2,$ show that $\lim_{x\to 0}f(x)=1$.	Question: Suppose $f:(-\delta,\delta)\to (0,\infty)$ has the property that $$\lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2.$$ Show that $\lim_{x\to 0}f(x)=1$.  My approach: Let $h:(-\delta,\delta)\to(-1,\infty)$ be such that $h(x)=f(x)-1, \forall x\in(-\delta,\delta).$ Note that if we can show that $\lim_{x\to 0}h(x)=0$, then we will be done. Now since we have $$\lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2\implies \lim_{x\to 0}\frac{(f(x)-1)^2}{f(x)}=0\implies \lim_{x\to 0}\frac{h^2(x)}{h(x)+1}=0.$$ Next I tried to come up with some bounds in order to use Sandwich theorem to show that $\lim_{x\to 0} h(x)=0,$ but the bounds didn't quite work out. The bounds were the following: $$\begin{cases}h(x)\ge \frac{h^2(x)}{h(x)+1},\text{when }h(x)\ge 0,\\h(x)<\frac{h^2(x)}{h(x)+1},\text{when }h(x)<0.\end{cases}$$ How to proceed after this?	real-analysis,calculus,limits
A.280	Why do engineers use derivatives in discontinuous functions? Is it correct?	"I am a Software Engineering student and this year I learned about how CPUs work, it turns out that electronic engineers and I also see it a lot in my field, we do use derivatives with discontinuous functions. For instance in order to calculate the optimal amount of ripple adders so as to minimise the execution time of the addition process: $$\text{ExecutionTime}(n, k) = \Delta(4k+\frac{2n}{k}-4)$$ $$\frac{d\,\text{ExecutionTime}(n, k)}{dk}=4\Delta-\frac{2n\Delta}{k^2}=0$$ $$k= \sqrt{\frac{n}{2}}$$ where $n$ is the number of bits in the numbers to add, $k$ is the amount of adders in ripple and $\Delta$ is the ""delta gate"" (the time that takes to a gate to operate). Clearly you can see that the execution time function is not continuous at all because $k$ is a natural number and so is $n$. This is driving me crazy because on the one hand I understand that I can analyse the function as a continuous one and get results in that way, and indeed I think that's what we do (""I think"", that's why I am asking), but my intuition and knowledge about mathematical analysis tells me that this is completely wrong, because the truth is that the function is not continuous and will never be and because of that, the derivative with respect to $k$ or $n$ does not exist because there is no rate of change. If someone could explain me if my first guess is correct or not and why, I'd appreciate it a lot, thanks for reading and helping!"	real-analysis,calculus,functions,derivatives,optimization
A.281	Alternate methods to prove that $n^{\frac{1}{n}} \rightarrow 1$ where $n \in \mathbb{N}$	So, I was trying to prove that $n^{\frac{1}{n}} \rightarrow 1$ where $n \in \mathbb{N}$. Here's what I did: Since $n^{1/n}>1$, let us suppose $n=(1+k)^n$ for $n>1$ and some $k>0$. Now, I used the binomial expansion and wrote: $$\begin{align} n&=1+nk+\frac{n(n-1)}{2}k^2+....+k^n \geq1+\frac{n(n-1)}{2}k \\ &\Rightarrow n-1 \geq \frac{n(n-1)}{2}k^2 \\ &\Rightarrow k^2 \leq \frac{2}{n} \end{align}$$ So now, we can always find an $n>0$ such that $\frac{2}{\epsilon^2}. Now, as $|n^{\frac{1}{n}}-1|\geq0$, we have, $n^{\frac{1}{n}}-1=k\leq(\frac{2}{n})^{\frac{1}{2}}<\epsilon$ And in this way, I proved that $n^{\frac{1}{n}}\rightarrow1$. I wanted to know how can I prove this without using Binomial expansion. I tried to do this using the Bernoulli's Inequality,  but couldn't get too far. Any help/hint would be highly appreciable.	real-analysis,sequences-and-series,convergence-divergence
A.282	How to prove the value o $\zeta(2)$ by Functional Analysis?	I have a question about $\sum_{n=1}^{\infty} 1/n^2$ = $\pi^2/6$ I know it can be proven with standard 1 variable analysis (working on Taylor series of $\arcsin$ or something like that) or basic complex analysis. But someone told me it has also great prove using Functional Analysis. He suggested it is proved on standard first course of FA, but it was not in my case. Can You write here the proof using FA or leave a link in comment? Thanks in advance.	real-analysis,complex-analysis,functional-analysis,sums-of-squares
A.283	Determine whether $(x_n)$ converges or diverges	Given $x_1 := a > 0$ and $x_{n+1} := x_n + \frac{1}{x_n}$ for $n \in \mathbb{N}$, determine whether $(x_n)$ converges or diverges.  Since $x_1 > 0$,  it seems obvious that the sequence is strictly increasing and always positive because we are always adding a positive number to each subsequent element in the sequence.  The trickier part is to show whether $(x_n)$ is bounded or not. If $(x_n)$ is bounded, then $\exists M \in \mathbb{R}$ such that  \begin{align}|x_n| \leq M ~\forall n \in \mathbb{N}\tag{1} \end{align} Alternatively, I think this means that $M$ could be a supremum of the sequence and another way to rewrite $(1)$ is given any $\epsilon > 0$ \begin{align} x_n + \epsilon \leq M \tag{2}\end{align} Choose $\epsilon = \frac{1}{x_n}$. Then  \begin{align} x_{n+1} = x_n + \frac{1}{x_n} \leq M \implies x_n + \epsilon \leq M \tag{3}\end{align} Thus I conclude that the inequality holds $\forall n \in \mathbb{N}$ and that $(x_n)$ is convergent. Thus by the Monotone Sequence Convergence Theorem, $(x_n)$ is convergent.  The part I am not sure about is my reasoning to show that $x_n$ being bounded is correct or not.	real-analysis,sequences-and-series,convergence-divergence
A.284	Why decimals of rational numbers behave periodically?	I am interesting in the proof of that every rational number cannot have in decimal form infinite number of digits that don't repeat (or the other way around). So, then is enough to prove following statement: For any $n \in \mathbb{N}$ rational number $\frac{1}{n}$ can be represented in decimal form such that it's digits, if there are infinitely many, are repeating. If this is true, then it is true for any $\frac{m}{n} = \frac{1}{n} + \frac{1}{n} + ... + \frac{1}{n}$ ($m$ times).	real-numbers,rational-numbers,decimal-expansion
A.285	How to determine the sum of a series that is neither geometric nor arithmetic but quadratic or cubic?	How to determine the formula of the sum of a series given its $n$th-term formula like: $$U_n= n^2+n$$ or $$U_n= 6n^2 -12n + 5$$	sequences-and-series
A.286	Knowing$\sum a_{k}$ converges, how to prove that $\lim _{n \rightarrow+\infty} n a_{n}=0$.	Let $a_{n}$ be decreasing and positive. Then $\sum a_{k}$ converges implies $\lim _{n \rightarrow+\infty} n a_{n}=0$. I think since $na_n$ is positive, the only thing to do is to find an upper bound for the sequence. But I don't know how to split the sequence to form the upper bound.	sequences-and-series,limits
A.287	What is the closed form of $\sum_{i=1}^n\frac{2i}{2^i}$	I looked at $\frac{\sum 2i}{\sum2^i}$ (division), however both expressions are not equal. I am looking for an expression like $\sum_{i=1}^n\frac{2i}{2^i}=5n$ for example.	sequences-and-series
A.288	Alternating Harmonic Series Spin-off	We know that the series $\sum (-1)^n/n$ converges, and clearly every other alternating harmonic series with the sign changing every two or more terms such as $$\left(1+\frac{1}{2}+\frac{1}{3}\right)-\left(\frac{1}{4}+\frac{1}{5}+\frac{1}{6}\right)+\left(\frac{1}{7}+\frac{1}{8}+\frac{1}{9}\right)-\cdots$$ must converge. My question here is that does the series below also converge? $$\sum\frac{\textrm{sgn}(\sin(n))}{n}\quad\textrm{or}\quad\sum\frac{\sin(n)}{n|\sin(n)|}$$ Loosely speaking, the sign changes every $\pi$ terms. I'd be surprised if it doesn't converge. Wolfram Mathematica, after a couple of minutes of computing, concluded the series diverges but I can't really trust it. My first approach (assuming the series converges) was that if we bundle up terms with the same sign like the example above every bundle must have three or four terms, and since the first three terms of all bundles make an alternating series I was going to fiddle with the remaining fourth terms but they don't make an alternating series so I guess there's no point in this approach. edit: I don't think we can use Dirichlet's test with $$b_n=\textrm{sgn}(\sin(n)).$$ The alternating cycle here is $\pi$ and I don't believe it would bound the series. For example if the cycle was a number very slightly smaller than $3+1/4$, then $B_n$ (sum of $b_n$) would get larger and larger every four bundles for some time. I believe this should happen for $\pi$ as well since it is irrational. I'm not entirely sure why but $|B_n|\leq3$ for most small $n$ though I guess it's because $\pi-3$ is slightly smaller than $1/7$? Anyway $B_{312\ 692}=4$, $B_{625\ 381}=5$, $B_{938\ 070}=6$, $B_{166\ 645\ 135}=-7$, and $B_{824\ 054\ 044}=8$. $|B_n|$ does not hit $9$ up to $n=1\ 000\ 000\ 000$ with $B_{1\ 000\ 000\ 000}=-2$.	sequences-and-series
A.289	Find $x^n+y^n+z^n$ general solution	If we know $$x+y+z=1$$$$x^2+y^2+z^2=2$$$$x^3+y^3+z^3=3$$ Is it possible to calculate the general solution for $a_n=x^n+y^n+z^n$? I know $a_5=6$ but the way to get it is more an algorithm than an actual solution.	sequences-and-series
A.290	Best method for proving that $7\times11^{2n+1}-3^{4n-1}$ is divisible by $10$	I am asked to prove by induction that $7\times11^{2n+1}-3^{4n-1}$ is divisible by $10$. I wonder whether there is a more direct method, for example factorizing by $10$. If an expression is divisible by $10$, does this mean that I can factorize it by $10$? Thanks in advance	sequences-and-series,elementary-number-theory,divisibility
A.291	Summation of $n$th partial products of the square of even numbers diverges, but for odd numbers they converge in this series I'm looking at. Why?	So I have the two following series: $$\sum_{n=1}^\infty \frac{\prod_{k=1}^n(2k)^2}{(2n+2)!}$$ $$\sum_{n=0}^\infty \frac{\prod_{k=0}^n(2k+1)^2}{(2n+3)!}$$ I figured out the $n$th partial products: $$\prod_{k=1}^n(2k)^2=4^n(n!)^2$$ $$\prod_{k=0}^n (2k+1)^2=\frac{((2n+1)!)^2}{4^n(n!)^2}$$ So putting these back into my series they become the following: $$\sum_{n=1}^\infty \frac{\prod_{k=1}^n(2k)^2}{(2n+2)!}=\sum_{n=1}^\infty\frac{4^n(n!)^2}{(2n+2)!}$$ Now this diverges as expected by the limit test test. However when I look at my other series: $$\sum_{n=0}^\infty \frac{\prod_{k=0}^n(2k+1)^2}{(2n+3)!}=\sum_{n=0}^\infty\frac{( (2n+1)!)^2}{4^n(n!)^2(2n+3)!}$$ By the limit test maybe diverges or maybe doesn't, and the ratio test is inconclusive. Since I wasn't sure what to use for the a comparison test I threw this into wolfram alpha and it told me it converges which is baffling to me since both series are very similar if we write them out: $$\sum_{n=1}^\infty \frac{\prod_{k=1}^n(2k)^2}{(2n+2)!}=\frac{2^2}{4!}+\frac{2^24^2}{6!}+\frac{2^24^26^2}{8!}\cdot\cdot\cdot\cdot$$ $$\sum_{n=0}^\infty \frac{\prod_{k=0}^n(2k+1)^2}{(2n+3)!}=\frac{1^2}{3!}+\frac{1^23^2}{5!}+\frac{1^23^25^2}{7!}+\cdot\cdot\cdot$$ They both have the nth parial product of the even/odd integers squared in the numerator, and are over a factorial that is two greater than $n$, so I'm not sure why one is diverging and the other is converging. Is wolframalpha wrong, as it can be at times? Or is there someething here that I am missing?	sequences-and-series,factorial,products
A.292	Find the limit of the sequence $ \frac{x^n}{n^k}$ as $n \to \infty$ for all values of$ x > $0 and $k = 1, 2,\cdots$	I have tried using the ratio lemma to tackle this question and also the fact $(n+1)^k \geq 1 + nk$ and I haven't reached an answer. How should I go about solving this problem?	sequences-and-series,limits,analysis,exponentiation,ratio
A.293	Summation $\sum_{j=2}^{n-1}j^2$ Properties	I'm dealing with something like $\sum_{j=2}^{n-1}j^2$. I know I can do this $\sum_{j=1}^{n-1}j^2 - \sum_{j=1}^{1}j^2$. Would that be equal to $\frac{j(j+1)(2j+1)}{6} - j^2$ or I'm missing some properties with $n-1$? If so, which ones?	sequences-and-series,summation
A.294	Why is there no hyper-hypercohomology?	"I am looking for a reference to answer the question in the title. Let me try to clarify a little what I mean: If a single sheaf $\mathscr F$ has a resolution $\mathscr G^\bullet$ by not necessarily injective objects, then the usual cohomology of $\mathscr F$ is isomorphic to the hypercohomology of $\mathscr G^\bullet$:  $$ H^i(X, \mathscr F) \cong \mathbb H^i(X,\mathscr G^\bullet). $$ Now, if one was starting with a complex of sheaves $\mathscr F^\bullet$ and a ""resolution"" thereof, i.e. a complex of complexes $(\mathscr G^\bullet)^\bullet$, then one should touch on a concept that could be called hyper-hypercohomology.  Yet, I never heard of its existence and I'm pretty sure it does not give you anything new, as soon as you work in the derived category. I just find myself unable to pin down why exactly this is the case.  Any ideas anyone?"	sheaf-cohomology,derived-categories
A.295	Help calculating series	I need help with understanding how to solve this task, because I'm a bit lost at the moment. Use the powerseries  $$f(x)=\frac{1}{1-x}$$ to decide the sum of the series  $\sum_{n=1}^{\infty} n(n+1)x^n$    and $\sum_{n=1}^{\infty} \frac{n(n+1)}{3^n}$ I don't understand how to manipulate the sums to use the power series of the function.	summation,power-series
A.296	A little confused about the Taylor series of $e^x$	We know that $$ e^x=\sum_{n=0}^{\infty}\frac{x^n}{n!},x\in \mathbb R, $$ which can be written out $$ e^x=\frac {x^0}{0!}+\frac {x^1}{1!}+\frac {x^2}{2!}+\cdots, $$ but $0^0$ isn't well defined.	taylor-expansion,exponential-function
A.297	difficult question	I am confused about a homework problem I have, and don't really know where to begin. I need to prove this. Any idea of where I can start. The statement is that Find all integers n that satisfies $\phi(n)=320$ where $\phi$ is the Euler’s Phi function.	totient-function
A.298	Variable transformation of a Dirac delta function	I am struggling to understand the variable transformation of a Dirac delta function. More specifically, a transformation of the following type, $$\delta(a\chi(z)-b) \rightarrow \delta(z-c)$$ Here, $a, b$ and $c$ are constants. The specific relationship between $\chi(z)$ and $z$ is, $$\chi(z)=\int{\frac{1}{H(z)}dz}$$ where $H(z)$ is a non-zero, positive and smoothly varying function of $z$. In the context of my Physics problem, for the sake of the interested audience, $H(z)$ is the Hubble parameter of the Universe while $\chi(z)$ is the comoving distance and $z$ is the redshift of any time in the past. So, let me add what I have done so far:  I start by defining the Dirac delta function in the form a unit step function $\Theta (a\chi(z)-b)$as, $$\frac{d}{d(a\chi(z))}\Theta(a\chi(z)-b)=\delta(a\chi(z)-b)$$ Then converting this in the form of z using the chain rule as, $$\frac{d}{dz}\Theta(a\chi(z)-b)\frac{dz}{d(a\chi(z))}$$ Using the relation with $H(z)$, we can write the equation as, $$\frac{d}{dz}\Theta(a\chi(z)-b)\frac{H(z)}{a}$$ Also, $\chi(z)$ can also be replaced by the integral as well, so that left side containing the unit step function can be written as, $$\frac{H(z)}{a}\frac{d}{dz}\Theta(\ a\int_0^z{\frac{1}{H(z')}dz'}-b)$$. After this, I am not sure what else to try. Hopefully, this addition helps. Also, please point out an error if you see one.	transformation,dirac-delta,change-of-variable
A.299	Prove $\cos \frac{\pi}5-\cos \frac{2 \pi}5=\frac12$ but without finding $\cos \frac{ \pi}5$	I can find the value of $\cos \left(\frac{ \pi}{5}\right)$, but is there a way to prove the equality without finding it? I tried looking for both algebraic and geometric methods, but couldn't find anything	trigonometry,proof-writing,alternative-proof
A.300	Uniformly continuous or not?	So I supposed to find out if $$f(x)=\frac{1}{1+\ln^2 x}$$ is uniformly continuous on $I=(0,\infty)$ So I have been thinking a lot. Could I say that $f$ is continuous on $[0,1]$ and therefore uniformly continuous here? Or is this not valid, because $\ln$ is not defined at $x=0$? And then say that the derivate is bounded at $[1,\infty]$?	uniform-continuity
A.301	Inequality between norm 1,norm 2 and norm $\infty$ of Matrices	Suppose $A$ is a $m\times n$ matrix. Then Prove that, $\begin{equation*} \|A\|_2\leq \sqrt{\|A\|_1 \|A\|_{\infty}} \end{equation*}$ I have proved the following relations: $\begin{align*} \frac{1}{\sqrt{n}}\|A\|_{\infty}\leq \|A\|_2\leq\sqrt{m}\|A\|_{\infty}\\ \frac{1}{\sqrt{m}}\|A\|_{1}\leq \|A\|_2\leq\sqrt{n}\|A\|_{1} \end{align*}$ Also I feel that somehow Holder's inequality for the special case when $p=1$ and $q=\infty$ might be useful.But I couldn't prove that. Edit: I would like to have a prove that do not use the information that $\|A\|_2=\sqrt{\rho(A^TA)}$ Usage of inequalities like Cauchy Schwartz or Holder is fine.	linear-algebra,matrices,inequality,norm,holder-inequality
A.302	$n$-th root of a complex number	I am confused about the following problem. With $w=se^{i{\phi}}$, where $s\ge 0$ and $\phi \in \mathbb{R}$, solve the equation $z^n=w$ in $\mathbb{C}$ where $n$ is a natural number. How many solutions are there? Now my approach is simply taking the $n$-th root which gives $$z=\sqrt[n]{s}e^{\frac{i\varphi}{n}}$$ However, it seems that this problem is asking us to show the existance of the $n$-th root. Can I assume that the $n$-th root of a complex number already exists? Moreoover, would I be correct to say that there is only one solution which is given above?	real-analysis,calculus,complex-analysis,analysis
A.303	Number of non-commutative Lie algebras of dimension 2	Theorem- Up to isomorphism, the only noncommutative Lie algebra of dimension 2 is that with basis $x , y$ and bracket determined by $[x,y] = x$. I understand that all vector spaces of dimension 2 over the field $K$ are isomorphic to each other. So the number of lie algebras of dimension 2 in a field $K$ is determined by the number of possible bilinear operations [ ]$:\ V \ X \ V  \rightarrow V$ satisfying the conditions $a)$ $[x,x]=0$ for all $x\in V$ $b)$ $[x,[y,z]]+[y,[z,x]]+[z,[x,y]]=0$ for all $x,y,z \in V$ The bilinear operations on the other hand is determined by the elements to which the pair of base elements are mapped to in the bilinear operation. And since in a lie algebra $[x,x]=[y,y]=0$ and $[x,y]=-[x,y]$ we ony need to determine $[x,y]$. Now how do we prove that $[x,y]=x$ and $[y,x]=-x$ always and why can't it be [y,x]=y or any other vector ?	lie-algebras,noncommutative-algebra
A.304	Elementary proof that a non-orientable manifold of real dimension $2$ does not admit a quasi-complex structure.	Is there an easy proof that a non-orientable real surface $X$ does not admit a quasi-complex structure? The proof I know is to observe that any quasi-complex structure on a real surface $X$ necessarily satisfies the integrability condition $$[T_X^{0,1}, T_X^{0,1}] \subset T_X^{0,1}$$ of the Newlander-Nirenberg theorem, because $T_X^{0,1}$ is a $1$-dimensional complex vector bundle, and the bracket $[-,-]$ is alternating, i.e. it vanishes on $T_X^{0,1}$. So by the Newlander-Nirenberg theorem, $X$ admits a complex structure, and complex manifolds have to be orientable. However, the Newlander-Nirenberg theorem is a deep theorem and feels a bit overkill. Also I don't really see why there cannot be a quasi-complex structure. Is there a more elementary proof to convince myself?	algebraic-geometry,reference-request,complex-geometry
A.305	What will be the value of floor function of $\lim\limits_{N\to\infty}\left\lfloor\sum\limits_{r=1}^N\frac{1}{2^r}\right\rfloor$	What would be the value of floor function of $\lim\limits_{N\to\infty}\left\lfloor\sum\limits_{r=1}^N\frac{1}{2^r}\right\rfloor$ would it be $1$ or would it be $0$ ? The formula I use for this is that of infinite summation series that is $\frac{a}{1-r}$ but I have no clue how to find out what the floor value of the above expression would be. P.s I am a high school student so please explain in simple terms, and yes I do know basic calculus. EDIT: I'm sorry it was given $\lim_{N \to \infty}$ in the problem	summation
A.306	On the irrationality of Euler Mascheroni constant	I saw one of the expansions of Euler Mascheroni constant in terms of Meissel Mertens constant as a consequence of Mertens theorem. $$ B = \gamma + \sum_p \left\{ \log\left( 1 - \frac 1p\right) + \frac 1p\right\}$$ This is that expansion. Now I don't understand why is it difficult to prove the irrationality of Euler Mascheroni constant. Since we have infinitely many prime numbers, the sum over all those primes in the above equation, if converges must be a irrational, then why is it not considered as a proof of irrationality?	irrational-numbers,euler-mascheroni-constant
A.307	Carmichael function and the largest multiplicative order modulo n	By definition, the Carmichael function maps$a $positive integer $n$ to the smallest positive integer $t$ such that $a^t\equiv1\pmod n$ for all integers $a$ with $\gcd(a,n)=1$. It is denoted as $\lambda(n)$. The Wikipedia page on Carmichael function states that $\lambda(n)=\max\{\operatorname{ord}_n(a):\gcd(a,n)=1\}$. My question is: why is this true? In other words, why is it the case that there always exists an integer $x$ coprime to $n$ with $\operatorname{ord}_n(x)=\lambda(n)$?	elementary-number-theory,carmichael-function
A.308	Riemann's definition of the zeta function	"I am having trouble understanding Riemann's definition of the zeta function, and I will need to give a brief summary here before I can get to my question. In his 1859 paper, Riemann derived the integral representation $$\zeta(s)=\sum_{n=1}^\infty\frac{1}{n^s}=\frac{1}{\Gamma(s)}\int_0^\infty \frac{x^{s-1}}{e^x-1}dx$$ that is valid for $\mbox{Re}(s)\gt 1$, and then modified the integral in order to define a function that is defined for all complex values of $s$, except $s=1$, where it has a simple pole. The extension is given by $$\zeta(s)=\frac{\Gamma(1-s)}{2\pi i}\int_C \frac{(-z)^s}{e^z-1}\frac{dz}{z}$$ where $C$ is a ""Hankel contour"", that is, a path that travels from $+\infty $ at a small distance $\epsilon$ above the positive $x$-axis, circles around the origin once in counterclockwise direction with a small radius $\delta$, and returns to $+\infty$ traveling at distance $\epsilon$ below the positive real axis. Taking the limit as $\epsilon\rightarrow 0$ and $\delta \rightarrow 0$ one can see that the integral $$\int_C \frac{(-z)^s}{e^z-1}\frac{dz}{z}$$ becomes $$(e^{i\pi s}-e^{-i\pi s})\int_0^\infty\frac{x^{s-1}}{e^x-1}dx$$ and then the rest follows easily from known identities satisfied by the Gamma function.  While the original real integral over $[0,\infty)$ is clearly divergent if $\mbox{Re}(s)\leq 1$, the contour integral over $C$ is defined for all complex $s$, because the path stays away from the singularity at $s=0$ and from the branch cut along the positive $x$-axis. My problem is understanding why the integral over $C$ does not depend on $\epsilon$ and $\delta$, so that we can keep them at a safe positive distance from the singularities for the definition, but we can take the limit for the purpose of evaluating the integral. I know that by Cauchy's theorem we can modify a path of integration (without changing the value of the integral) starting and ending at the same point as long as we do not cross any singularity, but this path starts and ends at infinity, so I am not sure how to rigorously proceed using Cauchy's theorem. Even if I start the path at $R+i\epsilon$ and end it at $R-i\epsilon$ for some large $R$, the path starting and ending points change as $\epsilon$ changes."	contour-integration,riemann-zeta
A.309	Number of solutions of equation over a finite field	I have a question regarding the number of solutions of a equation over a finite field $\mathbb{F}_p$. First of all, consider the equation $x^3=a$ over $\mathbb{F}_p$, where $p$ is a prime such that $p\equiv 2 (\text{mod }3)$. The book that I'm currently reading says that this equation has exactly one solution in $\mathbb{F}_p$ for every $a\in \mathbb{F}_p$, because $\gcd(3,p-1)=1$, but the book does not prove this. Unfortunately, this doesn't convince me enough. Is there is a convincing elementary straightforward proof justifying why is this true?	number-theory,elementary-number-theory,finite-fields
A.310	Finding positive integer solutions to $\frac{4}{x}+\frac{10}{y}=1$	Find the positive integer solutions for: $\frac{4}{x} + \frac{10}{y} = 1$  I had calculated the solutions manually but it was a very tedious process. Is there any better way to do this?	elementary-number-theory,solution-verification
A.311	Given a function $f(x)=\frac{9^{x}}{9^x+3}$, what is $f(\frac{1}{27})+ f(\frac{2}{27}) + f(\frac{3}{27})+ ...+ f(\frac{26}{27})$?	While I was going through past Olympiad math papers, I found this question without any explanation. Here is the question:  Given a function $f(x)=\frac{9^{x}}{9^x+3}$, what is $f(\frac{1}{27})+ f(\frac{2}{27}) + f(\frac{3}{27})+ ...+ f(\frac{26}{27})$?  The answer was 13. I took a really bad approach and converted $\frac{9^{x}}{9^x+3}$ to $1+\frac{9^{x}}{3}$, which I then noticed was wrong. I also accidentally multiplied $9^{\frac{1}{27}}$ with $9^{\frac{2}{27}}$, $9^{\frac{2}{27}}$ with $9^{\frac{3}{27}}$, and so on, before realizing that the functions were added and not multiplied. I suspect that there is something to the power of $\frac{n}{27}$, because 9 is a multiple of 27. However, I am not completely sure. Is there a law that tells me how I can solve this question? Since this is a Math Olympiad question, there is probably a maximum time limit of five minutes to do this question. This means that I probably won’t have time for tedious mathematical calculations with a calculator and online tools, or something like that. Please give me a quick, fast solution that is probably suitable for an 8th grader, at most a solution at a 10th grader level.	functions,contest-math,power-series,fractions
A.312	Proving $\left\lfloor \frac{\left\lfloor a/b \right\rfloor}{c} \right\rfloor=\left\lfloor\frac{a}{bc}\right\rfloor$ for positive integer $a$, $b$, $c$	How can we prove the following? $$\left\lfloor \frac{\left\lfloor \dfrac{a}{b} \right\rfloor}{c} \right\rfloor = \left\lfloor \frac{a}{bc} \right\rfloor$$ for $a,b,c \in \mathbb{Z}^+$  I don’t know if I’m doing something wrong, but I can’t prove it even though I’m pretty sure it’s true. Obviously, because the concept of algebra isn’t aware of the fact that we are restricting the variables to positive integers, and given my assumption that the equality doesn’t necessarily hold for non-integers, an element of non-algebraic problem solving is needed, i.e. making a change to the expression given our knowledge of that condition, which then allows for algebraic maneuvers that show that the equality holds. I think that’s what I’m missing. Thanks.	elementary-number-theory,proof-writing,ceiling-and-floor-functions
A.313	Let $a,b\in G$, a finite abelian group and $|a|=r, |b|=s$ with $\gcd(r,s)=1$. Prove that $|ab|=rs$.	Let $a,b\in G$, a finite abelian group and $|a|=r, |b|=s$ with $\gcd(r,s)=1$. Prove that $|ab|=rs$.  My attempt: Let $|ab|=n$. Since $G$ is ableian, $(ab)^n=a^nb^n=1$. Thus $r\mid n$ and $s\mid n$. Together with $\gcd(r,s)=1$, it follows that $rs\mid n$. This is where I'm stuck; need to show that $rs=n$. Any hints on how to proceed? Edit: I've come up with a solution that is a somewhat different approach to what has been provided in the hints. Here it goes: Since $G$ is abelian, $n\mid{\rm lcm}(r,s)$. But since $\gcd(r,s)=1$, ${\rm lcm}(r,s)=rs$ by an elementary result in number theory. Thus $n\mid rs$. Together with $rs\mid n$, we have that $n=rs$, which is what we want to prove.	group-theory,finite-groups,abelian-groups
A.314	Closed span of a sequence in Hilbert spaces.	Suppose that you have an orthonormal basis $\{e_n\}$ in a Hilbert space such that $\sum \|e_n-x_n\| < 1$. Is this condition enough to prove that the closed span of $\{x_n\}$ is $H$? My efforts to prove this have not led anywhere promising. I have tried showing that the only vector perpendicular to all of the $x_n$ would be $0$. Not sure which way I can proceed. Does anyone have an idea how to approach this? Thank you.	functional-analysis,hilbert-spaces
A.315	Backwards Induction (Exercise 2.2.6) Analysis 1 by Terence Tao	"I am new to the study of analysis and I decided to start with Terence's book in my endeavor. I want to show my ""proof"" of backwards induction since I have some difficulty in understanding this. I want to now if my proof is correct or have some error, because if have,$a $can't infer that. Any feedback is appreciated.   Let $n$ be a natural number, and let $P(m)$ be a property pertaining to the natural numbers such that whenever $P(m\text{++})$ is true, then $P(m)$ is true. Suppose that $P(n)$ is also true. Prove that $P(m)$ is true for all natural numbers $m ≤ n$; this is known as the principle of backwards induction. (Hint: apply induction to the variable $n$.)  First i want to show $P(m)$ is true $\forall$ $0\geq m$. H1: $\forall m$ $P(m\text{++})\implies P(m)$ H2: $P(0)$ C: $P(m)$ is true $\forall$ $0\geq m$. $0\geq m$ means $0=m+a$ for some natural number $a$, then $m=a=0$ for corollary 2.2.9. But $P(0)$ is true for H2, then the case $n=0$ is proved. Suppose now that works for $n$ and prove $n\text{++}$. then: H1: $\forall m$ $P(m\text{++})\implies P(m)$ H2: $P(n)\implies P(m)$ $\forall$ $n\geq m$ H3: $P(n\text{++})$. In H1, for $m=n$ we have $P(n\text{++})\implies P(n)$ and for H2 we now $P(n)\implies P(m)$ (specifically for $n=m$), then $P(n\text{++})\implies P(m)$ for $n\text{++}>m$. We need to prove that works for $n\text{++}=m$ but for that $P(n\text{++})$ is true for H3. We conclude that $P(n\text{++})\implies P(m)$ $\forall$ $n\text{++}\geq m$."	real-analysis,solution-verification,induction
A.316	Multiplicative of group of odd numbers modulo power of two	I want to understand the group structure of the group of units in the ring $\mathbb{Z}/2^m \mathbb{Z}$ for positive integers $m$. I expect this has been answered before here on MSE, but so far the automatic suggestions didn't turn it up. Here is what I understand: it is an abelian group, hence a product of cyclic ones. Also the order of this group is $2^{m-1}$ so that the orders of the factors in the product are all powers of two by Lagrange's theorem. Now these conditions are restrictive enough to compute the first few cases by hand. We have $(\mathbb{Z}/2 \mathbb{Z})^* = C_1$, $(\mathbb{Z}/4 \mathbb{Z})^* = C_2$, $(\mathbb{Z}/8 \mathbb{Z})^* = C_2 \times C_2$, $(\mathbb{Z}/16 \mathbb{Z})^* = C_2 \times C_4$, $(\mathbb{Z}/32 \mathbb{Z})^* = C_2 \times C_8$. I'm starting to believe that for $m > 1$ we have that $(\mathbb{Z}/2^m \mathbb{Z})^* = C_2 \times C_{2^{m-2}}$ but is there a simple conceptual explanation for that (if true)?	elementary-number-theory
A.317	$\int \frac{1}{\left(x^2+1\right)^n}dx$	Let be $n\in \mathbb{Z_+}$. Compute the following integral: $$\int \frac{1}{\left(x^2+1\right)^n}dx$$ I obtained that for $$n=1$$ the value of the integral is $$\tan^{-1}x+C$$ and for $$n=2$$ $$x\left(\frac{1}{2\left(x^2+1\right)}+\frac{\tan \:^{-1}}{2x}\right)+C$$ How to do the rest of the cases?	integration
A.318	How do you prove $e^{x} \geq \left(1+\frac{x}{n}\right)^{n}$ for $n \geq 1$	How do you prove $e^{x} \geq \left(1+\frac{x}{n}\right)^{n}$ for $n \geq 1$? I can prove this for natural numbers only via induction, but how do you prove this for any real $n \geq 1$? We start with the base case $n=1$. We have $e^x \geq 1+x$ by a variety of methods. For the induction step, assume $e^{x} \geq \left(1+\frac{x}{n}\right)^{n}$. Notice that taking the derivative of $(1+\frac{x}{n+1})^{n+1}$ gives us $(1+\frac{x}{n+1})^{n}$ and thus $(1+\frac{x}{n+1})^{n} < \left(1+\frac{x}{n}\right)^{n} \leq e^x = \frac{d}{dx} e^x$. I'm not sure how to extend this to the non-integer case. Any help would be appreciated.	calculus
A.319	A wrong argument for $\mathbb{R}$ being countable	We assume $A$ is the set of all countable subsets of the set of real numbers. We know $A$ is a partially ordered set $(A, \subseteq)$. Suppose $$A_1 \subseteq A_2 \subseteq \ldots \subseteq A_n \subseteq A_{n+1} \subseteq \ldots$$ is a chain in $A$. We can prove $B=\bigcup_{n \in \Bbb{N}} A_n$ is a countable set. For each natural number $m$, we have $A_m \subseteq B$. So $B$ is an upper bound for $A$. This shows each chain in $A$ has an upper bound according to Zorn's lemma. $A$ has a maximal element $X$, and we know $X$ is a countable set. Now we prove $X = \Bbb{R}$. If $X \neq \Bbb{R}$, then there is an $x \in \Bbb{R}$ such that $x \notin X$. Let $Y=X \cup \{x\}$. It's obvious that $Y$ is a countable subset of the real numbers and $X \subsetneq Y$. This contradicts $X$ being a maximal element. Thus, $X = \Bbb{R}$ and $\Bbb{R}$ is a countable set. What is wrong with this argument?	elementary-set-theory,cardinals
A.320	An inverse trigonometric integral	So my integral is $$\int_{0}^{1}\frac{\sin^{-1}(x)}{x}$$ To avoid confusion let me re-write the integral as $$\mathcal I = \int_0^1 \frac{\arcsin(x)}{x}$$ I started off with a trig-substitution that is let $x = \sin(t)$ and $t = \arcsin(x)$ which means that $dx = \cos(t) dt$ So our integrand becomes $$\mathcal I = \int_0^{\frac{\pi}{2}} \frac{t}{\sin(t)} \cos(t) dt\tag{Bounds have changed}$$ $$= \int_0^{\frac{\pi}{2}} t\space\cot(t) dt$$ Then using Integration by Parts,$\space$$u = t$ $\implies du = dt$ and $dv = \cot(t)$ $\implies v = \ln(\sin(t))$ So our integrand thus becomes, $= t\space\ln(\sin(t))$ from $0$ to $\frac{\pi}{2}$ $$-\int_0^{\frac{\pi}{2}} \ln(sin(t))dt\tag{t*ln(sin(t)) = 0}$$ From here, I don't know how to proceed further. Any help/hint is appreciated :) Thanks in advance	integration,definite-integrals,trigonometric-integrals
A.321	Solve Equation $ax+by=d$ where $d \neq \gcd(a,b)$ using Bézout	I want to solve this equation: $3x+4y=14$ I present you what I have so far: $\gcd(3,4)=1$ which is not $14$. I notice that: $3(6) + 4(-1) =14$ So using Bézout : $3(6-4k) + 4(-1+3k) = 14 (1)$, where $k$ integer. So we have $k>1/3$ and $k<3/2$. So $k = 1$. By replacing $k$ in equation $(1)$ we get: $a=2, b=2$, which indeed solves the equation. However, I dont get my previous solution $(a,b)=(4,-1)$ which is also correct. Am I applying Bézout wrong? Or am I not supposed to find the solution that I used to find the new ones, if they exist? Do I have $2$ solutions and that's it or am I missing something? Thank you.	elementary-number-theory,solution-verification
A.322	How do I calculate the sum of sum of triangular numbers?	As we know, triangular numbers are a sequence defined by $\frac{n(n+1)}{2}$. And it's first few terms are $1,3,6,10,15...$. Now I want to calculate the sum of the sum of triangular numbers. Let's define $$a_n=\frac{n(n+1)}{2}$$ $$b_n=\sum_{x=1}^na_x$$ $$c_n=\sum_{x=1}^nb_x$$ And I want an explicit formula for $c_n$. After some research, I found the explicit formula for $b_n=\frac{n(n+1)(n+2)}{6}$. Seeing the patterns from $a_n$ and $b_n$, I figured the explicit formula for $c_n$ would be $\frac{n(n+1)(n+2)(n+3)}{24}$ or $\frac{n(n+1)(n+2)(n+3)}{12}$. Then I tried to plug in those two potential equations, If $n=1$, $c_n=1$, $\frac{n(n+1)(n+2)(n+3)}{24}=1$, $\frac{n(n+1)(n+2)(n+3)}{12}=2$. Thus we can know for sure that the second equation is wrong. If $n=2$, $c_n=1+4=5$, $\frac{n(n+1)(n+2)(n+3)}{24}=5$. Seems correct so far. If $n=3$, $c_n=1+4+10=15$, $\frac{n(n+1)(n+2)(n+3)}{24}=\frac{360}{24}=15$. Overall, from the terms that I tried, the formula above seems to have worked. However, I cannot prove, or explain, why that is. Can someone prove (or disprove) my result above?	sequences-and-series,triangular-numbers
A.323	How to use the Euclidean Algorithm to find the inverse of $\overline{x^2 -x}$ in $(\mathbb{R} [x]/(x^4 + 1))^*$?	I've tried a lot with the Euclidean Algorithm, but I still can't figure it out. Do you know how I can use the Euclidean Algorithm to find the inverse of $\overline{x^2 -x}$ in $(\mathbb{R} [x]/(x^4 + 1))^*$? Thanks in advance!	abstract-algebra,ring-theory,inverse,euclidean-algorithm
A.324	The Double Basel Problem	I have been playing with the series which I had been calling the 'Double Basel problem' for the past couple of hours $$ \sum_{n=1}^{\infty} \sum_{m=1}^\infty \frac{1}{{n^2 +m^2}}. $$ After wrestling with this for awhile, I managed to generalize a result demonstrated here. This identity is: $$ \sum_{m=1}^{\infty}\frac{1}{x^2+m^2} = \frac{1}{2x}\left[ \pi \coth{\pi x} - \frac{1}{x}\right]. $$ Hence the original series becomes: $$ \sum_{n=1}^{\infty} \frac{1}{2n}\left[\pi \coth{\pi n} - \frac{1}{n} \right]. $$ I have no idea where to go next with this problem.  I seriously doubt that this series is convergent; however, I have been unable to prove it.  Can you prove that this series is divergent? If it converges what is its value?  Thanks so much!	sequences-and-series,fourier-series,harmonic-numbers
A.325	Find consecutive composite numbers	How to find 100 consecutive composite numbers? After many attempts I arrived at the conclusion that to find $m$ consecutive composite numbers we can use this $n!+2, n! +3, ..., n! + n$ where $n! + 2$ is divisible by $2$, $n! + 3$ is divisible by $3$ and so on... and where $m$ = $n-1$ Thus $n!+2, n! +3, ..., n! + n$ tells that there are $(n-1)$ consecutive numbers. However, there seems to be some gaps or incompetence. For example: $4!+2, 4! +3, 4! +4$ $→$ $26, 27, 28$. Although it's right there are for sure smaller numbers such as $8, 9, 10$ and $14, 15 ,16.$ Is there another method for solving such a problem mathematically? Is it a correct method or have I misunderstood it?	number-theory,prime-numbers
A.326	Every projective module is a direct summand of free module.	"I was reading ""Serial Rings"" by Gennadi Puninski. There it is written that , ""Since every module is a homomorphic image of a free module, every projective module is a direct summand of free module"".(ie. if $P$ is a  projective module, there exists a free module F such that, $ F=P \oplus T$ for some module $T$.) But I can't understand how ""Every module is a homomorphic image of a free module"" implies that ""Every projective module is a direct summand of free module"". (I have found a proof for ""Every projective module is a direct summand of free module"" but the first part of the above mentioned sentence wasn't used there.)"	modules,direct-sum,projective-module,free-modules
A.327	Determinant not equal to volume error (closed)	The determinant of a $3\times 3$ matrix $\begin{vmatrix} 1 & 1 &1 \\  x & y & z \\ x^2 & y^2 &z^2 \\ \end{vmatrix} $ is the volume of a parallelopiped with its three sides as the vectors whose tails rest on origin and heads at the coordinates $(1,x,x^2),(1,y,y^2)$ and $(1,z,z^2)$ $^{[1]} $. The determinant of this matrix can be simplified to $(x-y)(y-z)(z-x)$.  Proof: Subtracting column$1 $from column 2, and putting that in column  2, $\begin{equation*} \begin{vmatrix} 1 & 1 &1 \\  x & y & z \\ x^2 & y^2 &z^2 \\ \end{vmatrix}  =  \begin{vmatrix} 1 & 0 &1 \\  x & y-x & z \\ x^2 & y^2-x^2 &z^2 \\ \end{vmatrix}  \end{equation*}$ $ = z^2(y-x)-z(y^2-x^2)+x(y^2-x^2)-x^2(y-x) $ rearranging the terms, $ =z^2(y-x)-x^2(y-x)+x(y^2-x^2)-z(y^2-x^2) $ taking out the common terms $(y-x)$ and $(y^2-x^2)$, $ =(y-x)(z^2-x^2)+(y^2-x^2)(x-z) $ expanding the terms $(z^2-x^2)$ and $(y^2-x^2)$ $ =(y-x)(z-x)(z+x)+(y-x)(y+x)(x-z) $ $ =(y-x)(z-x)(z+x)-(y-x)(z-x)(y+x) $ taking out the common term $(y-x)(z-x)$ $ =(y-x)(z-x) [z+x-y-x] $ $ =(y-x)(z-x)(z-y) $ $ =(x-y)(y-z)(z-x) $  As the $x$ coordinate of the heads of these three vectors is $1$, the head of these vectors lies in a plane perpendicular to the $x$-axis and a distance of $1$ unit away from the origin. (If we connect these three points, we get a triangle.) This plane will cut the parallelopiped into two equal triangular pyramids whose base lies in the plane. The perpendicular distance from the base of the pyramid to the tip is $1$ unit. The volume of the required parallelogram is the sum of the volume of the two triangular pyramids. $\text{volume of a pyramid}=\frac{1}{3}bh$. The height is $1$ units. The area of a triangle is, by Shoelace formula, $$A = \frac{1}{2} \begin{vmatrix} 1 & 1 &1 \\  x_1 & x_2 & x_3 \\ y_1 & y_2 & y_3 \\ \end{vmatrix} $$ where the vertices of the triangle are $(x_1,y_1),(x_2.y_2),(x_3,y_3)$ $^{[2]}$ The vertices of the required traingle has the coordinates $(x,x^2),(y,y^2)$ and $(z,z^2)$. So the area of the triangle, $$A=\frac{1}{2}\begin{vmatrix} 1 & 1 &1 \\  x & y & z \\ x^2 & y^2 &z^2 \\ \end{vmatrix}$$ which, as shown above, can be simplified to $\frac{1}{2} (x-y)(y-z)(z-x)$ So, the volume is $$\frac{1}{3}bh=\frac{1}{3}\times\frac{1}{2}(x-y)(y-z)(z-x)\times 1$$ $$= \frac{1}{6}(x-y)(y-z)(z-x)  $$ But, shouldn't the volume be equal to the determinant which is $(x-y)(y-z)(z-x)$ ?  References [1]Youtube video by 3Blue1brown: https://youtu.be/Ip3X9LOh2dk?t=345 [2]Wikipedia article:https://en.wikipedia.org/wiki/Shoelace_formula	geometry,determinant
A.328	Proving $\sum_{k=1}^{n}\cos\frac{2\pi k}{n}=0$	I want to prove that the below equation can be held.  $$\sum_{ k=1 }^{ n  } \cos\left(\frac{  2 \pi k }{  n  } \right) =0, \qquad n>1 $$  Firstly I tried to check the equation with small values of $n$ $$  \text{As } n=2 $$ $$  \cos\left(\frac{  2 \pi \cdot 1   }{  2  } \right) + \cos\left(\frac{  2 \pi \cdot 2   }{  2   } \right)  $$ $$ = \cos\left(\pi\right)  + \cos\left(2 \pi\right)    $$ $$ = -1+ 1 =0  ~~ \leftarrow~~ \text{Obvious}  $$ But $$  \text{As}~~ n=3  $$ $$  \cos\left(\frac{  2 \pi \cdot  1   }{  3   } \right) +\cos\left(\frac{  2 \pi  \cdot  2   }{  3   } \right) + \cos\left(\frac{  2 \pi  \cdot  3   }{  3   } \right)  $$ $$ = \cos\left(\frac{  2 \pi  }{  3  } \right) + \cos\left(\frac{  4 \pi  }{  3  } \right) + \cos\left( 2\pi \right)  $$ $$ = \cos\left(\frac{  2 \pi  }{  3  } \right) + \cos\left(\frac{  4 \pi  }{  3  } \right) + 1  =?$$ What formula(s) or property(s) can be used to prove the equation?	sequences-and-series,trigonometry,trigonometric-series
A.329	How do I show that if $A$ is compact and $U \supseteq A$ is open, then there is an open $V$ with $A \subseteq V \subseteq \overline{V} \subseteq U$?	This question is from Wayne Patty's Topology Section 5.2.  Consider $A$ be a compact subset of a regular space and let $U$ be an open set such that $A\subseteq U$. Prove that there is an open set $V$ such that $A \subseteq  V \subseteq \overline{V} \subseteq U$.  Let $p \in A$ which implies $p \in U$. Then a result is given in   the book (Theorem 5.11): A $T_1$-space $(X, \mathcal T)$ is regular if and only if for each member $p$ of $X$ and each neighbourhood $U$ of $p$, there is a neighbourhood $V$ of $p$ such that $\overline{V}\subseteq U$. So, I got $ V \subseteq \overline{V} \subseteq U$. But I am unable to prove that $A\subseteq V \subseteq \overline{V}$. I thought that I should let $V\subseteq \overline{V} \subseteq A$ but I am not able to find a contradiction. Can you please help with that?	general-topology,separation-axioms
A.330	Shilov's Linear Algebra - Chapter 1, Problem 9	Calculate the $n$-th order determinant: $$\Delta= \begin{vmatrix} x&a&a&\ldots&a\\ a&x&a&\ldots&a\\ a&a&x&\ldots&a\\ \cdot&\cdot&\cdot&\ldots&\cdot\\ a&a&a&\ldots&x \end{vmatrix}$$ The answer is $\Delta=[x+a(n-1)](x-a)^{n-1}$. If we add all the other columns to the first column, we get the first multiplicative factor of the answer, and are left with the following determinant: $$\begin{vmatrix} 1&a&a&\ldots&a\\ 1&x&a&\ldots&a\\ 1&a&x&\ldots&a\\ \cdot&\cdot&\cdot&\ldots&\cdot\\ 1&a&a&\ldots&x \end{vmatrix}$$ How can we calculate this determinant to obtain the answer?	linear-algebra,determinant
A.331	Finding roots of $4^x+6^x=9^x$ by hand	The function $f(x)=4^x+6^x-9^x$ is such that $f(0)=1>0, f(1)=1>0, f(2)=-29$ and next $g(x)=(4/9)^x+(6/9)^x-1 \implies f'(x)<0$ for all real values of $x$. So $g(x)$ being monotonic the equation $$4^x+6^x=9^x$$ has exactly one real solution. The question is whether this real root can be found analytically by hand.	real-analysis
A.332	Every number can be expressed as a product of (least prime factor)*(largest integer dividing n less than n) [EDITED]	"Let $L:{\Bbb N} \to {\Bbb N}$ such that $L(n) = {\text{least prime factor p of n}}$. Let $g:{\Bbb N} \to {\Bbb N}$ such that $g(n) = {\text{biggest positive integer d such that d|n and 1}} \leqslant {\text{d < n}}$. Show that:$$g(n) = \frac{n}{{L(n)}},\forall n \in {\Bbb N}{\text{ such that }}n \geqslant 2$$ My proof: Since $n \geqslant 2$, $n$ is either composite or prime. If $n$ is prime(i.e $n = p$ where $p$ is prime), then $$g(n) = 1 = \frac{p}{p} = \frac{n}{{L(n)}}$$ This is because $L(n)$ is defined as least prime factor of n. If$n $is composite, by the Fundamental Theorem of Arithmetic: ""Any positive integer bigger than 1 can be expressed as a product of primes."" Let $p$ be the smallest prime of the product $$n = {p_1}^{{\alpha _1}}p_2^{{\alpha _2}}...{p_m}^{{\alpha _m}},{\text{ }}i < j,{\text{ }}{p_i} < {p_j}{\text{ and }}i,j \in {{\Bbb Z}^ + }{\text{ and }}{\alpha _i} \in {{\Bbb Z}^{ \geqslant 0}}$$ That is, pick $p = p_1$. Notice that $p$ is necessarily $L(n)$ because since $p = p_1$ implies that $p|n$ and is the smallest prime that divides n. Therefore, $$p_1 = p = L(n)$$ Then, if $x = p = L(n)$, then $1 < x < n$ and $x|n$ (by definition). This implies that $$\exists y \in {{\Bbb Z}^ + }:y = \frac{n}{x} = \frac{n}{{L(n)}} = {p_1}^{{\alpha _1} - 1}{p_2}^{{\alpha _2}}...{p_m}^{{\alpha _m}}$$ But $y|n$ such that $y < n$. Implies that $y$ is the greatest integer less than$n $such that $y|n$. Now let $d = g(n)$. Since $$\frac{n}{d} = \frac{n}{{g(n)}} = m$$ then $m$ is prime and $m = p$. BWOC, If $n=md$ and $m = ab$ where $1 < a,b < m \Rightarrow n = abd \Rightarrow ad|n$. Since $1. But we found another factor of $n$ ($ad$) that divides $n$! This contradicts the definition of $d=g(n)$. Also, since $d = g(n)$ is the biggest factor of n, implies that $m = \frac{n}{d} = \frac{n}{g(n)}$ is the smallest prime factor of n. So $m = L(n) = p$. Hence, we have that $$x = m = p = L(n) = \frac{n}{g(n)} \Rightarrow g(n) = \frac{n}{p} =\frac{n}{L(n)} = y$$ or just $$g(n) = \frac{n}{L(n)}$$ Q.E.D."	elementary-number-theory
A.333	Bivariate normal density	$( X, Y)$ have a bivariate normal density centered at the origin with $E(X^2)$ = $E(Y^2) = 1$, and $E(XY) = p$ . In polar coordinates $(X, Y)$ becomes $(R,\Phi)$ where $R^2 = X^2 + Y^2$. Prove that $\Phi$ has a density given by $$\frac{\sqrt{1-p^2}}{2\pi(1-2p\sin(\varphi)\cos(\varphi))}$$ And is uniformly distributed iff $p = 0$. (To this point everything is clear) what i do not understand is how to conclude that $P\{XY > 0\} =  \frac{1}{2} +\pi^{-1} \arcsin (p)$ and $P\{XY < 0\}= \pi^{-1} \arccos (p)$.	probability,probability-theory,probability-distributions
A.334	logarithm proof for $a^{log_a(b)}=b$	I have tried proving for $a^{log_a(b)}=b$ , but I feel is incorrect, so how can I prove this? I have proved it as follows: $log_aa^{log_a(b)}=log_ab$ $log_a(b)log_aa= log_ab$ $log_a(b)= log_ab$	logarithms
A.335	Matrix exponential converges to a matrix	Let $A$ be a square matrix. To show: Matrix exponential converges to some matrix $X$. $$ \lim_{N \rightarrow \infty} \sum_{k=0}^{N}\frac{A^k}{k!} =X  $$ In some proofs that I have seen it is stated that because (for a sub-multiplicative norm) $$ 0 \le  \sum_{k=0}^{\infty} \left\Vert  \frac{A^k }{k!} \right\Vert  \le  \sum_{k=0}^{N} \frac{\Vert A \Vert ^k }{k!} then the series $\sum_{k=0}^{N}\frac{A^k}{k!}$ has to be convergent. That however isn't clear to me. To me more intuitive way to show convergence would be to show that $$ \lim_{N \rightarrow \infty} \left\Vert \sum_{k=0}^{N} \frac{A^k}{k!} -X \right\Vert  =0$$ and use some intuitive matrix norm for which it is clear that all elements of $\frac{A^k}{k!} -X$ converge to zero. Any hints?	matrix-exponential
A.336	For each $n \in \mathbb{N}$, specify matrices $A, B \in \mathbb{R}^{n \times n}$ for which $A B \neq B A$ is true.	I have got the following task: (1) For each $n \in \mathbb{N}$, specify matrices $A, B \in \mathbb{R}^{n \times n}$ for which $A B \neq B A$ is true. (2) Determine $$ M:=\left\{A \in \mathbb{R}^{2 \times 2}: A B=B A \quad \forall B \in \mathbb{R}^{2 \times 2}\right\} $$ thus the matrices $A \in \mathbb{R}^{2 \times 2}$, which commute with all matrices $B \in \mathbb{R}^{2 \times 2}$. Could someone please help me with this or give me an approach? That would be very helpful. Thanks.	linear-algebra,matrices
A.337	Suppose that all the tangent lines of a regular plane curve pass through some fixed point. Prove that the curve is part of a straight line.	"Question. Suppose that all the tangent lines of a regular plane curve pass through some fixed point. Prove that the curve is part of a straight line. Prove the same result if all the normal lines are parallel. I am working on differential geometry from the book by Pressley and I have a doubt in the solution of the above question whose (brief) solution is given by: Solution: We can assume that the curve $\gamma$ is unit-speed and that the tangent lines all pass through the origin (by applying a translation to $\gamma$). Then, there is a scalar $\lambda(t)$ such that $\gamma'(t) = \lambda(t)\gamma(t)$ for all $t$. Then, $\gamma '' = \lambda'\gamma   + \lambda \gamma' = (\lambda' + \lambda^2)\gamma$. Can anyone please explain me how does this line follow : "" Then, there is a scalar $\lambda(t)$ such that $\gamma'(t) = \lambda(t)\gamma(t)$ for all $t$."" Thanks in advance."	differential-geometry,curves,tangent-line
A.338	Find all integer solutions of equation $y = \frac{a+bx}{b-x}$	How to find all integer solutions for the equation $y = \frac{a+bx}{b-x}$, where a and b are known integer values? P.S. x and y must be integer at the same time	elementary-number-theory,diophantine-equations
A.339	Extension of Euclid's lemma	This is$a $somewhat obvious fact that is intuitively obvious to me, but I haven't been able to construct a proof of it. Euclid's lemma states for for $p$ a prime and $ab$ a product of integers (let's take everything to be positive for simplicity), if $p \mid ab$, then $p \mid a$ or $p \mid b$. This is clear, and I know how to prove it. Let's extend it somewhat. Suppose that $a$ and $b$ are two relatively prime integers, and we have $a \mid bc$ for some other integer $c$. Then $a \not \mid b$, so it must divide $c$. This fact is obvious to me, but I can't figure out how to prove it. Does anyone have any hints or advice? Do I need the assumption of positivity? (For my purposes at the moment, I only need them to be positive, but there is value in having the most general result possible). EDIT: Updated attempt: We have that $a,b$ are relatively prime, so there exist $r,s \in \mathbb{Z}$ such that $ar + bs = 1$ by Bézout's lemma. Multiply through by $c$ to get $arc + bsc = c$. Then $a \mid arc$ and $a \mid bsc$, so $a \mid c$. How is that?	elementary-number-theory
A.340	I have the following problem: Let $|x_{n+1} - x_n| < 1/3^n$. Show that $(x_n)$ is a Cauchy sequence.	We have that $(x_n)$ is a sequence of real numbers. And the relation on the title: $$ |x_{n+1} - x_n| < \frac{1}{3^n}. $$ We must prove that this is a Cauchy sequence. I know that an Cauchy sequence follows the definition: given $\epsilon>0$, exists $n_0 > 0$, such that $m,n > n_o \Rightarrow |x_m - x_n|< \epsilon$ But I don't know how to use both informations to prove the exercise. If someone please may help me, I'd be very thankful.	real-analysis,sequences-and-series
A.341	Do all arithmetic sequences with coprime coefficients contain a prime?	Given $a, b \in Z^+$, where $\gcd(a, b) = 1$, we can define an arithmetic sequence $c_i = a + i \cdot b$. The sequence is thus $\{a, a+b, a+2b, \cdots\}$. Do all such sequences contain a prime? Do they contain an infinite number of primes? Example: $a=2, b=3$. Then, $c_1 = a+b = 5$, which is prime. Meanwhile, $a=4, b=2$ does not have primes, but $\gcd(a, b) = 2 \neq 1$, so this isn't a counterexample.	elementary-number-theory,arithmetic-progressions,coprime
A.342	Four students are giving presentations	In four sections of a course, running (independently) in parallel, there are four students giving presentations that are each Exponential in length, with expected value of$10 $minutes each. How much time do we expect to be needed until all four of the presentations are completed? I'm a little thrown off by this question since it's in the chapter of order statistics in my book. But I believe that this is just gamma distribution. If each student has expected value of $10$ minutes each. Shouldn't the time needed till all four of the presentations are completed be $40$ minutes? $(10 \cdot 4 = 40)$ Or is it the following. Calculate the density of the fourth order statistics $$f(x_4) =\frac{2}{5}e^{\frac{-x}{10}}\left(1-e^{\frac{-x}{10}}\right)^3.$$ Then $$E(X_4) = \int_0^\infty\frac{2x}{5}e^\frac{-x}{10}\left(1-e^\frac{-x}{10}\right)^3 \,dx= 125/6.$$ So is the answer $40$ minutes or $125/6$ minutes? Any help is greatly appreciated.	solution-verification,exponential-distribution,order-statistics,gamma-distribution
A.343	When two dice are identical, why are ordered pairs considered for determining probability of getting sum x?	Problem statement : +++++++++++++++ Given two identical unbiased dice, determine the probability of getting sum as 7. Event  = Sum of dots on the top face of both dice is 7. $E = {(1,6),\ (2,5),\ (3,4),\ (4,3),\ (3,4),\ (5,2),\ (6,1)}$ $|Sample Space|$ = $36$. Hence, $P(E) = 1/6$ I have a doubt here. As the two dice are given identical, why do we have to consider ordered pairs? Shouldn't it be unordered consisting of only 3 possible pairs $\{(1,6),\ (2,5),\ (3,4)\} $? Hence, $|S| = 21$ and $P(E) = 3/21$.	probability
A.344	Any collection of subsets of a set is a subbasis for a topology	Theorem Any collection of subsets $\mathcal{A}$ of a nonempty set $X$ forms the subbasis for a unique topology $\tau$ on $X$. This theorem is absolutely amazing to me. I really enjoy the idea of it as a powerful tool, but I have come up with a counterexample that I just can't get over. So the theorem states that any collection of subsets of a nonempty $X$ form a subbasis for a unique topology on $X$. The emphasis there is any. So, consider the following counterexample: Let $X= \{a,b,c,d,e\}$ and let $\mathcal{A}=\{\{a\}\}$. Clearly, this is a collection of subsets of $X$. Assume that, by our theorem, then $\mathcal{A}=\{\{a\}\}$ is a subbasis for some topology on $X$. Okay, so since $\mathcal{A}$ is a subbasis of some topology on $X$, let's try taking intersections of members of $\mathcal{A}$. Well, $\{a\}\cap\{a\}=\{a\}$. Then our basis for our topology is $\mathcal{B} = \{\{a\}\}$ This is problematic because this means that our basis $\mathcal{B}$ is just $\{a\}$, but note that $\displaystyle\bigcup \mathcal{B} = \{a\}$ and $\{a\} \neq X.$ Therefore, $X \not \in \tau.$ How do we get $X$ in $\tau$? Is my counterexample logically consistent?	general-topology
A.345	Elementary geometry question: How to calculate distance between two skew lines?	"I am helping someone with highschool maths but I got stacked in a elementary geometry problem. I am given the equation of two straigh lines in the space $r\equiv \begin{cases} x=1 \\ y=1 \\z=\lambda -2 \end{cases}$ and $s\equiv\begin{cases} x=\mu \\ y=\mu -1 \\ z=-1\end{cases}$ and asked for some calculations. First I am asked the relative position of them so I get they are skew lines. After that I am asked for the distance between the two lines. In order to get the distance I have to calculate the line that is perpendicular to both of them in the ""skewing"" point, check the points where it touches the other two lines (sorry, not sure about the word in English) and calculate the module of this vector. Im having trouble calculating the perpendicular line. I know I can get the director vector using vectorial product, but  I'm not sure how to find a point so that I can build the line."	geometry
A.346	Greatest lower bound in Q	I have a set $$ \{ r \in \mathbb Q \mid r^2 >2, r>0 \}$$ I was wondering why it does not have the greatest lower bound. Isn't $0 \in \mathbb Q$ a greatest lower bound for this set in rational numbers?	supremum-and-infimum
A.347	GCD (a,b) =1 prove GCD ( (a+b), (a-b) ) = 1 or 2	if GCD of $(a, b) = 1$, prove that GCD $(a+b, a-b) = 1$ or $2 .$ The proof goes like: Let GCD $( a+b, a-b ) = d$ and let there exist integers m and n such that $ a+b =md$  and $ a-b = nd.$ By adding and subtracting these two equations we get: $2a = (m+n)d$ and $2b = (m-n)d$ , because $a, b$ are coprime then $2$ GCD $(a,b)$ = GCD $(2a, 2b),$ and so on. My question is, why do we have to add and subtract above equations? I need to understand the concept of this prove in some more details. Thanks!	divisibility,gcd-and-lcm
A.348	Question about determinant of a block matrix	I was studying block matrices and suddenly this question came to my mind. Let $A, B \in \Bbb R^{n \times n}$. From this Wikipedia page, $$\det \begin{pmatrix} A & B\\ B & A\end{pmatrix} = \det(A-B)\det(A+B)$$ even if $A$ and $B$ do not commute. Does a similar condition hold for the following block matrix? $$\begin{pmatrix} A & -B\\  B & A \end{pmatrix}$$	matrices,determinant,block-matrices
A.349	Inverse to Stirling's Approximation	The equation for Stirling's Approximation is the following: $$x! = \sqrt{2\pi x} * (\frac{x}{e})^x$$ Writing as a function for y gives us the following: $$y = \sqrt{2\pi x} * (\frac{x}{e})^x$$ Is there a way to solve this equation for x, effectively finding an inverse to this function?	factorial,inverse-function
A.350	Induction proof for natural numbers in a division operation	I want to proove that 2 and 3 divide $x^3 - x, x \in \mathbb{N}$ and I'm stuck at the inductive step, here's where I'm at: For all $x \in \mathbb{N}$, let $P(x)$ be the  proposition: 2 and 3 divide $x^3 - x$ Basic step: the first term in $\mathbb{N}$ is $0$, then: $\frac{0^3 - 0}{2} = 0$ et $\frac{0^3 - 0}{3} = 0$, thus $P(0)$ is true. Inductive step: For the inductive hypothesis, we assume that $P(k)$ is true for an arbitrary nonnegative integer k bigger than 0. That is, we assume that 2 and 3 divide $k^3 - k$ To carry out the inductive step using this assumption, we must show that when we assume that $P(k)$ is true, then $P(k + 1)$ is also true. That is, we must show that 2 and 3 divide $(k+1)^3 - (k+1)$ Is the next step here is that we need to prove that $\frac{(k+1)^3-(k+1)}{2}$ and $\frac{(k+1)^3-(k+1)}{3}$ are integers? thus 2 and 3 divide $(k+1)^3 - (k+1)$?	induction,divisibility,natural-numbers
A.351	Application of Fatou's Lemma but something simpler is better?	The question Let $(X,\mathcal{A},\mu)$ be a measure space. Let $A_n$ be a sequence of sets in $\mathcal{A}$. Define $A := \{ x \in X $ such that  for all but finitely many $n \in \mathbb{N}$ it holds that $x ∈ A_n$ $ \}$ Show that $\lim_{n\to\infty}\inf \mu (A_n) \geq \mu(A)$ My attempt $ \mu (A_n) = \int_X \chi_{A_n} d\mu$ and so by Fatou's lemma: $\lim_{n\to\infty}\inf \mu (A_n) = \lim_{n\to\infty}\inf \int_X \chi_{ A_n} \geq \int_X \lim_{n\to\infty}\inf \chi_{A_n} d\mu$ Now all I need to show is that $\lim_{n\to\infty}\inf \chi_{A_n}(x)  = \chi_A(x)$ a.e Consider $x\in A$ then eventually $x\in A_n \forall n $ eventually and so $\chi_{A_n}(x) = 1 \forall n $ and so $\lim_{n\to\infty}\inf \chi_{A_n}(x)  = \lim_{n\to\infty} 1 = 1= \chi_A(x)$ Now consider $x\not\in A$ then $\forall N \in \mathbb{N} \exists n >N$ such that $x \not\in A_n$ and so $\inf_{m \geq n} \chi_{A_n}(x) = 0 \forall n$ and hence $\lim_{n\to\infty}\inf \chi_{A_n}(x)  = \lim_{n\to\infty} 0 = 0= \chi_A(x)$ Ando so the required follows. However this feels awfully complicated and I was wondering if anyone has any tips for something simpler	measure-theory
A.352	find a positive continuous function with a finite area : $\int_0^\infty f(x) dx$ , but the $f(x)\rightarrow$ doesn't exist.	find a positive continuous function with a finite area : $\int_0^\infty f(x) dx$ , but the limit of $f(x)$ as $x$ goes to infinity doesn't exist. I tried finding such a function but I failed .	real-analysis,calculus,limits
A.353	Can someone explain why if two random variables, X and Y, are uncorrelated, it does not necessarily mean they are independent?	I understand that two independent random variables are by definition uncorrelated as their covariance is equivalent to 0: $Cov(x,y) = E(xy)- E(x)E(y)$ $E(x)*E(y) = E(xy)$, when x and y are two random independent variables. Therefore, $Cov(x,y) = 0$. However, I am having trouble understanding if two random variables, X and Y, are uncorrelated, it does not necessarily mean they are independent. Could someone also give me a real world example of when two random variables are neither independent nor casually connected? I believe it will help me understand this concept better.	probability,probability-theory,independence,covariance,causality
A.354	Prove that if $p_1\mid a$ and $p_2\mid a$ then $p_1p_2\mid a$	So I am supposed to be proving that if $p_1$ and $p_2$ are distinct primes and $p_1\mid a$ and $p_2\mid a$ then $p_1p_2\mid a$, and I need to use Euclid's Lemma except as far as I understand Euclid's lemma is the converse of this statement and I have tried for the last few hours to work with Euclid's and GCDs to figure this one out and I just don't know where to start since I can't wrap my head around this one. Can anyone help me out?	elementary-number-theory
A.355	$f(f(x)^2+f(y))=xf(x)+y$	Find all functions $f:\mathbb{R}\rightarrow\mathbb{R}$ such that $$f(f(x)^2+f(y))=xf(x)+y$$ for all $x,y\in{\mathbb{R}}$.  Here is my approach to the problem:  We see that $f(x)=x$ is an obvious solution (Just trying easy linear equations). I think this would be the only solution to the problem.  Am I right? And how to prove that there is no other solution? (Note: I am a beginner at functional equations)	linear-algebra,proof-writing,contest-math,functional-equations
A.356	How can I evaluate $ \sum_{n=0}^{\infty}{\frac{x^{kn}}{(kn)!}} $ where $k$ is a natural number?	I suddenly interested in the differential equation $$ f^{(k)}(x)=f(x) $$ So I tried to calculate for some $n$. When $ k=1 $, we know the solution $$ f(x)=A_0e^x=\sum_{n=0}^{\infty}{\frac{A_0x^n}{n!}} $$ Also, for $ k=2 $, $$ f(x)=Ae^x-Be^{-x}=\sum_{n=0}^{\infty}{(\frac{A_0x^{2n}}{(2n)!}+\frac{A_1x^{2n+1}}{(2n+1)!})} $$ where $ A_0=A+B $ and $ A_1=A-B $. Inductively, I could guess that the solution of the differential equation would be in the form $$ f(x)=\sum_{n=0}^{\infty}{\sum_{i=0}^{k-1}{\frac{A_ix^{kn+i}}{(kn+i)!}}} $$ But I could neither prove that it is the only solution nor get the explicit formula. How should I evaluate $ \sum_{n=0}^{\infty}{\frac{x^{kn}}{(kn)!}} $, cause if we know the answer for it, we can evaluate the original expression by differentiating it. Thanks to WolframAlpha, I know the answer for $ k=3 $, $$ \sum_{n=0}^{\infty}{\frac{x^{3n}}{(3n)!}}=\frac{1}{3}(2e^{-\frac {x}{2}}\cos{(\frac {\sqrt{3}}{2}x)}+e^{x}) $$ I think the answer might related to $ \sin $ and $ \cos $ of $ \frac {2\pi}{k} $.	sequences-and-series,ordinary-differential-equations,power-series,functional-equations
A.357	How many functions can be used to describe to a finite series?	I was learning more about series today and would like to know if there are existing proofs I could look at about this problem. Basically, if you are given an infinite series representing a function f : $\Bbb N \Rightarrow \Bbb R$ but only shown the first n numbers, how many functions f, written in terms of n, could you write to represent that series. I'm not including piecewise functions, because I assume that would always be infinite. Take the series $(2, 4, ...)$ with 2 numbers given. $f(n)=2n$ , $f(n)=n^2-n+2$ , and $f(n)=2^n$ would all be functions that could fit this series, although they differ after the first two numbers. I believe there are more polynomials that fit this description but I'm not sure how many. My question is, essentially, are there an infinite number of functions for which $f(1) = 2$ and $f(2) = 4$, and if this is the case, does this also apply to any finite number of outputs? (e.g. the first n digits of pi written as $(3, 1, 4, 1, 5, 9...)$) If not, could you find out how many possible functions there are?	sequences-and-series,number-theory
A.358	Confusion about the formula of the area of a surface of revolution	Before I read the formula of the area of revolution which is $\int 2\pi y \,ds$, where $ds = \sqrt{1 + \frac{dy}{dx}^2}$, I thought of deriving it myself. I tried to apply the same logic used for calculating the volume of revolution (e.g., $\int \pi y^2 dx $). My idea is to use many tiny hollow cylinders (inspired from the shell method), each has a surface area of $(2\pi y) (dx)$:  $2\pi y$ is the circumference of the cylinder, and $dx$ is the height of the cylinder  Their product is the surface area of the hollow (e.g., empty from the inside) cylinder. With this logic, the area is $\int 2\pi y dx$. Where is my mistake? Also it's confusing why for the volume it was enough to partition the object using cylinders and for areas not.	calculus,area
A.359	Apparent inconsistencies in integration	In a problem, the substitution $$\tan\theta=\frac{x}{2}$$ was made. In the end, the answer was in terms of sines, and to convert back, $sin\theta$ was defined as $$\sin\theta=\frac{x}{\sqrt{4+x^2}}$$ This is a typical example of some stuff about integration I'm struggling to understand; (1) Why are the absolute values of square roots never taken? This is something I keep seeing in every situation involving an integral. (Here, if $\theta$ is in the third quadrant, sines would be negative and tans would be positive. So this definitely doesn't work for the third quadrant.) (2) Expanding upon the stuff in the parantheses up there, a possible explanation is that while doing trig substitutions, the angle is always a principal angle of the inverse trigonometric operation on whatever you're making the substitution. Is there such a rule?	integration,trigonometry,roots,substitution,trigonometric-integrals
A.360	Fourier transform of function $1/ \vert x \vert$	What is the Fourier transform of function $$f(x) = \frac{1}{\vert x \vert}?$$ This is not a homework. I would also appreciate help for calculating it myself.	fourier-transform
A.361	Is a Riemann-integrable function always differentiable?	Let $f:[a,b]\to\mathbb{R}$ be Riemann-integrable and $F(x)=\int_a^xf(t)dt$. Is this function $F$ always differentiable? Because the antiderivative is defined as $F'=f$ right, so you would think that it always holds.	real-analysis
A.362	Kuratowski's Theorem using Axiom of Choice	"I can't seem to be able to prove Kuratowski's Theorem using the Axiom of Choice, although they are equivalent assertions. Kuratowski's Lemma: Every partial order has a maximal chain. Axiom of Choice: For every set X of disjoint nonempty sets there exists a set$Y $such that for every set $Z \in X, Y \cap Z$ is a singleton. My attempt: Consider any chain $C_0$ of the partial order. If $\exists x \in X \setminus C_0$ which is comparable with some element of $C_0$, let $C_1 := C_0 \cup \{ x \}$. Iterate this process . If at some point we cannot find such an x, then we have found a maximal chain. Suppose we can find such an $x$ infinitely, then the sets $i\geq 1 \Rightarrow X_i := C_{i+1} \setminus C_i$ are disjoint singletons. Hence by axiom of choice there exists $Y$ for which $X_i \subseteq Y$. Inorder to finish the proof, I need to prove something of the form ""If a is comparable with some element of $C_0$, then $\exists j$ s.t. $a \in C_j$"". I can't seem to prove this. P.S: x is comparable with y iff $x R y \lor y R x$."	order-theory,axiom-of-choice
A.363	Non-negative martingale $X_n \rightarrow 0$ a.s. prove that $P[X^* \geq x | \mathcal{F}_0]= 1 \wedge X_0 / x$	I need to prove the following statement. Let $X$ be a non negative martingale such that $X_n\rightarrow 0$ a.s. when $n\rightarrow \infty$. Define $X^*=supX_n$. Prove that for all $x>0$ $$P[X^*  \geq x | \mathcal{F}_0]= 1 \wedge X_0 / x$$ I think I've got the easy case  if  $x\leq X_0$  Then necessarily $x\leq X^*$ for the sup property. Then it follows that for $1\leq X_0 /x$ we have that $P[X^*  \geq x | \mathcal{F}_0]= 1$. But I can't figure out the other case.	probability,martingales,stopping-times
A.364	Inequality in metric space	For a point $x$ and a non-empty subset $A$ of a metric space $(X, d)$, define $\begin{align}\inf\left\{ d(x,a):a\in A\right\}\end{align}$ Prove that if $y$ is another point in $X$ then $$d(x,A)\leqslant d(x,y)+d(y,A)$$	general-topology,analysis,metric-spaces
A.365	Are Infinite ordinals and their successor equinumerous?	Ordinals in set theory are well-ordered by $\in$ or equivalently $\subset$. If we define all ordinals greater or equal to $\omega$ as infinite ordinals. Is it true that every infinite ordinal is equinumerous to its successors. Basically my question is the proof or refutation of the following statement: Given infinite ordinal $\alpha$. Does there exist an injection from $\alpha^+$ to $\alpha$.	set-theory
A.366	Show that if a normed space $X $ has a linearly independent subset of $n$ elements, so does the dual space $X'$	Show that if a normed space $X $ has a linearly independent subset of $n$ elements, so does the dual space $X'$ My attempt : $\text{Given that  a  normed space  $X$ has  a linearly indepenedent  susbset of  $n-$  element}\tag1$ let the subset be  $S=\{ e_1,e_2,e_3,....,e_n\}$ Define $e_i \in X$ by $f_j(e_i)= \delta_{ij} = \begin{cases} 1 & i=j \\0 , & i \neq j  \end{cases}$ where $1\le i\le n$ and $1\le j\le n$ From $(1)$ we have  $c_1e_1+...+c_ne_n=0\implies c_1f(e_1)+...+c_nf(e_n)=0$ After  that im not able to proceed further	functional-analysis
A.367	Prove that $d(x,M)=\frac{|\langle f,x \rangle|}{||f||}$	I want to show that for $E$ a normed space, $f\in E^*$ and $M=\{x\in E\,:\, f(x)=0\}$:  Write $M^\perp$ Show that $d(x,M)=\frac{|\langle f,x\rangle|}{||f||}$.  This is my attempt: For the second part: We have that $f\in E^*$ and for $x\in E$ and $m\in M$ $$\ |\langle f,x-m|\rangle \leq ||f|| ||x-m|| \Rightarrow \frac{|\langle f,x\rangle|}{||f||} \leq ||x-m||. $$ Therefore, $$ \frac{|\langle f,x\rangle|}{||f||} \leq \inf_{m\in M}||x-m|| =d(x,M). $$ The second inequalyty is that I can't prove, I think that any corollary of Hanh-Banach could help me to prove that  $$d(x,M)\leq \frac{|\langle f,x\rangle|}{||f||} $$ Does anyone have any idea and could check my proof? Update I found the same question in this link Orthogonality Relations Exercise, Brezis' Book Functional Analysis	functional-analysis,normed-spaces
A.368	Basel Problem approximation error bounded by $\mathcal O(1/x)$?	In this answer it is stated that $$ \sum_{n\geq1}\frac{1}{n^2}=\sum_{n\leq x}\frac1{n^2}+\mathcal O(1/x). $$ Is this statement true as $x\to\infty$? What I've done: If $x$ is fixed, then I think the answer is almost trivial, because we may set $C=\pi^2x/6$, so $$ \sum_{n=x}^\infty\frac1{n^2}\leq\sum_{n=1}^\infty\frac1{n^2}=\frac{\pi^2}{6}=\frac{C}{x}, $$ therefore $$ \sum_{n\geq1}\frac1{n^2}=\sum_{n\leq x}\frac{1}{n^2}+\sum_{n=x}^\infty\frac{1}{n^2}\leq\sum_{n\leq x}\frac{1}{n^2}+C/x=\sum_{n\leq x}\frac{1}{n^2}+\mathcal O(1/x). $$ But is there a constant independent of $x$ that makes this true?	approximation
A.369	Recurrent integral	How to calculate integral $$J_n=\int_{-\pi}^\pi \frac{\sin{(nx)}}{(1+2^n) \sin{x}}\,\mathrm{d}x\:?$$ I tried partial integration but did not succeed in finding a recurrent relation? Also, tried Moivre formula for $I_n+iJ_n$, where $I_n=\int_{-\pi}^\pi \frac{\cos{(nx)}}{(1+2^n) \sin{x}} dx$, but also without success. Any help is welcome. Thanks in advance.	integration,definite-integrals,recurrence-relations
A.370	What if we take step functions instead of simple functions in the Lebesgue integral	When we define the Lebesgue integral, we first define it for simple functions $s(x) = \sum\limits_{j=1}^n c_j\chi_{A_j}(x)$ (where $A_j$ are measurable) as $\int sd\mu = \sum\limits_{i=j}^n c_j \mu(A_j)$ and then for $f\ge 0$ as $\int fd\mu = \sup\{\int sd\mu$ : s simple and $0\le s\le f\}$. But I was wondering what could go wrong if instead of taking simple functions in the supremum, we would take step functions, i.e. $s(x)=\sum\limits_{j=1}^nc_i\chi_{I_j}(x)$ where $I_j$ are intervals (any type, like $(a,b), (a,b], [a,b), [a,b])$).	measure-theory,lebesgue-integral,step-function,simple-functions
A.371	Can I say $|f(x)g(x)|=||fg||$	Let $f,g:[0,1]\to \Bbb{R}$ be continuous functions. Show that $$||fg||\le||f||\space||g||$$ What I have got so far : $|f(x)| \le\max|f(x)|=$ norm of $f$, $||f||$.$\forall x\in[0,1]$. (Note: I have replaced supremum with maximum.) $|f(x)||g(x)| =|f(x)g(x)|\le \max |f(x)|g(x)=||f|| \space\space |g|\le \max|f(x)|\space \max|g(x)|=|f||\space\space||g||$ $|f(x)g(x)|\le||f||\space\space||g||$ As I have to show that $||fg||\le||f||\space||g||$ : Can I say $|f(x) g(x)|=||fg||$? I'm not sure about that  Because $|f(x)g(x)| \le\max|f(x)g(x)|=\max|f(x)| \space \max|g(x)|$ I feel I am missing concept to prove $|f(x) g(x)|=||fg||$, through which I think I finally can prove $||fg||\le||f||\space||g||$ please If you guys could clarify.	functional-analysis,analysis,norm
A.372	When did we move from $\mathbb{Z}\left[\sqrt{d}\right]$ to the ring of integers $\mathcal{O}_{\mathbb{Q}\left[\sqrt{d}\right]}$ and why?	Gauss made great progress in number theory in $\mathbb{Z}$ by working in $\mathbb{Z}[i]$ (or equivalently $\mathbb{Z}\left[\sqrt{-1}\right]$), so much so that we call $\mathbb{Z}[i]$ the Gaussian integers now. And it was even known to the old mathematicians that solutions to Pell's equation $x^2 - dy^2 = 1$ could be better analysed by working in $\mathbb{Z}\left[\sqrt{d}\right]$. But now in modern number theory we study much more the ring of integers $\mathcal{O}_{\mathbb{Q}\left[\sqrt{d}\right]}$. I find this confusing, as if we want to study Pell's equation with $d = 5$, we have that $\mathcal{O}_{\mathbb{Q}\left[\sqrt{5}\right]} = \mathbb{Z}\left[\frac{1 + \sqrt{5}}{2}\right]$ instead of $\mathbb{Z}\left[\sqrt{5}\right]$, which is not what we need. I was under the assumption that modern number theory usually tries to generalise its techniques but I don't see how this is a sensible generalisation and I don't see why the ring of integers is any more useful than just plain old $\mathbb{Z}\left[\sqrt{d}\right]$. So my question is: Why is the ring of integers defined the way it is?	abstract-algebra,ring-theory,soft-question,definition
A.373	Show that $\sqrt n$ is irrational if $n$ is not a perfect square, using the method of infinite descent.	"Show that $\sqrt n$ is irrational if $n$ is not$a $perfect square, using the method of infinite descent. I know how to prove this by doing a contradiction proof and using The Fundamental Theorem of Arithmetic, but now I'm asked to use infinite descent to prove it. Then the very next problem says ""Why does the method of the text fail to show that $\sqrt n$ is irrational if $n$ is a perfect square?"" I'm confused by this. Any hints or solutions are greatly appreciated. I was thinking of the standard argument, let $\sqrt n = {a\over b}$ where $gcd(a,b)=1$ and then through some algebra arrive at a common factor for both $a$ and $b$ which contradicts the fact that $gcd(a,b)=1$ and so we can apply this over and over again, but then I don't understand how the next problem says to explain why this method fails."	elementary-number-theory
A.374	Finding all monic complex polynomials $P(x)$ such that $P(x)|P(x^2)$	Find all monic complex polynomials $P(x)$ such that $P(x)|P(x^2)$.  My progress so far is that I have find that for degree 1, $P(x)=x, x^2$ are the only ones. For degree 2, they are $P(x)=x^2+x+1, x^2, x^2-1, x^2-x, x^2-2x+1$. I also prove that these are only solutions for degree 1 and 2. However I do not see how this generalizes. Any help please?	algebra-precalculus,polynomials,divisibility
A.375	Can anyone help me solve this diophantine equation?	Find all integer solutions to $x^2 + 7 = 2^n$. I've done the case where n is an even integer but now I'm a little lost. Could anyone walk me through the solution?	elementary-number-theory
A.376	Evaluating the limit of a sqrt function using Riemann Sums	$\lim\limits_{n\to\infty}\dfrac{\sqrt1+\sqrt2+\sqrt3+\ldots+\sqrt n}{n\sqrt n}$ I am having trouble doing this problem. I have attempted to take the Riemann Sum but cannot get past the square root. I also tried to upper-bound and lower-bound it, but I got stuck doing this.	calculus,limits,riemann-sum
A.377	What is $\mathbb{R}^{n+1}-\mathbb{R}^n$?	In C.H. Edwards's Advanced Calculus of Several Variables he defines the ordinate set $\mathcal{O}_f$ of a function $f:\mathbb{R}^n\to\mathbb{R}$ as the set of points between $\mathbb{R}^n$ and the graph of $f,$ including the points of evaluation, $\mathbf{x}\in\mathbb{R}^n$ and the points in the graph $\left\{\mathbf{x},f\left(\mathbf{x}\right)\right\}\in\mathbb{R}^{n+1}$.  Later on he defines a set $\hat{\mathcal{G}}=\partial\mathcal{O}_f-\mathbb{R}^n,$ where $\partial\mathcal{O}_f$ is the boundary of $\mathcal{O}_f.$  The intent seems clear.  First $$\mathbb{R}^{n+1}-\mathbb{R}^n=\mathbb{R}^n\times\left(\mathbb{R}-\left\{0\right\}\right)$$ where $\times$ means Cartesian product.  Then $$\hat{\mathcal{G}}=\left(\mathbb{R}^{n+1}-\mathbb{R}^n\right)\cap\partial\mathcal{O}_f.$$ But long ago I learned that $\mathbb{R}^n$ is the set of all real number n-tuples, and $\mathbb{R}^{n+1}$ is the set of all (n+1)-tuples, so elements of $\mathbb{R}^{n}$ are not elements of $\mathbb{R}^{n+1}$ and $\mathbb{R}^{n}$ is not a subset of $\mathbb{R}^{n+1}.$ So am I correct in concluding that $\mathbb{R}^{n+1}-\mathbb{R}^n$ is not really the relative complement of the two sets?	multivariable-calculus,elementary-set-theory,vector-spaces
A.378	Do exponent rules follow different rules from radicals	Does $\left(-3\right)^\frac{2}{2}$ not equal $\sqrt{\left(-3\right)^2}$?	exponential-function
A.379	Properties of a set of all isomorphisms $ f: G \to G $	I'm kinda stuck with this task. Let $G$ be a group and $ S $ the set of all isomorphisms $ f: G \to G$. I first want to show that $ (S, \circ) $ is also a group. I believe I've shown that all the properties of a group is fulfilled with $ (S, \circ) $: i) Assume that $ x \in $ and $ f_1,f_2 \in S$. Then $f_2(x) = y \in G$, since $ f_1 $ is an ismorphism. $ f_2(y) = z \in G $ since $ f_2 $ is an ismorphism. Then $ f_1(f_2(x)) = f_1(y) = z = f_1 \circ f_2(x),$ hence $f_1, f_2 \in S \longrightarrow f_1 \circ f_2 \in S.$ ii) $id$ is an isomorphism $ \longrightarrow id \in S$. iii) $ f $ is an isomorphism $ \longrightarrow \exists f^{-1}$, one can show that $ f^{-1} $ is also an isomorphism, $ \longrightarrow f^{-1} \in S$. Now, assume that $ | S | = 1$, then I want to show that $ G $ is abelian and each element has an order of 1 or 2. Im kinda lost with that $ | S | = 1 $. If $ f \in S $ then $f^{-1} \in S $ due to previous result, should not $ | S | = 1 $ imply that $ f = f^{-1} = id$? And how do I move forward from this? Any hints is muy appreciated!	group-theory,elementary-number-theory,group-isomorphism,automorphism-group
A.380	If $\int_0^\pi f(t) \sin(t)dt =\int_0^\pi f(t) \cos(t)dt = 0$, then $f(x)=0$ admits two solutions	Let $f\colon [0,\pi]\to\mathbb{R}$ be a continuous function.  If $\int^{\pi}_{0}f (t) \sin(t)dt =\int^{\pi}_{0} f (t) \cos(t)dt = 0$, then $f(x)=0$ admits two solutions in $[0,\pi]$  I try to show if $f(x)>0$ and then get the contradiction but I failed to prove that, so maybe can someone help me with that? thanks in advance.	integration,analysis
A.381	$A_1 \times ... \times A_n$ is countable if $A_1, ..., A_n$ are countable	Suppose that $A_1, ..., A_n$ are countable sets. Show that the cartesian product $A := A_1 \times ... \times A_n$ is countable. My attempt: Sets are said to be countable if they are finite or if they have the same cardinality as some subset of $\mathbb{N}$ (i.e. we can find some bijection $f: A \rightarrow S$ or $f: S \rightarrow A$ where $S \subset \mathbb{N}$). Assume that $A_1, ..., A_n$ are countable sets. Then, there exists bijections $fi: \mathbb{N} \rightarrow A_i$ for $i = 1, ..., n$. Define $g: \mathbb{N} \rightarrow A$ as follows My issue arises here in finding such a bijective function without it being too complicated. How would I go about finding one? I am also open to any suggestions. Any assistance is welcomed.	elementary-set-theory,induction
A.382	Set of functions from $SS = \{ A_{1}, A_{2}, A_{3},...\}$ to $\{ \mathbf{T}, \mathbf{F} \} $ is countable?	Let $SS=\{ A_1,A_2,A_3,\ldots\}$, and let $V = \{ v \mid v: SS \to \{ \mathbf{T}, \mathbf{F} \} \}$ .Is the set  V countable?  Justify your answer. My instinct is to say that $V = \{ v \mid v: SS \to \{ \mathbf{T}, \mathbf{F} \} \}$ is uncountable, and to prove this using a diagonalization argument, i.e. create a table of values of the functions $v_{1}, v_{2},...$ for natural numbers $n_1, n_2,\ldots$ and define a function $v_{m} : SS \to \{ \mathbf{T}, \mathbf{F} \}$ where it takes all values in the diagonals of the table $T, F$ and flips them, and then show that $v_{m}$ cannot appear anywhere in the list. Is my guess correct, and if so would this be a reasonable approach to the problem?	elementary-set-theory
A.383	Automorphisms of the disk without the maximum principle	For pedagogical purposes, I am looking for an elementary proof (i.e. without resorting to the maximum principle) that $f_a(z):=\frac{z-a}{1-\overline{a}z}$ maps the unit disk into itself when $|a|<1$. The usual argument (at least usual for me) is to look at $f_a(e^{it})$ and check that $|f_a(e^{it})|=1$. Then we are done by the maximum principle and the fact that $f_a(a)=0$. However, I cannot help but think that there should be an elementary way to do this, using only basic facts about complex numbers such as the triangle inequality. Unfortunately I am running into circles.	complex-analysis,complex-numbers
A.384	What does this bracket notation mean?	I am currently taking MIT6.006 and I came across this problem on the problem set. Despite the fact I have learned Discrete Mathematics before, I have never seen such notation before, and I would like to know what it means and how it works, Thank you: $$ f_3(n) = \binom n2$$ (Transcribed from screenshot)	discrete-mathematics,algorithms
A.385	Properties of size function in a general Euclidean domain	In ring theory a given ring $R$ is called a Euclidean domain if there exists a function $\sigma:R -\{0\}\rightarrow \{0,1,2,3...\}   $ which satisfies the division algorithm i.e. $ $ if $a,b \in R$ then there exists $q,r \in R$ such that  $b=aq+r$ and either $r=0$ or $\sigma(r)\lt \sigma(a) $ Now I want to ask if we can prove, using just this definition that an element of larger degree won't divide an element of smaller degree. In specific rings such as (i) the integers we can say$$\sigma(a)=|a|$$ $$\sigma(ab)=\sigma(a)\sigma(b)$$ ii) polynomials where$$\sigma(f(x))=deg(f(x))$$$$\sigma(ab)=\sigma(a)+\sigma(b)$$ So in both the above cases the size of product of two elements will always be greater than or equal to the size of individual elements and hence the larger element can never divide the smaller element. But is this true in general for all Euclidean domains ? And how will we prove that	ring-theory,factoring,euclidean-domain
A.386	Prove that $\sum _{n=-\infty }^{\infty } e^{-n^2 \pi x}=\frac{1}{\sqrt{x}}\sum _{n=-\infty }^{\infty } e^{-\frac{n^2 \pi }{x}}$	"In Wikipedia's proof of Riemann's functional equation for the zeta function (here, and click ""Show Proof""), I find the assertion that $$\sum _{n=-\infty }^{\infty } e^{-n^2 \pi x} = \frac{1}{\sqrt{x}}\sum _{n=-\infty }^{\infty } e^{-\frac{n^2 \pi }{x}}$$ I can't work out how this works. Is it to do with Jacobi theta functions? Mathematica (which seems to use Jacobi's original notation) simplifies the expression on the left hand side above to the Jacobi elliptic theta function (Wikipedia here, plus the 'Auxiliary Functions' section that follows) $$ \begin{aligned} \sum _{n=-\infty }^{\infty } e^{-n^2 \pi x} &= \vartheta_{3}(0,e^{-\pi  x}) \\&= \vartheta_{00}(0,e^{-\pi  x}) \\&= \vartheta(0,e^{-\pi  x}) \end{aligned} $$ Wikipedia defines $$\vartheta(z;\tau) := \sum _{n=-\infty }^{\infty } e^{\pi i n^2 \tau+2 \pi i n z}$$ But setting $z = 0$ and $\tau = e^{-\pi  x}$ then gives $$\vartheta(0,e^{-\pi  x}) = \sum _{n=-\infty }^{\infty } e^{\pi i n^2 e^{-\pi  x}}$$ which is clearly not equivalent to the original expression. I suspect that this may have something to do with nomes, which Wikipedia mentions but I cannot get my head around. So, my two questions are:  How do I prove the original equivalence? What am I doing wrong in relation to the Jacobi theta function?"	proof-explanation,riemann-zeta,theta-functions
A.387	Any positive integer greater than $11$ is a nonnegative linear combination of $5$ and $4$. My solution	Let $n\in\mathbb{Z}^{+}$, then there exists $k\in\mathbb{Z}_0^+$, such that $n=5k + i, i\in\{0,1,2,3,4\}$. Now analyzing by cases we have:  If $i=0$, then $\begin{align*}         n = 5k \Rightarrow n = 5k + 4(0). \end{align*}$ If $ i = 1 $, then $\begin{align*}        n & = 5k + 1 \\          & = 5k-5(3) +5(3) +1 \\          & = 5(k-3) + 15 + 1 \\          & = 5(k-3) +16 \Rightarrow n = 5(k-3) +4(4). \end{align*}$ If $ i = 2 $, then $\begin{align*}    n & = 5k + 2 \\      & = 5k-5(2) +5(2) +2 \\      & = 5(k-2) + 10 + 2 \\      & = 5(k-2) +12 \Rightarrow n = 5(k-2) +4(3). \end{align*}$ If $i=3$, then $\begin{align*}    n & = 5k + 3 \\      & = 5k-5 + 5 + 3 \\      & = 5(k-1) +8 \Rightarrow n = 5(k-1) +4(2). \end{align*}$ If $i=4$, then $\begin{align*}     n = 5k + 4 \Rightarrow n = 5k + 4(1). \end{align*}$  Thus, every positive number can be expressed as a linear combination of $5$ and $4$. Now using that $n>11$, so we have: $\begin{align*} n       &> 11 \\ 5k + i  &> 5(2) +1 \\ 5k-5(2) &> 1-i \\ 5 (k-2) &> 1-i \\ k-2     &> \frac{1-i}{5} \\ k       &> 2+\frac{1-i}{5}. \end{align*}$ So by increasing over the values ​​that $ i $ takes, we have: $\begin{align*} k &> 2+ \frac{1-i}{5} \geq 2+ \frac{1-0}{5}\\ k &> 2 + 0.2 = 2.2 \end{align*}$ But $k\in\mathbb{Z}_0^+ \Rightarrow k \geq 3 \Rightarrow n \geq 15 $. Thus we have that every positive integer greater than or equal to $15$ is a non-negative linear combination of $5$ and $4$. Finally, let's look at the cases that are still unverified, which are $12$, $13$ and $14$. $\begin{align*} 12 &= 5(0) +4(3) \\ 13 &= 5(1) +4(2) \\ 14 &= 5(2) +4(1). \end{align*}$ Therefore, any positive integer greater than $11$ is a nonnegative linear combination of $5$ and $4$. I think this is the correct solution, I await your comments. If anyone has a different solution or correction of my work I will be grateful.	elementary-number-theory
A.388	Linear algebra find $k$	Given the linear system: $$\begin{cases}  x_1 + kx_2 − x_3 = 2\\  2x_1 − x_2 + kx_3 = 5 \\ x_1 +10x_2 −6x_3 =1 \end{cases}$$ for which values of $k$ has the system (2): (a) No solutions (b) A unique solution. (c) Infinitely many solutions. I've been trying echelon form where i switched $R_1$ with $R_3$ and then i switched $R_2$ with $R_3$ So I have $\left[\begin{array}{ccc|c}1&10&-6&1\\1&k&-1&2\\2&-1&k&5\end{array}\right]$ but then I'm stuck and don't know how to get any further.	linear-algebra
A.389	Convergence of a Special Series as N is large	I'm trying to find a general formula for the series and x is a constant: $$\sum\limits_{i=1}^N  \frac{i}{(1+r)^i}$$ I have deduced the general formula for the sum. $$\frac{(1+r)^{N+1}-(1+r)-rN}{r^2(1+r)^N}$$ Will this sum converge to some value when N is very large? Could someone explain how to deal with it?	summation
A.390	Prove that if $x_n$ converges to $\omega$, $t_n$ converges to $\omega$ too	"By the sequence $(x_n)_{n\in\Bbb{N}}$, define a new sequence $(t_n)_{n\in\Bbb{N}}$ such that $t_n:=\frac {x_1+x_2+...+x_n}{n}$. If $\lim_{n\rightarrow\infty}t_n=\omega$, how can I show that $\lim_{t\rightarrow\infty}x_n=\omega$? Original post had ``If $\lim_{n\rightarrow\infty}x_n=\omega$, ..."""	sequences-and-series,convergence-divergence
A.391	$C_r $ inequality	Show that for each $r> 0$ $$\mathbb{E} |X+Y|^r \leq c_r (\mathbb{E} |X|^r + \mathbb{E} |Y|^r),$$ where $c_r$ is a constant given by $\begin{equation}    c_r = \left\{         \begin{array}{ll}    1 & \mathrm{if\ } 0 < r \le 1 \\    2^{r-1}     & \mathrm{if\ } 1 < r         \end{array}       \right. \end{equation}$ I've tried to use other inequalities for the proof of this one but I still get stuck for the case of $2^{r-1}$.	probability-theory,probability-distributions
A.392	How to compute inverse polynomials modulo an integer	I am working with polynomials in the ring $\mathbb{Z}[X]/(X^n-1)$, so only polynomials with degree at most $n-1$ are allowed, and multiplications must be reduced modulo $X^n-1$. The thing is that I have a polynomial $f(X)$ and I want to compute its inverse modulo an integer $p$, $f_p(X)$, such that $$ f * f_p \equiv 1 \bmod p $$ How could I do that? Any known algorithm? I have heard about the extended Euclidean algorithm, but I'm not quite sure how to use it, for example, given $f(X) = -1+X+X^2-X^4+X^6+X^9-X^{10}$.	polynomials,ring-theory,euclidean-algorithm
A.393	Prove an entire function is a constant under an inequality	f is an entire function, suppose $|f(z^{2})| \leq  2|f(z)|$ for all C, then f is a constant. I 'm trying to use Liouville's theorem, but it seems that it isn't helpful.	complex-analysis
A.394	Trying to find the $\delta$ in epsilon-delta continuity proof.	I am trying to prove the following function is continuous for all irrationals: $f(x) = \begin{cases} 0,  & \text{if $x$ is irrational} \\ 1/n, & \text{if $x = m/n$} \end{cases}$ The question assumes $m/n$ is in lowest terms. I have shown that it is discontinuous for all rationals, and now I believe I have to either use the $\epsilon-\delta$ definition of continuity or sequential continuity to show the function is continuous for when $x$ is irrational. I split my attempt into two cases: Our value of $x$ is irrational, then I want: $$\forall \epsilon > 0, \exists \delta > 0, |x-a| $a$ is irrational here. Using that $x$ is irrational I get that $f(x) = 0$ as does $f(a)$ so no matter the $\delta$ we have our condition for continuity  satisfied as $0 < \epsilon$ for all $\delta$ Our value of $x$ is rational i.e. $x = \frac{m}{n}$ subbing in we want: $$\forall \epsilon > 0, \exists \delta > 0, |\frac{m}{n}-a| I am struggling to find the $\delta$ necessary. I am able to bound $|\frac{m}{n}-a|$ by $1+2|a|$ if I say that $\delta \le 1$. However I do not know how to find a $\delta$ to yield the second inequality. Should I change my approach to sequential continuity?	continuity,epsilon-delta
A.395	Why we have to check both additivity and homogenity for linearity?	$f: V \to W $ over $K$ with $a,b \in V$ and $k \in K$. Additivity: $f(a+b) = f(a) + f(b)$ Homogenity: $k*f(b) =f(k * b)$ I have a visual understanding that a function is linear if the structure is kept while projecting it with $f$ but why it is not enough to check if the function is additive? I would be glad to have some easy examples and an intuition why we would have to check both conditions.	linear-algebra
A.396	What is the MLE $\theta^*$ of $\theta$?	I have that $x_1, x_2,...,x_n$ are from a rv $X$ that has the density function $f_X(x)=\frac{2x}{\theta^2} \quad$ for $0 \le x \le \theta  \quad$ and $f_X(x)=0 \quad$ otherwise. Ihave to determine the MLE of $\theta^*$ of $\theta$  Here is how I have done it:  $L(\theta)= \frac{2}{\theta^{2n}}\prod_{i=1}^nx_i$  $\frac{\partial L(\theta)}{\partial \theta} =...=\frac{-4n}{\theta^{2n+1}}\prod_{i=1}^nx_i + \frac{2}{\theta^{2n}}\frac{\partial(\prod_{i=1}^nx_i)}{\partial \theta}$  Is this correct? and also how do I calculate the CDF $F_{\theta^*}$, the pdf $f_{\theta^*}$ and the expectation $E[\theta^*]$ of the maximum likelihood estimator $\theta^*$?	statistics,statistical-inference,maximum-likelihood,cumulative-distribution-functions,robust-statistics
A.397	If the limit does not converge, can the sum? Or, how could the sum converge?	Alright, I thought I had seen everything but last night I saw this identity (`twas attributed to Ramanujan), $$ 1 + 2 + 3 + 4 + \cdots = -\frac{1}{12} $$ Then I saw a proof that was seemingly correct. So alright, I believe it, hey it is no crazier than having infinities of different sizes and I finally have some closure with that fact. But then, I recalled the benchmark induction proof everyone learns, $$ \sum_{i=1}^{n} i = \frac{n(n+1)}{2} $$ Then kicks in the remains of all those calculus courses I once took, making me thing that, hey wait! We have this, $$ \lim_{n\rightarrow\infty} \frac{n(n+1)}{2} = \infty $$ I think in this case we said the limit does not exist or the function diverges (correct me if I am wrong!) But... but... according to the identity above, $$ \sum_{i=1}^{n=\infty} i = -\frac{1}{12} $$ But then shouldn't, $$ \lim_{n\rightarrow\infty} \frac{n(n+1)}{2} \stackrel{?}{=} -\frac{1}{12} $$ So what I am seeing here is that even if the limit does not converge, the sum does. Also, a long time ago I remember being told that the sum of two positive integers is always positive. Furthermore, addition is suppose to be closed under integers right? Here we not only have a negative number as a result of the sum of positive integers but a negative non-integer at that.	limits,summation
A.398	A question on differentiability at a point	Is a continuous function differentiable at $x=c$ if the limit of its derivative has a value at that point? That is, if $$\lim_{x \rightarrow c} f'(x) = L = \lim_{x \rightarrow c^+} f'(x) =\lim_{x \rightarrow c^-} f'(x)$$ Intuitively, the slopes of the tangents approach the same value and since the function is continuous a jump-discontinuity isn't possible so it appears the slope at the point should be $L$ too. However, I cannot seem to locate such a theorem, so I suspect my intuition is wrong. Is it?	real-analysis,calculus
A.399	Disjoint axis-aligned rectangles in the plane	Let $A$ be some set of axis-aligned rectangles in the plane, each pair of which has empty intersection. Prove that $A$ is a countable set. (An axis-aligned rectangle is a set of the form $$M = {\{\langle x,y \rangle \in \mathbb{R^2} | a \leq x \leq b , c \leq y \leq d}\}$$ for $a,b,c,d$ such that $a < b$ and $ c < d$.) Attempt: I tried using the density of the $\mathbb{Q}$ in $(\mathbb{R},\leq)$, but without any success.	real-analysis,combinatorics
A.400	What is the probability hat two particular players verse in Wimbledon if it begins with $16$ players?	Sixteen people play in the quarter-finals at Wimbledon. The winner of the quarter-finals play again in the semi-final to decide who enters the finals. What is the probability that two particular people will play each other if the tournament begins with 16 players? So I have so far (case 1 + case 2) = verse player in quarter OR verse player in semi = $\frac{1}{15} + \frac{14}{15}...$ I'm not sure what else to include in the second case Also the third part of the question asks What is the probability when $2^n$ players begin? The worked solutions show $\frac{1}{2^n-1}+\frac{2^n-2}{2^n-1}\left(\frac{1}{2^{n-2}}\right)^2\:=\:\frac{1}{2^{n-1}}$ which I cannot get. Even symbolab doesn't show the same simplification. As with the previous question, I do not understand the $\left(\frac{1}{2^{n-2}}\right)^2$	probability,combinatorics,algebra-precalculus
