A.1	Finding value of $c$ such that the range of the rational function $f(x) = \frac{x^2 + x + c}{x^2 + 2x + c}$ does not contain $[-1, -\frac{1}{3}]$	I am comfortable when I am asked to calculate the range of a rational function, but how do we do the reverse? I came across this problem. If $$f(x)= \frac{x^2 + x + c}{x^2 + 2x + c}$$ then find the value of $c$ for which the range of $f(x)$ does not contain $[-1, -\frac{1}{3}]$.	functions
A.1	Encontrar el valor de $c$ de tal manera que el rango de la función racional $f(x) = \frac{x^2 + x + c}{x^2 + 2x + c}$ no contenga $[-1, -\frac{1}{3}]$	Me siento cómodo cuando me piden que calcule el rango de una función racional, pero ¿cómo hacemos lo contrario? Me encontré con este problema. si $$f(x)= \frac{x^2 + x + c}{x^2 + 2x + c}$$ entonces encontrar el valor de $c$ para el cual el rango de $f(x)$ no contiene $[-1, -\frac{1}{3}]$.	functions
A.2	Solving differential equations of the form $f'(x)=f(x+1)$	How to solve differential equations of the following form:  $\frac{df}{dx} = f(x+1)$	ordinary-differential-equations
A.2	Resolución de ecuaciones diferenciales de la forma $f'(x)=f(x+1)$	Cómo resolver ecuaciones diferenciales de la siguiente forma: $\frac{df}{dx} = f(x+1)$	ordinary-differential-equations
A.3	Approximation to $\sqrt{5}$ correct to an exactitude of $10^{-10}$	I am attempting to resolve the following problem:  Find an approximation to $\sqrt{5}$ correct to an exactitude of $10^{-10}$ using the bisection algorithm.  From what I understand, $\sqrt{5}$ has to be placed in function of $x$ but I am not sure where to go from there. Also, a function in Mathematica are given to do the calculations in which the function $f(x)$, $a$ and $b$ (from the interval $[a, b]$ where $f(a)$ and $f(b)$ have opposite signs), the tolerance and the number of iterations.	numerical-methods,algorithms,bisection
A.3	Aproximación a la corrección $\sqrt{5}$ con una exactitud de $10^{-10}$	Estoy tratando de resolver el siguiente problema: encontrar una aproximación a $\sqrt{5}$ correcta a una exactitud de $10^{-10}$ usando el algoritmo de bisección. Por lo que entiendo, $\sqrt{5}$ debe colocarse en función de $x$ pero no estoy seguro de dónde ir desde allí. También, se da una función en Matemáticas para hacer los cálculos en los que la función $f(x)$, $a$ y $b$ (desde el intervalo $[a, b]$ donde $f(a)$ y $f(b)$ tienen signos opuestos), la tolerancia y el número de iteraciones.	numerical-methods,algorithms,bisection
A.4	How to compute this combinatoric sum?	I have the sum $$\sum_{k=0}^{n} \binom{n}{k} k$$ I know the result is $n 2^{n-1}$ but I don't know how you get there. How does one even begin to simplify a sum like this that has binomial coefficients.	combinatorics,number-theory,summation,proof-explanation
A.4	¿Cómo calcular esta suma combinatoria?	Tengo la suma $$\sum_{k=0}^{n} \binom{n}{k} k$$ sé que el resultado es $n 2^{n-1}$ pero no sé cómo llegar allí. ¿Cómo se comienza a simplificar una suma como esta que tiene coeficientes binomial.	combinatorics,number-theory,summation,proof-explanation
A.5	A family has two children. Given that one of the children is a boy, what is the probability that both children are boys?	A family has two children. Given that one of the children is a boy, what is the probability that both children are boys?  I was doing this question using conditional probability formula.  Suppose, (1) is the event, that the first child is a boy, and (2) is the event that the second child is a boy. Then the probability of the second child to be boy given that first child is a boys by formula, $P((2)|(1))=\frac{P((2) \cap (1))}{P((1))}=\frac{P((2))P((1))}{P((1))} = P((2))$ ...since second child to be boy doesn't depend on first child and vice versa. Please provide the detailed solution and correct me if I am wrong.	probability,proof-verification,conditional-probability
A.5	¿Cuál es la probabilidad de que ambos hijos sean niños, dado que uno de ellos es un niño?	Una familia tiene dos hijos. Dado que uno de los niños es un niño, ¿cuál es la probabilidad de que ambos niños sean niños? Estaba haciendo esta pregunta usando la fórmula de probabilidad condicional. Supongamos, (1) es el evento, que el primer niño es un niño, y (2) es el evento de que el segundo niño es un niño. Entonces la probabilidad del segundo niño es un niño dado que el primer niño es un niño por fórmula, $P((2)|(1))=\frac{P((2) \cap (1))}{P((1))}=\frac{P((2))P((1))}{P((1))} = P((2))$ ...ya que el segundo niño es un niño no depende del primer niño y viceversa. Por favor proporcione la solución detallada y corrija si estoy equivocado.	probability,proof-verification,conditional-probability
A.6	How to calculate mod of number with big exponent	I want to find $$ 5^{133} \mod 8. $$ I have noticed that $5^n \mod 8 = 5$ when $n$ is uneven and 1 otherwise, which would lead me to say that $5^{133} \mod 8 = 5$ But I don't know how to prove this. How can I prove that this is the case (or find another solution if it is not)?	algebra-precalculus,arithmetic
A.6	Cómo calcular el mod de número con un gran exponente	Quiero encontrar $$ 5^{133} \mod 8. $$ He notado que $5^n \mod 8 = 5$ cuando $n$ es desigual y 1 de otra manera, lo que me llevaría a decir que $5^{133} \mod 8 = 5$ Pero no sé cómo probar esto. ¿Cómo puedo probar que este es el caso (o encontrar otra solución si no es)?	algebra-precalculus,arithmetic
A.7	Finding out the remainder of $\frac{11^\text{10}-1}{100}$ using modulus	If $11^\text{10}-1$ is divided by $100$, then solve for '$x$' of the below term $$11^\text{10}-1 = x \pmod{100}$$  Whatever I tried: $11^\text{2} \equiv 21 \pmod{100}$.....(1) $(11^\text{2})^\text{2} \equiv (21)^\text{2} \pmod{100}$ $11^\text{4} \equiv 441 \pmod{100}$ $11^\text{4} \equiv 41 \pmod{100}$ $(11^\text{4})^\text{2} \equiv (41)^\text{2} \pmod{100}$ $11^\text{8} \equiv 1681 \pmod{100}$ $11^\text{8} \equiv 81 \pmod{100}$ $11^\text{8} × 11^\text{2} \equiv (81×21) \pmod{100}$ ......{from (1)} $11^\text{10} \equiv 1701 \pmod{100} \implies 11^\text{10} \equiv 1 \pmod{100}$ Hence, $11^\text{10} -1 \equiv (1-1) \pmod{100} \implies 11^\text{10} - 1 \equiv 0 \pmod{100}$ and thus we get the value of $x$ and it is $x = 0$ and $11^\text{10}-1$ is divisible by $100$. But this approach take a long time for any competitive exam or any math contest without using calculator. Any easier process on how to determine the remainder of the above problem quickly? That will be very much helpful for me. Thanks in advance.	elementary-number-theory,modular-arithmetic,divisibility,alternative-proof
A.7	Identificación del resto de $\frac{11^\text{10}-1}{100}$ utilizando el módulo	Si $11^\text{10}-1$ se divide por $100$, entonces resuelve '$x$' del siguiente término $$11^\text{10}-1 = x \pmod{100}$$ Lo que sea que yo probado: $11^\text{2} \equiv 21 \pmod{100}$.....(1) $(11^\text{2})^\text{2} \equiv (21)^\text {2} \pmod{100}$ $11^\text{4} \equiv 441 \pmod{100}$ $11^\text{4} \equiv 41 \pmod{100}$ $(11^\text{4} )^\text{2} \equiv (41)^\text{2} \pmod{100}$ $11^\text{8} \equiv 1681 \pmod{100}$ $11^\text{8} \equiv 81 \pmod{100}$ $11^\text{8} × 11^\text{2} \equiv (81×21) \pmod{100}$ ......{de (1)} $11^\text {10} \equiv 1701 \pmod{100} \implica 11^\text{10} \equiv 1 \pmod{100}$ Por lo tanto, $11^\text{10} -1 \equiv (1-1) \pmod{ 100} \implies11^\text{10} - 1 \equiv 0 \pmod{100}$ y así obtenemos el valor de $x$ y es $x = 0$ y $11^\text{10}-1 $ es divisible por $100$. Pero este enfoque lleva mucho tiempo para cualquier examen competitivo o concurso de matemáticas sin usar la calculadora. ¿Algún proceso más sencillo sobre cómo determinar rápidamente el resto del problema anterior? Eso me será de mucha ayuda. Gracias de antemano.	elementary-number-theory,modular-arithmetic,divisibility,alternative-proof
A.8	finding value of $\lim_{n\rightarrow \infty}\sqrt[n]{\frac{(27)^n(n!)^3}{(3n)!}}$	Finding value of $\lim_{n\rightarrow \infty}\sqrt[n]{\frac{(27)^n(n!)^3}{(3n)!}}$ what i try $\displaystyle l=\lim_{n\rightarrow \infty}\bigg(\frac{(27)^n(n!)^3}{(3n)!}\bigg)^{\frac{1}{n}}$ $\displaystyle \ln(l)=\lim_{n\rightarrow \infty}\frac{1}{n}\bigg[n\ln(27)+3\ln(n!)-\ln((3n)!)\bigg]$ How do i solve it help me please	limits
A.8	hallar el valor de $\lim_{n\rightarrow \infty}\sqrt[n]{\frac{(27)^n(n!)^3}{(3n)!}}$	Encontrar el valor de $\lim_{n\rightarrow \infty}\sqrt[n]{\frac{(27)^n(n!)^3}{(3n)!}}$ lo que intento $\displaystyle l=\lim_{n\rightarrow \infty}\bigg(\frac{(27)^n(n!)^3}{(3n)!}\bigg)^{\frac{1}{n}}$ $\displaystyle \ln(l)=\lim_{n\rightarrow \infty}\frac{1}{n}\bigg[n\ln(27)+3\ln(n!)-\ln((3n)!)\bigg]$ Cómo puedo resolverlo ayúdame por favor	limits
A.9	Simplifying this series	I need to write the series  $$\sum_{n=0}^N nx^n$$  in a form that does not involve the summation notation, for example $\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$. Does anyone have any idea how to do this? I've attempted multiple ways including using generating functions however no luck	sequences-and-series
A.9	Simplificando esta serie	Necesito escribir la serie $$\sum_{n=0}^N nx^n$$ en una forma que no involucre la notación de suma, por ejemplo $\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$. ¿Alguien tiene alguna idea de cómo hacer esto? he intentado varias formas incluyendo el uso de funciones de generación pero sin suerte	sequences-and-series
A.10	Find the values of a>0 for which the improper integral $\int_{0}^{\infty}\frac{\sin x}{x^{a}} $ converges .	Find the values of a>0 for which the improper integral $\int_{0}^{\infty}\frac{\sin x}{x^{a}} $ converges .  Do  I have to expand integrand using series expansion??	improper-integrals
A.10	Encuentra los valores de a>0 para los cuales converge la integral incorrecta $\int_{0}^{\infty}\frac{\sin x}{x^{a}} $ .	Encuentra los valores de a>0 para los cuales converge la integral incorrecta $\int_{0}^{\infty}\frac{\sin x}{x^{a}} $. ¿Tengo que expandir la integrand utilizando la expansión de serie?	improper-integrals
A.11	What's the cross product in 2 dimensions?	The math book i'm using states that the cross product for two vectors is defined over $R^3$: $$u = (a,b,c)$$ $$v = (d,e,f)$$ is: $$u \times v = \begin{vmatrix} \hat{i} & \hat{j} & \hat{k} \\ a & b & c \\ d & e & f \\ \end{vmatrix} $$ and the direction of the resultant is determined by curling fingers from vector v to u with thumb pointing in direction of the cross product of u x v.  Out of curiosity, what's the cross product if u and v are defined over $R^2$ instead of $R^3$ instead: $$u = (a,b)$$ $$v = (d,e)$$ Is there a "degenerate" case for the cross product of $R^2$ instead $R^3$?  like this is some type of 2x2 determinant instead? for instance if had a parameterization: $$\Phi(u,\ v) = (\ f(u),\ \ g(v)\ )$$ and needed to calculate in $R^2$: $$ D = \Bigg| \frac{\partial{\Phi}}{\partial{u}} \times \frac{\partial{\Phi}}{\partial{v}} \Bigg| $$ There are plenty of examples in the book for calculating the determinate D in $R^3$ but none at all for $R^2$ case. As in: $$ \iint_{V} f(x,y) dx\ dy = \iint_{Q} f(\Phi(u,v) \Bigg| \frac{\partial{\Phi}}{\partial{u}} \times \frac{\partial{\Phi}}{\partial{v}} \Bigg| $$ $$ \Phi(u,v)=(2u \cos v,\ \ u \sin v) $$	multivariable-calculus,vectors
A.11	¿Cuál es el producto cruzado en 2 dimensiones?	El libro de matemáticas que estoy usando establece que el producto cruzado de dos vectores se define sobre $R^3$: $$u = (a,b,c)$$ $$v = (d,e,f)$$ es: $$u \times v = \begin{vmatrix} \hat{i} & \hat{j} & \hat{k} \\ a & b & c \\ d & e & f \\ \end{ vmatrix} $$ y la dirección de la resultante se determina doblando los dedos desde el vector v hacia u con el pulgar apuntando en la dirección del producto cruzado de u x v. Por curiosidad, ¿cuál es el producto cruzado si u y v se definen sobre $R? ^2$ en lugar de $R^3$ en lugar de: $$u = (a,b)$$ $$v = (d,e)$$ ¿Existe un caso "degenerado" para el producto cruzado de $R^2? $ en lugar de $R^3$? ¿Es así algún tipo de determinante 2x2? por ejemplo, si tuviera una parametrización: $$\Phi(u,\ v) = (\ f(u),\ \ g(v)\ )$$ y necesitara calcular en $R^2$: $$ D = \Bigg| \frac{\partial{\Phi}}{\partial{u}} \times \frac{\partial{\Phi}}{\partial{v}} \Bigg| $$ Hay muchos ejemplos en el libro para calcular el D determinado en $R^3$ pero ninguno en absoluto para el caso $R^2$. Como en: $$ \iint_{V} f(x,y) dx\ dy = \iint_{Q} f(\Phi(u,v) \Bigg| \frac{\partial{\Phi}}{\partial {u}} \times \frac{\partial{\Phi}}{\partial{v}} \Bigg| $$ $$ \Phi(u,v)=(2u \cos v,\ \ u \sin v ) $$	multivariable-calculus,vectors
A.12	Finding the roots of a complex number	I was solving practice problems for my upcoming midterm and however I got stuck with this question type. It is asking me to find all roots and then sketch it. $(1+i\sqrt{3})^{1/2}$ How do we proceed?	linear-algebra,complex-numbers,polar-coordinates
A.12	Encontrar las raíces de un número complejo	Estaba practicando problemas para mi examen de mitad de periodo y aun asi me quede estancado con este tipo de pregunta. Me esta preguntando encontrar todas las raíces y luego hacer un bosqueto. $(1+i\sqrt{3})^{1/2}$ ¿Como procedemos?	linear-algebra,complex-numbers,polar-coordinates
A.13	How to simplify expression $\int_a^b f(x)dx+\int_{f(a)}^{f(b)} f^{-1}(x)dx \ ?$	How to simplify expression $$\int_a^b f(x)dx+\int_{f(a)}^{f(b)} f^{-1}(x)dx \ ?$$ The answer is $bf(b)-af(a)$  but I am wondering how to get the answer.	calculus
A.13	Cómo simplificar la expresión $\int_a^b f(x)dx+\int_{f(a)}^{f(b)} f^{-1}(x)dx \ ?$	Cómo simplificar la expresión $$\int_a^b f(x)dx+\int_{f(a)}^{f(b)} f^{-1}(x)dx \ ?$$ La respuesta es $bf(b)-af(a)$ pero me pregunto cómo obtener la respuesta.	calculus
A.14	Help solving first-order differential equation	I have first-order differential equation $$y=xy'+ \frac{1}{2}(y')^{2}$$ Maybe, with this someone will find way to solve it $$\frac{1}{2}y'(2x+y')=y$$ I thought I can use $x^2+y=t$ for subtitution and when I derivate, I have $t'=2x+y'\\(t'-2x)t'=2t-2x^2$ which is acctualy the same as previous. I don't have idea how to start..	ordinary-differential-equations
A.14	Ayuda para resolver la ecuación diferencial de primer orden	Tengo la ecuación diferencial de primer orden $$y=xy'+ \frac{1}{2}(y')^{2}$$ Tal vez, con esto alguien encontrará una manera de resolver $$\frac{1}{2}y'(2x+y')=y$$ pensé que puedo usar $x^2+y=t$ para subtitución y para cuando derivo, tengo $t'=2x+y'\\(t'-2x)t'=2t-2x^2$ que es exactamente el mismo que el anterior. No tengo idea de como iniciar.	ordinary-differential-equations
A.15	Derive the sum of $\sum_{i=1}^n ix^{i-1}$	For the series   $$1 + 2x + 3x^2 + 4x^3 + 5x^4 + ... + nx^{n-1}+... $$   and $x \ne 1, |x| < 1$. I need to find partial sums and finally, the sum $S_n$ of series. Here is what I've tried:   We can take a series $S_2 = 1 + x + x^2 + x^3 + x^4 + ...$ so that $\frac{d(S_2)}{dx} = S_1$ (source series). For the $|x| < 1$ the sum of $S_2$ (here is geometric progression): $\frac{1-x^n}{1-x} = \frac{1}{1-x}$ $S_1 = \frac{d(S_2)}{dx} = \frac{d(\frac{1}{1-x})}{dx} = \frac{1}{(1-x)^2}$  But this answer is incorrect. Where is my mistake? Thank you.	sequences-and-series,convergence,summation,power-series
A.15	Derivar la suma de $\sum_{i=1}^n ix^{i-1}$	Para las series $$1 + 2x + 3x^2 + 4x^3 + 5x^4 + ... + nx^{n-1}+... $$ y $x \ne 1, |x| < 1$. Necesito encontrar sumas parciales y finalmente, la suma $S_n$ de series. Esto es lo que he intentado: podemos tomar una serie $S_2 = 1 + x + x^2 + x^3 + x^4 + ...$ para que $\frac{d(S_2)}{dx} = S_1$ (serie fuente). Para el $|x| < 1$ la suma de $S_2$ (aquí esta la progresión geométrica): $\frac{1-x^n}{1-x} = \frac{1}{1-x}$ $S_1 = \frac{d(S_2)}{dx} = \frac{d(\frac{1}{1-x})}{dx} = \frac{1}{(1-x)^2}$ Pero esta respuesta es incorrecta. ¿Dónde está mi error? Gracias	sequences-and-series,convergence,summation,power-series
A.16	Finding $ \int_0^1\frac{\ln(1+x)\ln(1-x)}{1+x}dx$	Calculate   $$\int_0^1\frac{\ln(1+x)\ln(1-x)}{1+x}\,dx$$  My try :  Let : $$I(a,b)=\int_0^1\frac{\ln(1-ax)\ln(1+bx)}{1+x}\,dx$$ Then compute $\frac{d^2 I(a,b)}{dadb}$. I'm happy to see ideas in order to kill this integral.	integration,sequences-and-series,definite-integrals,closed-form
A.16	Encontrar $ \int_0^1\frac{\ln(1+x)\ln(1-x)}{1+x}dx$	Calculo $$\int_0^1\frac{\ln(1+x)\ln(1-x)}{1+x}\,dx$$ Mi intento: Deja: $$I(a,b)=\int_0^1\frac{\ln(1-ax)\ln(1+bx)}{1+x}\,dx$$ Luego calcule $\frac{d^2 I(a,b)}{dadb}$. Estoy feliz de ver ideas para matar esta integral.	integration,sequences-and-series,definite-integrals,closed-form
A.17	Calculate $\int _{x=0}^{\infty} \frac{\sin(x)}{x}$ with the function $\frac{e^{iz}}{z}$	I want to calculate $\int _{x=0}^{\infty} \frac{\sin(x)}{x}$ with the function $f(z) = \frac{e^{iz}}{z}$. I thought about using the closed path $\Gamma = \gamma _1 + \gamma _R + \gamma _2 + \gamma _{\epsilon}$, when: $\gamma_1 (t) = t, t \in [i\epsilon, iR]$ $\gamma_R (t) = Re^{it}, t \in [-\frac{\pi}{2}, \frac{\pi}{2}]$ $\gamma_2 (t) = t, t \in [-iR, -i\epsilon]$ $\gamma_{\epsilon} (t) = \epsilon e^{it}, t \in [-\frac{\pi}{2}, \frac{\pi}{2}]$ I use the fact that $\frac{\sin(x)}{x}$ is an even function and has an anti derivative, so the integral on a closed path is zero. I managed to show that $\int_{\gamma _{\epsilon}} f = -i\pi$ when $\epsilon \to 0$. However I am struggling to show that $\int_{\gamma _R} f = 0$ when $R \to \infty$ Help would be appreciated	complex-analysis,improper-integrals
A.17	Calcule $\int _{x=0}^{\infty} \frac{\sin(x)}{x}$ con la función $\frac{e^{iz}}{z}$	Quiero calcular $\int _{x=0}^{\infty} \frac{\sin(x)}{x}$ con la función $f(z) = \frac{e^{iz}}{z}$. pensé en usar el camino cerrado $\Gamma = \gamma _1 + \gamma _R + \gamma _2 + \gamma _{\epsilon}$, cuando: $\gamma_1 (t) = t, t \in [i\epsilon, iR]$ $\gamma_R (t) = Re^{it}, t \in [-\frac{\pi}{2}, \frac{\pi}{2}]$ $\gamma_2 (t) = t, t \in [-iR, -i\epsilon]$ $\gamma_{\epsilon} (t) = \epsilon e^{it}, t \in [-\frac{\pi}{2}, \frac{\pi}{2}]$ uso el hecho de que $\frac{\sin(x)}{x}$ es una función uniforme y tiene una derivada anti, por lo que la integral en un camino cerrado es cero. logré mostrar que $\int_{\gamma _{\epsilon}} f = -i\pi$ cuando $\int _{x=0}^{\infty} \frac{\sin(x)}{x}$0. Sin embargo, estoy luchando para mostrar que $\int _{x=0}^{\infty} \frac{\sin(x)}{x}$1 cuando $\int _{x=0}^{\infty} \frac{\sin(x)}{x}$2 Ayuda sería apreciada	complex-analysis,improper-integrals
A.18	Evaluate $\lim_{n \rightarrow \infty } \frac {[(n+1)(n+2)\cdots(n+n)]^{1/n}}{n}$	Evaluate $$\lim_{n \rightarrow \infty~} \dfrac {[(n+1)(n+2)\cdots(n+n)]^{\dfrac {1}{n}}}{n}$$ using Cesáro-Stolz theorem. I know there are many question like this, but i want to solve it using Cesáro-Stolz method and no others. I took log and applied Cesáro-Stolz, I get $$\log{2}+n\log\cfrac{n}{n+1}$$ Which gives me answer as $\frac{2}{e}$ . But answer is $\frac{4}{e}$. Could someone help?. Edit:  On taking log,  $$\lim_{n \to \infty} \frac{-n\log n + \sum\limits_{k=1}^{n} \log \left(k+n\right)}{n} \\= \lim_{n \to \infty} \left(-(n+1)\log (n+1) + \sum\limits_{k=1}^{n+1} \log \left(k+n\right)\right) - \left(-n\log n + \sum\limits_{k=1}^{n} \log \left(k+n\right)\right) \\ = \lim_{n \to \infty} \log \frac{2n+1}{n+1} - n\log \left(1+\frac{1}{n}\right) = \log 2 - 1$$ Which gives $2/e$	sequences-and-series
A.18	Evaluar $\lim_{n \rightarrow \infty } \frac {[(n+1)(n+2)\cdots(n+n)]^{1/n}}{n}$	Evaluar $$\lim_{n \rightarrow \infty~} \dfrac {[(n+1)(n+2)\cdots(n+n)]^{\dfrac {1}{n}}}{n}$$ usando el teorema de Cesáro-Stolz. Sé que hay muchas preguntas como esta, pero quiero resolverlo usando el método de Cesáro-Stolz y ninguno de los otros. Tomé el registro y aplicé Cesáro-Stolz, obtengo $$\log{2}+n\log\cfrac{n}{n+1}$$ que me da la respuesta como $\frac{2}{e}$ . Pero la respuesta es $\frac{4}{e}$. ¿Puede alguien ayudar? Edit: Tomando el log,  $$\lim_{n \to \infty} \frac{-n\log n + \sum\limits_{k=1}^{n} \log \left(k+n\right)}{n} \\= \lim_{n \to \infty} \left(-(n+1)\log (n+1) + \sum\limits_{k=1}^{n+1} \log \left(k+n\right)\right) - \left(-n\log n + \sum\limits_{k=1}^{n} \log \left(k+n\right)\right) \\ = \lim_{n \to \infty} \log \frac{2n+1}{n+1} - n\log \left(1+\frac{1}{n}\right) = \log 2 - 1$$ El cual da $2/e$	sequences-and-series
A.19	Greatest common factor of $ p^4-1$	I was asked to find the greatest common factor of $p^4-1$ for all primes > 5, First I got the value of $7^4 - 1$ which has divisors of $2^4* 3 *5*2$ and $11^4 - 1$ which has divisors $2^4 *3 * 5*61$ which has a GCF of $2^4*3*5$ I can prove that $p^4 - 1$ is divisible by 3 and 5 by casework and 8  by $(p^2+1)(p-1)(p+1)$ are even integers, but I don't know how to prove divisibility of $2^4$, I do not want to bash it since we must check about 7 numbers to prove its divisibility by assigning $16n + x$ where x <16	divisibility,greatest-common-divisor
A.19	El factor común más grande de $ p^4-1$	Me pidieron que encuentre el factor común más grande de $p^4-1$ para todos los números primos > 5, Primero obtuve el valor de $7^4 - 1$ que tiene divisores de $2^4* 3 *5*2$ y $11^4 - 1$ que tiene divisores $2^4 *3 * 5*61$ que tiene un GCF de $2^4*3*5$ puedo probar que $p^4 - 1$ es divisible por 3 y 5 por el trabajo de caso y 8 por $(p^2+1)(p-1)(p+1)$ son números pares enteros, pero no sé cómo demostrar la divisibilidad de $2^4$, no quiero romperlo ya que debemos comprobar alrededor de 7 números para probar su divisibilidad asignando $p^4-1$0 donde x <16	divisibility,greatest-common-divisor
A.20	Calculate all $n \in \Bbb N \setminus \{41\}$ such that $\phi(n)=40$?	I'm looking for an $n \in \Bbb N$ for which $\phi(n) = 40$ where $\phi$ is a Euler-Totient Function   I already found one, namely, $n=41$ How the calculate the $n's$?	totient-function
A.20	Calcular todos los $n \in \Bbb N \setminus \{41\}$ tales que $\phi(n)=40$?	Estoy buscando un $n \in \Bbb N$ para el cual $\phi(n) = 40$ donde $\phi$ es una función de Euler-Totient ya encontré uno, específicamente, $n=41$ ¿Cómo calcular el $n's$?	totient-function
A.21	Finding the last two digits of $9^{9^{9^{…{^9}}}}$ (nine 9s)	I'm continuing on my journey learning about modular arithmetic and got confused with this question: Find the last two digits of $9^{9^{9^{…{^9}}}}$ (nine 9s). The phi function is supposed to be used in this problem and so far this is what I've got: $9^{9^{9^{…{^9}}}} ≡ x (\text{mod } 100)$ Where $0 ≤ x ≤ 100$ $9^{9^{9^{…{^9}}}} \text{ (nine 9s) }= 9^a$ In order to know $9^a (\text{mod } 100)$, we need to know $a (\text{mod } \phi(100))$ As $\phi(100)= 40$, we get $a = b (\text{mod } 40)$ $9^{9^{9^{…{^9}}}} \text{ (eight 9s) }= 9^b$ In order to know $9^b (\text{mod } 40)$, we need to know $b (\text{mod } \phi(40))$ As $\phi(40)= 16$, we get $b = c ( \text{mod }16)$ $9^{9^{9^{…{^9}}}}\text{ (seven 9s) }= 9^c $ In order to know $9^c (mod 16)$, we need to know $c (\text{mod } phi(16))$ as $\phi(16)= 8 $ we need to find $c (\text{mod } 8)$ As $9 = 1 (\text{mod } 8)$ $c = 1 (\text{mod } 8)$ I feel like I might have made a mistake somewhere along the way because I'm having a lot of trouble stitching it all back together in order to get a value for the last two digits. Could anyone please help me with this? Thank you!	number-theory,modular-arithmetic
A.21	Encontrar los dos últimos dígitos de $9^{9^{9^{…{^9}}}}$ (nueve 9s)	Estoy continuando en mi viaje aprendiendo sobre la aritmética modular y me confundí con esta pregunta: encontrar los dos últimos dígitos de $9^{9^{9^{…{^9}}}}$ (nueve 9s). La función phi se supone que se utiliza en este problema y hasta ahora esto es lo que tengo: $9^{9^{9^{…{^9}}}} ≡ x (\text{mod } 100)$ Donde $0 ≤ x ≤ 100$ $9^{9^{9^{…{^9}}}} \text{ (nine 9s) }= 9^a$ Para saber $9^a (\text{mod } 100)$, necesitamos saber $a (\text{mod } \phi(100))$ Como $\phi(100)= 40$, tenemos $a = b (\text{mod } 40)$ $9^{9^{9^{…{^9}}}} \text{ (eight 9s) }= 9^b$ Para saber $9^{9^{9^{…{^9}}}}$0, tenemos que saber $9^{9^{9^{…{^9}}}}$1 Como $9^{9^{9^{…{^9}}}}$2, tenemos $9^{9^{9^{…{^9}}}}$3 $9^{9^{9^{…{^9}}}}$4 Para saber $9^{9^{9^{…{^9}}}}$5, tenemos que saber $9^{9^{9^{…{^9}}}}$6 como $9^{9^{9^{…{^9}}}}$7 necesitamos encontrar $9^{9^{9^{…{^9}}}}$8 Como $9^{9^{9^{…{^9}}}}$9 $9^{9^{9^{…{^9}}}} ≡ x (\text{mod } 100)$0 siento que podría haber cometido un error en algún lugar en el camino porque estoy teniendo muchos problemas en reunir todo de nuevo para obtener un valor para los dos últimos dígitos. ¿Podría alguien ayudarme con esto por favor gracias!	number-theory,modular-arithmetic
A.22	Find number of d's that satisfies $d, d+1, d+2... = N$ for an $N$	I have a challenge about a cat in a trip where he can walk in the way of $d, d+1, d+2...$ and the sum of that should give $N$, given an $N$, how many ways of chosing $d$ are posible? Example: $N=30$ -> $Ans=3$ $d_1=4; d_2=6; d_3=8$  For $d_1: 4 + 5 + 6 + 7 + 8 = 30$ Edit: Another way to see it is: How many subsets in the sumation up to N are posible in the way $(\sum (d+n) - \sum(d-1))$=N	sequences-and-series,number-theory
A.22	Encuentra el número de d que satisface $d, d+1, d+2... = N$ para un $N$	Tengo un reto sobre un gato en un viaje donde puede caminar en el camino de $d, d+1, d+2...$ y la suma de eso debería dar $N$, dado un $N$, ¿cuántas formas de elegir $d$ son posibles? Ejemplo: $N=30$ -> $Ans=3$ $d_1=4; d_2=6; d_3=8$ Para $d_1: 4 + 5 + 6 + 7 + 8 = 30$ Editar: Otra forma de verlo es: ¿Cuántos subconjuntos en la sumación hasta N son posibles en la forma $(\sum (d+n) - \sum(d-1))$=N	sequences-and-series,number-theory
A.23	How do i find the lcm	Qn: If the product of two integers is  $2^7 \cdot 3^8 \cdot 5^2 \cdot 7^{11}$ and their greatest common divisor is $2^3 \cdot 3^4 \cdot 5$, what is their least common multiple? I have issue with this question please help me solve it. I tried assuming that lcm is $x$ =. Then,        Gcd $\cdot x = 2^3  \cdot 3^4 \cdot 5x$. And, product factors /Gcd $x$	prime-numbers,greatest-common-divisor,least-common-multiple
A.23	¿Cómo puedo encontrar el Icm?	Pn: Si el producto de dos números enteros es $2^7 \cdot 3^8 \cdot 5^2 \cdot 7^{11}$ y su mayor divisor común es $2^3 \cdot 3^4 \cdot 5$, ¿cuál es su múltiplo común menos común? Tengo un problema con esta pregunta por favor ayúdame a resolverlo. Traté de asumir que lcm es $x$ =. Luego,        Gcd $\cdot x = 2^3  \cdot 3^4 \cdot 5x$. Y, factores del producto /Gcd $x$	prime-numbers,greatest-common-divisor,least-common-multiple
A.24	Is this the only way to evaluate $\sqrt{2i-1}?$	work out the $\sqrt{2i-1}?$ $2i-1=(a+bi)^2$ $a^2+2abi-b^2$ $a^2-b^2=-1$ $2ab=2$  $a^2=b^{-2}$ $b^{-2}-b^2=-1$ $-b^{4}+1=-1$ $b^4=2$ $b=\sqrt[4]{2}$ Can we solve $\sqrt{2i-1}$ in another way?	algebra-precalculus
A.24	¿Es esta la única manera de evaluar el $\sqrt{2i-1}?$	Resuelve $\sqrt{2i-1}?$ $2i-1=(a+bi)^2$ $a^2+2abi-b^2$ $a^2-b^2=-1$ $2ab=2$  $a^2=b^{-2}$ $b^{-2}-b^2=-1$ $-b^{4}+1=-1$ $b^4=2$ $b=\sqrt[4]{2}$ ¿Podemos resolver $\sqrt{2i-1}?$1 de otra manera?	algebra-precalculus
A.25	What can be P(0), when $P(x^2+1)=(P(x))^2+1$ and P(x) is polynomial?	What can be $P(0)$, when $P(x^2+1)=(P(x))^2+1$ and $P(x)$ is polynomial? Let $P(0)=0$, then $P(1)=1$, $P(2)=2$, $P(5)=5$, $P(26)=26$, $P(677)=677$ ... and so on. Then $P(x)=x$, because all the points on $y=P(x)$ are $y=x$. If $P(0)=2$, then $P(x)=(x^2+1)^2+1$ for the same reason. But when $P(0)=3$, we have that $\lim_{x→∞}\log_xP(x)$ does not converge into an integer. So I think $P(x)$ cannot be a polynomial. Then what are the values of $P(0)$ that makes $P(x)$ polynomial?	polynomials
A.25	¿Qué puede ser P ((0), cuando $P(x^2+1)=(P(x))^2+1$ y P ((x) es polinomio?	¿Qué puede ser $P(0)$, cuando $P(x^2+1)=(P(x))^2+1$ y $P(x)$ es polinomio? ¿Dejaremos que $P(0)=0$, entonces $P(1)=1$, $P(2)=2$, $P(5)=5$, $P(26)=26$, $P(677)=677$ ... y así sucesivamente. Entonces $P(x)=x$, porque todos los puntos en $y=P(x)$ son $y=x$. Si $P(0)$2, entonces $P(x)=(x^2+1)^2+1$ por la misma razón. Pero cuando $P(0)3$, tenemos que $\lim_{x→∞}\log_xP(x)$  no converge en un número entero. Así que creo que $P(x)$ no puede ser un polinomio. Entonces, ¿cuáles son los valores de $P(0)$ que hace $P(x)$ polinomio?	polynomials
A.26	How to solve an indefinite integral using the Taylor series?	I am trying to show that the following integral is convergent but not absolutely.  $$\int_0^\infty\frac{\sin x}{x}dx.$$  My attempt:  I first obtained the taylor series of $\int_0^x\frac{sin x}{x}dx$ which is as follows:   $$x-\frac{x^3}{3 \times 3!}+\frac{x^5}{5\times5!}-\frac{x^7}{7 \times 7!}+\cdots = \sum_{n=0}^\infty (-1)^n\frac{x^{(2n+1)}}{(2n+1) \times (2n+1)!} $$  Now $\int_0^\infty\frac{\sin x}{x}dx=\lim_{x\to \infty} \sum_{n=0}^\infty (-1)^n\frac{x^{(2n+1)}}{(2n+1) \times (2n+1)!}$ and I got stuck here! What is the next step?	real-analysis,calculus,integration,taylor-expansion,riemann-integration
A.26	¿Cómo resolver una integral indefinida usando la serie Taylor?	Estoy tratando de mostrar que la siguiente integral es convergente pero no absolutamente. $$\int_0^\infty\frac{\sin x}{x}dx.$$ Mi intento: primero obtuve la serie de Taylor de $\int_0^x\frac{sin x}{x}dx$ que es la siguiente: $$x-\frac{x^3}{3 \times 3!}+\frac{x^5}{5\times5!}-\frac{x^7}{7 \times 7!}+\cdots = \sum_{n=0}^\infty (-1)^n\frac{x^{(2n+1)}}{(2n+1) \times (2n+1)!} $$ Ahora $\int_0^\infty\frac{\sin x}{x}dx=\lim_{x\to \infty} \sum_{n=0}^\infty (-1)^n\frac{x^{(2n+1)}}{(2n+1) \times (2n+1)!}$ y me quedé atascado aquí! ¿Cuál es el siguiente paso?	real-analysis,calculus,integration,taylor-expansion,riemann-integration
A.27	What is the value of $e^{3i \pi /2}$?	When solving for the value, we know that $e^{\pi i}=-1$ . I am confused as to what is the right answer when you evaluate this.I am getting two possible answers: $e^{3\pi i/2}$ = $(e^{\pi i})^{3/2}$ so this could be $(\sqrt{-1})^3=i^3=-i$ or it could be $\sqrt{(-1)^3}=\sqrt{-1}=i$. Which one is the correct answer, and where am I going wrong? Thanks.	complex-numbers,exponentiation
A.27	¿Cuál es el valor de $e^{3i \pi /2}$?	Cuando se resuelve el valor, sabemos que $e^{\pi i}=-1$. Estoy confundido en cuanto a cuál es la respuesta correcta cuando se evalúa esto. Estoy recibiendo dos posibles respuestas: $e^{3\pi i/2}$ = $(e^{\pi i})^{3/2}$ así que esto podría ser $(\sqrt{-1})^3=i^3=-i$ o podría ser $\sqrt{(-1)^3}=\sqrt{-1}=i$. ¿Cual seria la respuesta correcta, y en donde me estoy equivocando? Gracias.	complex-numbers,exponentiation
A.28	If $\sin(18^\circ)=\frac{a + \sqrt{b}}{c}$, then what is $a+b+c$?	If $\sin(18)=\frac{a + \sqrt{b}}{c}$ in the simplest form, then what is $a+b+c$? $$ $$ Attempt: $\sin(18)$ in a right triangle with sides $x$ (in front of corner with angle $18$ degrees), $y$, and hypotenuse $z$, is actually just $\frac{x}{z}$, then $x = a + \sqrt{b}, z = c$. We can find $y$ as $$ y = \sqrt{c^{2}- (a + \sqrt{b})^{2}} $$ so we have  $$ \cos(18) = \frac{y}{z} = \frac{\sqrt{c^{2}- (a + \sqrt{b})^{2}}}{c}$$ I also found out that $$b = (c \sin(18) - a)^{2} = c^{2} \sin^{2}(18) - 2ac \sin(18) + a^{2}$$ I got no clue after this.  The solution says that $$ \sin(18) = \frac{-1 + \sqrt{5}}{4} $$ I gotta intuition that we must find $A,B,C$ such that $$ A \sin(18)^{2} + B \sin(18) + C = 0 $$  then $\sin(18)$ is a root iof $Ax^{2} + Bx + C$, and $a = -B, b = B^{2} - 4AC, c = 2A$.  Totally different. This question is not asking to prove that $sin(18)=(-1+\sqrt{5})/4$, that is just part of the solution.	algebra-precalculus,trigonometry,euclidean-geometry,contest-math
A.28	Si $\sin(18^\circ)=\frac{a + \sqrt{b}}{c}$, entonces ¿qué es $a+b+c$?	Si $\sin(18)=\frac{a + \sqrt{b}}{c}$ en la forma más simple, entonces ¿qué es $a+b+c$? $$ $$ Intento: $\sin(18)$ en un triángulo rectángulo con lados $x$ (en frente de la esquina con ángulo $18$ grados), $y$, y la hipotenusa $z$, es realmente sólo $\frac{x}{z}$, entonces $\sin(18)=\frac{a + \sqrt{b}}{c}$0. Podemos encontrar $y$ como $\sin(18)=\frac{a + \sqrt{b}}{c}$1 así que tenemos $\cos(18)=\frac{a + \sqrt{b}}{c}$2 También descubrí que $$b = (c \sin(18) - a)^{2} = c^{2} \sin^{2}(18) - 2ac \sin(18) + a^{2}$$ I no tengo idea después de esto. La solución dice que $\sin(18)=\frac{a + \sqrt{5}}{c}$4 Tengo la intuición de que debemos encontrar $A,B,C$ tal que $\sin(18)=\frac{a + $$ A \sin(18)^{2} + B \sin(18) + C = 0 $$ entonces $\sin(18)$ es una raíz de Ax^{2} + Bx + C$, y $a = -B, b = B^{2} - 4AC, c = 2A$. totalmente diferente. Esta pregunta no está pidiendo probar que $sin(18)=(-1+\sqrt{5})/4$, que es sólo parte de la solución.	algebra-precalculus,trigonometry,euclidean-geometry,contest-math
A.29	Dividing Complex Numbers by Infinity	My PreCalculus teacher recently reviewed the properties of limits with us before our test and stated that any real number divided by infinity equals zero. This got me thinking and I asked them whether a complex number (i.e. $3+2i$ or $-4i$) divided by infinity would equal zero.  This completely stumped them and I was unable to get an answer. After doing some theoretical calculation, knowing that $i=\sqrt{-1}$, I calculated that a complex number such as $\frac{5i}{\infty}=0$ since  $$\frac{5}{\infty}\cdot \frac{\sqrt{-1}}{\infty} = 0\cdot 0 = 0,$$  using properties utilized with real numbers that would state that $\frac{5x}{\infty} = 0$ since $$\frac{5}{\infty}\cdot \frac{x}{\infty} = 0\cdot 0 = 0.$$ Is this theoretical calculation correct or is there more to the concept than this?	algebra-precalculus,limits,complex-numbers,infinity
A.29	Dividiendo los números complejos por infinito	Mi profesor de PreCalculus recientemente revisó las propiedades de los límites con nosotros antes de nuestra prueba y declaró que cualquier número real dividido por infinito es igual a cero. Esto me hizo pensar y les pregunté si un número complejo (es decir, $3+2i$ o $-4i$) dividido por infinito sería igual a cero. Esto los sorprendió completamente y no pude obtener una respuesta. Después de hacer algún cálculo teórico, sabiendo que $i=\sqrt{-1}$, calculé que un número complejo como $\frac{5i}{\infty}=0$ desde $$\frac{5}{\infty}\cdot \frac{\sqrt{-1}}{\infty} = 0\cdot 0 = 0,$$ usando propiedades utilizadas con números reales que indicarían que $\frac{5x}{\infty} = 0$ desde $$\frac{5}{\infty}\cdot \frac{x}{\infty} = 0\cdot 0 = 0.$$ ¿Es este cálculo teórico correcto o hay más en el concepto que esto?	algebra-precalculus,limits,complex-numbers,infinity
A.30	Find $a^3+b^3+c^3-3abc$ (binomial theorem)	$$a=\sum_{n=0}^\infty\frac{x^{3n}}{(3n)!}\\b=\sum_{n=1}^\infty\frac{x^{3n-2}}{(3n-2)!}\\c=\sum_{n=1}^\infty\frac{x^{3n-1}}{(3n-1)!}$$Find $a^3+b^3+c^3-3abc$: $(a)\ 1$ $(b)\ 0$ $(c)-1$ $(d)-2$  Please help me solve this question. I added $a,b$ and $c$. It gives me the expansion of $e^x$. But i dont know how to use it.	binomial-theorem
A.30	Encuentra $a^3+b^3+c^3-3abc$ (teorema binomial)	$$a=\sum_{n=0}^\infty\frac{x^{3n}}{(3n)!}\\b=\sum_{n=1}^\infty\frac{x^{3n-2}}{(3n-2)!}\\c=\sum_{n=1}^\infty\frac{x^{3n-1}}{(3n-1)!}$$Encuentra $a^3+b^3+c^3-3abc$: $(a)\ 1$ $(b)\ 0$ $(c)-1$ $(d)-2$ Por favor ayúdame a resolver esta pregunta. Añadí $a,b$ y $c$. Me da la expansión de $e^x$. Pero no sé cómo usarlo.	binomial-theorem
A.32	Are definitions axioms?	I just want to ask a very elementary question. When we introduce a "definition" in a first order logical system. For example when we say  Define: $Empty(x) \iff \not \exists y (y \in x) $  Isn't that definition itself an "axiom", call it a definitional axiom. I'm asking this because the one place predicate symbol Empty() is actually new, it is not among the listed primitives of say Zermelo, which has only identity and membership as primitive symbols.  So when we are stating definitions are we in effect stating axioms? but instead of being about characterizing a primitive, they are definitional axioms giving a complete reference to a specified set of symbols in the system. Is that correct? Now if that is the case, then why we don't call it axiom when we state it, I mean why we don't say for example: Definitional axiom 1) $Empty(x) \iff \not \exists y (y \in x)$ Zuhair	terminology,definition,first-order-logic,axioms
A.32	¿Son las definiciones axiomas?	Quiero hacer una pregunta muy elemental. Cuando introducimos una "definición" en un sistema lógico de primer orden. Por ejemplo, cuando decimos Define: $Empty(x) \iff \not \exists y (y \in x) $ ¿No es esa definición en sí misma un "axioma", llámenlo un axioma de definición. Lo estoy preguntando porque el símbolo predicado de un lugar Empty() es en realidad nuevo, no está entre los primitivos enumerados de decir Zermelo, que sólo tiene identidad y pertenencia como símbolos primitivos. Así que cuando estamos diciendo definiciones estamos en efecto diciendo axiomas? pero en lugar de ser acerca de caracterizar un axioma primitivo, son axiomas de definición dando una referencia completa a un conjunto específico de símbolos en el sistema. ¿Es eso correcto? Ahora si ese es el caso, entonces porque no lo llamamos axioma cuando lo declaramos, entonces digo por qué no queremos decir por ejemplo: 1) Definición axioma $Empty(x) \iff \not \exists y (y \in x)$ Zuhair	terminology,definition,first-order-logic,axioms
A.33	Physical meaning and significance of third derivative of a function	Given a physical quantity represented by a function $f(t,x)$ what is (if there is any) the actual meaning of the third derivative of $f$, $\frac{\partial^3 f}{\partial t^3}$ or $\frac{\partial^3 f}{\partial x^3}$	physics
A.33	Significado físico y significancia de la tercera derivada de una función	Dada una cantidad física representada por una función $f(t,x)$, cuál es (si existe) el significado real de la tercera derivada de $f$, $\frac{\partial^3 f}{\partial t^3}$ o $\frac{\partial^3 f}{\partial x^3}$	physics
A.34	Extending Knuth up-arrow/hyperoperations to non-positive values	So... I had the silly idea to extend Knuth's up-arrow notation so that it included zero and negative arrows. It is normally defined as $$\begin{align*} a \uparrow b & = a^b \\ a \uparrow^n b & = \underbrace{a \uparrow^{n - 1} (a \uparrow^{n - 1} (\dots(a \uparrow^{n - 1} a) \dots ))}_{b\text{ copies of } a} \end{align*}$$ so, basically the hyperoperation sequence starting from exponentiation. For now, I will only consider $a,b > 0$. If we try to go backwards from $a \uparrow b$, the "trivial" extension (letting down arrows represent negative up arrows, because why the heck not) is: $$\begin{align*} a \;b & = a \cdot b \\ a \downarrow b & = a + b \\ a \downarrow \downarrow b & = \text{see below} \end{align*} \\ \vdots$$ But I had trouble coming up with an expression for $a \downarrow \downarrow \downarrow b$. Maybe it doesn't exist. Alternatively, maybe there is a way of defining $a \; b$ (zero arrows) such that it does exist. So my question is: Is there an extension of Knuth's up-arrow notation such that $a \downarrow^n b$ exists for all $n \geq 3$?  Edit: Welp, I messed this question up. I initially thought $a \downarrow \downarrow b = a + 1$ was correct, but it is actually $b + 1$. So I thought I had an example of an extension when I did not. I have modified the question accordingly.  An extension would define $a \uparrow^n b$ for each $n \leq 0$ which satisfies the recursive definition of the notation.  Edit 2: Okay, turns out $a \downarrow \downarrow b = b + 1$ isn't correct either, as this would imply $a \downarrow b = a + b - 1$. For example, $4 \downarrow 3 = 4 \downarrow \downarrow (4 \downarrow \downarrow 4) = 4 \downarrow \downarrow (4 + 1) = (4 + 1) + 1 = 6 = 4 + 3 - 1$. But it is really close; perhaps we need an exception, such as $$\begin{align*} a \downarrow \downarrow b = \begin{cases}b + 1 & \text{if } a < b \\ b + 2 & \text{if } a = b \end{cases}\end{align*}.$$ The case $a > b$ does not show up when evaluating $a \downarrow b$, but it will be need to be defined if we try to extend further to $a \downarrow \downarrow \downarrow b$. For instance, we could abuse the fact that the case $a > b$ is allowed to be anything, and let $$a \downarrow \downarrow b = b + 1 + \left\lfloor \frac{a}{b} \right\rfloor,$$ but finding $a \downarrow \downarrow \downarrow b$ may be intractable as a result.	hyperoperation,ackermann-function
A.34	Extensión de las notaciónes flechas/hiperoperaciones de Knuth a valores no positivos	Así que... tuve la tonta idea de extender la notación de flecha ascendente de Knuth para que incluyera flechas cero y negativas. Normalmente se define como $$\begin{align*} a \uparrow b & = a^b \\ a \uparrow^n b & = \underbrace{a \uparrow^{n - 1} (a \uparrow^{n - 1} (\dots(a \uparrow^{n - 1} a) \dots ))}_{b\text{ copies of } a} \end{align*}$$. Así que básicamente la secuencia de hiperoperación que comienza de la exponenciación. Por ahora, solo consideraré $a,b > 0$. Si intentamos ir hacia atrás desde $a \uparrow b$, la extensión "trivial" (dejar las flechas hacia abajo representa flechas ascendentes negativas, porque por qué no lo hace) es: $$\begin{align*} a \;b & = a \cdot b \\ a \downarrow b & = a + b \\ a \downarrow \downarrow b & = \text{see below} \end{align*} \\ \vdots$$ Pero tuve problemas para encontrar una expresión para $a \downarrow \downarrow \downarrow b$. Tal vez no exista. Alternativamente, tal vez haya una manera de definir $a \; b$ (flechas cero) de tal manera que exista. Así que mi pregunta es: ¿Existe una extensión de la notación de flecha ascendente de Knuth tal que exista para todos los $n \geq 3$14 Edit: Bien, pues, podría confundir esta pregunta. Al principio creia que $a \downarrow \downarrow b = a + 1$ era correcto, pero en realidad es$b + 1$. Asi que pense que tenia un ejemplo de una extension pero en realidad no la tenia. Habia modificado la pregunta acordemente.  Una extension definiria $a \uparrow^n b$ para cada $n \leq 0$ que satisface la definicion recursiva de la notacion.  Edit 2: Bueno, resulta que $a \downarrow \downarrow b = b + 1$ no es siquiera correcto, pues implicaria que $a \downarrow b = a + b - 1$. por ejemplo, $4 \downarrow 3 = 4 \downarrow \downarrow (4 \downarrow \downarrow 4) = 4 \downarrow \downarrow (4 + 1) = (4 + 1) + 1 = 6 = 4 + 3 - 1$. Pero enrealidad esta cerca; quizas necesitamos una excepcion, tal que $$\begin{align*} a \downarrow \downarrow b = \begin{cases}b + 1 & \text{if } a < b \\ b + 2 & \text{if } a = b \end{cases}\end{align*}.$$ El caso $a > b$ no se muestre cuando se evalue $a \downarrow b$, pero tendra que ser definido si intentamos entender mas all de $a \downarrow \downarrow \downarrow b$. Por lo tanto, podriamos abusar el hecho de que el caso $a > b$ esta permitido de ser cualquier cosa, y dejar que $$a \downarrow \downarrow b = b + 1 + \left\lfloor \frac{a}{b} \right\rfloor,$$ pero encontrando $a \downarrow \downarrow \downarrow b$ puede resultar intratable como resultado.	hyperoperation,ackermann-function
A.35	When does a function NOT have an antiderivative?	I know this question may sound naïve but why can't we write $\int e^{x^2} dx$ as $\int e^{2x} dx$? The former does not have an antiderivative, while the latter has. In light of this question, what are sufficient conditions for a function NOT to have an antiderivative. That is, do we need careful examination of a function to say it does not have an antiderivative or is there any way that once you see the function, you can right away say it does not have an antiderivative?	integration
A.35	¿Cuándo una función NO tiene un antiderivado?	Sé que esta pregunta puede sonar ingenuo pero ¿por qué no podemos escribir $\int e^{x^2} dx$ como $\int e^{2x} dx$? La primera no tiene un antiderivado, mientras que la segunda lo tiene. A la luz de esta pregunta, ¿cuáles son las condiciones suficientes para que una función NO tenga un antiderivado? Es decir, necesitamos una examinacion cuidadoso de una función para decir que no tiene un antiderivado o hay alguna manera de que una vez que ves la función, puedes decir inmediatamente que no tiene un antiderivado?	integration
A.36	Proof by contradiction, status of initial assumption after the proof is complete.	First of all I'd like to say that I have looked for the answers to my specific question and have not found it in the existing topics. The question is fairly simple. Say, we need to  prove statement P by the method of contradiction. Assuming that $\lnot P$ holds, using the list of statements proven earlier to hold or derived by us during the proof, we arrive to P being $true$. $$\lnot P  \to A_1 \to\ ... \ \to A_n \to P$$ $$\lnot P  \to  P \iff \lnot(\lnot P) \lor P \iff P $$ We can therefore add P to the list of our proven statements, because it was derived. Most of the proofs contain something in the lines of "the obtained contradiction proves that our initial assumption ($\lnot P$) was wrong and so $P$ holds". What I don't understand is, if the initial assumption ($\lnot P$) is thus proven to be false, then why can we be sure that anything derived from it holds (in particular, that P holds)? On the other hand, if it cannot be derived then the assumption ($\lnot P$) can in fact be true.  Can someone explain why this type of argument cannot be used?	logic,proof-writing
A.36	Prueba por contradicción, estado de suposición inicial después de que la prueba esté completa.	En primer lugar me gustaría decir que he buscado las respuestas a mi pregunta específica y no la he encontrado en los temas existentes. La pregunta es bastante simple. Digamos que necesitamos probar la afirmación P por el método de contradicción. Suponiendo que $\lnot P$ es cierto, utilizando la lista de afirmaciones probadas anteriormente para sostener o derivadas por nosotros durante la prueba, llegamos a P siendo $true$. $$\lnot P  \to A_1 \to\ ... \ \to A_n \to P$$ $$\lnot P  \to  P \iff \lnot(\lnot P) \lor P \iff P $$ Podemos agregar P a la lista de nuestras afirmaciones probadas, porque fue derivada. La mayoría de las pruebas contienen algo en las líneas de "la contradicción obtenida prueba que nuestra suposición inicial ($\lnot P$) era incorrecta y por lo tanto $P$ es válida". Lo que no entiendo es que, si la suposición inicial ($\lnot P$) es así demostrada ser falsa, entonces por qué podemos estar seguros de que cualquier cosa derivada de ella es válida (en particular, que P se mantiene) ¿En la otra mano, entonces no se puede explicar por qué alguien puede utilizar esta suposición si se deriva de la verdad ($\lnot P$) de hecho? ¿Podria alguien explicar porque este tipo de argumento no puede ser usado?	logic,proof-writing
A.37	Non trivial examples of $f\circ g = g \circ f$ but $f^{-1} \neq g$ and $f\neq\mathrm{id}\neq g$.	Are there real-valued functions $f$ and $g$ which are neither each other's inverses, the identity, nor linear, yet exhibit the behaviour $$f\circ g = g \circ f?$$ Examples such as $f(x) = 2x$ and $g(x)=3x$ are "trivial" in this sense.  Moreover, given a function $f$, can one go about obtaining an example of a function $g$ which commutes with $f$?  I suppose this would be similar to fixed point iteration? E.g. if $f\colon\mathbb R\smallsetminus\{1\}\to\mathbb R$ is defined by $f(x) = 2x/(1-x)$, I would need a function $g$ such that  $$g(x) = f^{-1}\circ g\circ f =\frac{g(\frac{2x}{1-x})}{2+g(\frac{2x}{1-x})},$$ so maybe choosing an appropriate "starting function" $g_0$ and finding a fixed point of $g_{n+1} \mapsto f^{-1}\circ g_n\circ f$ would be a possible strategy, but I can't seem to find a suitable $g_0$.	real-analysis,functional-analysis,functions
A.37	Ejemplos no triviales de $f\circ g = g \circ f$ pero de $f^{-1} \neq g$ y $f\neq\mathrm{id}\neq g$.	¿Existen funciones de valor real $f$ y $g$ que no son inversas, la identidad, ni lineales entre sí, pero que muestran el comportamiento $$f\circ g = g \circ f?$$ Ejemplos como $f(x) = 2x$ y $g(x)=3x$ son "triviales" en este sentido? Además, dada una función $f$, ¿puede uno ir a obtener un ejemplo de una función $g$ que se desplaza con $f$? Supongo que esto sería similar a la iteración de punto fijo? Por ejemplo, si $f\colon\mathbb R\smallsetminus\{1\}\to\mathbb R$ es definido por $f(x) = 2x/(1-x)$, necesitaría una función $g$ tal que $$g(x) = f^{-1}\circ g\circ f =\frac{g(\frac{2x}{1-x})}{2+g(\frac{2x}{1-x})},$$ así que quizás elegir una "función de inicio" apropiada $g_0$ y encontrar un punto fijo de $g_{n+1} \mapsto f^{-1}\circ g_n\circ f$ sería una estrategia posible, pero no puedo encontrar una $g_0$ adecuada.	real-analysis,functional-analysis,functions
A.38	Uses of Axiom of Choice	I am a first-year maths student but I occasionally drift away from our taught material. Some years ago I saw the ZFC axioms for the first time, but now that I am in college, and although the stuff I've been taught so far is nowhere near ZFC (in terms of difficulty), it happened to me that we use the axiom of choice all the time in every module, even if we don't know it by name yet. For example, in the proof that, for every non-negative integers $a, b$, there exist integers $q, r: a = bq + r$ (with the known restrictions on r), and the proof starts like this: $Choose$ the largest integer $q : qb <= a$... blah blah blah. Is it the axiom of choice that allows us to execute this simple yet so important step?  And a couple more questions: Can you name some other simple proofs, theorems, results etc for which the axiom of choice is essential? Also, I've read that AOC has long been a topic of dispute for mathematicians, and that even today, some people do not accept it. Are there any alternative axiomatic systems that work equally well without needing AOC? Thanks!	set-theory
A.38	Utilizaciones del axioma de la elección	Soy estudiante de matemáticas de primer año pero de vez en cuando me alejo de nuestro material enseñado. Hace algunos años vi los axiomas de ZFC por primera vez, pero ahora que estoy en la universidad, y aunque las cosas que me han enseñado hasta ahora no están cerca de ZFC (en términos de dificultad), me pasó que usamos el axioma de elección todo el tiempo en cada módulo, incluso si aún no lo conocemos por nombre. Por ejemplo, en la prueba de que, para cada número entero no negativo $a, b$, existen números enteros $q, r: a = bq + r$ (con las restricciones en r), y la prueba comienza así: $Choose$ el número entero más grande $q : qb <= a$ ... bla bla bla bla. ¿Es el axioma de elección que nos permite ejecutar este simple pero aun asi un paso tan importante? Y un par de preguntas más: ¿Llamáis a algunos otros simples, o los resultados, por el cual el trabajo de los sistemas de axiomas no es esencial, Además, he leído que el AOC ha sido durante mucho tiempo un tema de disputa para los matemáticos y que, incluso hoy en día, algunas personas no lo aceptan. ¿Existen sistemas axiomáticos alternativos que funcionen igualmente bien sin necesidad de AOC? Gracias	set-theory
A.39	How to know which value is bigger?	Which is bigger between $2018^{2019}$ or $\ 2019^{2018}\ $? When taking logs of both sides and  I get:  $2019\log(2018)\ $ and $\ 2018 \log(2019)$ I know $\log 2019\gt \log 2018$ so does this mean that $2019^{2018}$ is the biggest one? And did I do it properly?	algebra-precalculus,logarithms
A.39	¿Cómo saber cuál es el valor más grande?	¿Cuál es mayor entre $2018^{2019}$ o $\ 2019^{2018}\ $? Cuando tomo registros de ambos lados y obtengo: $2019\log(2018)\ $ y $\ 2018 \log(2019)$ sé $\log 2019\gt \log 2018$ así que esto significa que $2019^{2018}$ es el más grande? ¿Y lo hice correctamente?	algebra-precalculus,logarithms
A.40	What is the meaning of the term "linear"	$a_1x_1+a_2x_2+a_3x_3+...+a_nx_n=$ is called a linear equation because it represents the equation of a line in an n dimensional space. So "linear" comes from the word "line".Basically there should not be any higher power of x failing which the graph of the function will not be a straight line. simillarly $a(x)y+b(x)y'+c(x)y"+d(x)y'''+...+q(x)=0$ is also called linear differential equation because all the derivatives have power equal to 1 which is similar to the above definition of a linear equation. A function f is called linear if: $f(x+y)=f(x)+f(y)$ and $f(cx)=cf(x)$. Here c is a constant. In this definition of linearity of function "$f$" what does the word linear means? How does it relate to a straight line? Finally what does the term linear means in case of linear vector spaces? Where is the reference to a straight line? So, whether linear is just a word used in different contexts? Does it have different meaning in different situation? Or linearity refers to some relation to a straight line? At Least please explain how the linearity of function f and linear vector space relate to the equation of a line.	linear-algebra
A.40	¿Qué significa el término "lineal"	$a_1x_1+a_2x_2+a_3x_3+...+a_nx_n=$ se llama ecuación lineal porque representa la ecuación de una línea en un espacio n dimensiones. Así que "lineal" proviene de la palabra "linea".Básicamente no debe haber ninguna potencia superior de x fallido en el que el gráfico de la función no será una línea recta. de manera similar $a(x)y+b(x)y'+c(x)y"+d(x)y'''+...+q(x)=0$ también se llama ecuación diferencial lineal porque todas las derivadas tienen una potencia igual a 1 que es similar a la definición anterior de una ecuación lineal. Una función f se llama lineal si: $f(x+y)=f(x)+f(y)$ y $f(cx)=cf(x)$. Aquí c es una constante. En esta definición de linealidad de función "$f$" ¿qué significa la palabra lineal? ¿Cómo se relaciona con una línea recta? ¿Qué significa el término lineal en caso de espacios de líneas de vectores lineales? ¿Dónde está la referencia a una línea recta? ¿O en algún otro contexto, cuál es el significado de la relación de la línea recta? ¿Tendra un significado diferente en una situacion diferente? ¿O linealidad se refiere a alguna relacion de una linea recta? ¿Por favor, por favor, explicar si se utiliza una palabra diferente en contextos diferentes de una función lineal? 	linear-algebra
A.41	Confusion in how to find number of onto functions if two sets are given	In the book it is given if A and B are two finite sets containing $m$ and $n$ elements, respectively, then the number of onto functions from A to B will be if  $n \leq m$  $$\sum_{r=1}^n (-1)^{(n-r)} {n \choose r}(r)^m $$ well I can't understand it and I am aware with combinations.	combinatorics,functions,combinations
A.41	Confusión en cómo encontrar el número de funciones sobreyectivas si se dan dos conjuntos	En el libro se da si A y B son dos conjuntos finitos que contienen los elementos $m$ y $n$, respectivamente, entonces el número de en funciones de A a B será si $n \leq m$ $$\sum_{r=1}^n (-1)^{(n-r)} {n \choose r}(r)^m $$ bueno no puedo entenderlo y soy consciente de las combinaciones.	combinatorics,functions,combinations
A.42	What is a simple, physical situation where complex numbers emerge naturally?	I'm trying to teach middle schoolers about the emergence of complex numbers and I want to motivate this organically. By this, I mean some sort of real world problem that people were trying to solve that led them to realize that we needed to extend the real numbers to the complex.  For instance, the Greeks were forced to recognize irrational numbers not for pure mathematical reasons, but because the length of the diagonal of a square with unit length really is irrational, and this is the kind of geometrical situation they were already dealing with. What similar situation would lead to complex numbers in terms that kids could appreciate? I could just say, try to solve the equation $x^2 + 1 = 0$, but that's not something from the physical world. I could also give an abstract sort of answer, like that $\sqrt{-1}$ is just an object that we define to have certain properties that turn out to be consistent and important, but I think that won't be entirely satisfying to kids either.	complex-numbers,physics,applications
A.42	¿Qué es una situación física simple en la que los números complejos surgen naturalmente?	Estoy tratando de enseñar a los estudiantes de secundaria sobre la aparición de números complejos y quiero motivar esto orgánicamente. Por esto, quiero decir una especie de problema del mundo real que la gente estaba tratando de resolver que les llevó a darse cuenta de que necesitábamos extender los números reales al complejo. Por ejemplo, los griegos se vieron obligados a reconocer los números irracionales no por razones matemáticas puras, sino porque la longitud de la diagonal de un cuadrado con longitud unitaria es realmente irracional, y este es el tipo de situación geométrica con la que ya estaban tratando. ¿Qué situación similar llevaría a números complejos en términos que los niños podrían apreciar? Podría decir, tratar de resolver la ecuación $x^2 + 1 = 0$, pero eso no es algo del mundo físico. También podría dar un resumen de la respuesta, como que $\sqrt{-1}$ es un tipo de objeto que definimos para tener ciertas propiedades que se vuelven consistentes y que no es importante para satisfacer a los niños.	complex-numbers,physics,applications
A.43	Prove $\sum_{n\geq1}\frac1{n^2+1}=\frac{\pi\coth\pi-1}2$	I am trying to prove  $$\sum_{n\geq1}\frac1{n^2+1}=\frac{\pi\coth\pi-1}2$$ Letting $$S=\sum_{n\geq1}\frac1{n^2+1}$$ we recall the Fourier series for the exponential function $$e^x=\frac{\sinh\pi}\pi+\frac{2\sinh\pi}\pi\sum_{n\geq1}\frac{(-1)^n}{n^2+1}(\cos nx-n\sin nx)$$ Plugging in $x=\pi$ $$e^\pi=\frac{\sinh\pi}\pi+\frac{2\sinh\pi}\pi\sum_{n\geq1}\frac{(-1)^n}{n^2+1}(\cos n\pi-n\sin n\pi)$$ $$e^\pi=\frac{\sinh\pi}\pi+\frac{2\sinh\pi}\pi\sum_{n\geq1}\frac{(-1)^n}{n^2+1}((-1)^n-n\cdot0)$$ $$e^\pi=\frac{\sinh\pi}\pi+\frac{2\sinh\pi}\pi S$$ $$S=\frac{\pi e^\pi}{2\sinh\pi}-\frac12$$ But that is nowhere near to correct. What did I do wrong, and how do can I prove the identity? Thanks.	real-analysis,sequences-and-series
A.43	Prueba $\sum_{n\geq1}\frac1{n^2+1}=\frac{\pi\coth\pi-1}2$	Estoy tratando de probar $$\sum_{n\geq1}\frac1{n^2+1}=\frac{\pi\coth\pi-1}2$$ dejando $$S=\sum_{n\geq1}\frac1{n^2+1}$$ recordamos la serie de Fourier para la función exponencial $$e^x=\frac{\sinh\pi}\pi+\frac{2\sinh\pi}\pi\sum_{n\geq1}\frac{(-1)^n}{n^2+1}(\cos nx-n\sin nx)$$ Introducir $x=\pi$ $$e^\pi=\frac{\sinh\pi}\pi+\frac{2\sinh\pi}\pi\sum_{n\geq1}\frac{(-1)^n}{n^2+1}(\cos n\pi-n\sin n\pi)$$ $$e^\pi=\frac{\sinh\pi}\pi+\frac{2\sinh\pi}\pi\sum_{n\geq1}\frac{(-1)^n}{n^2+1}((-1)^n-n\cdot0)$$ $$e^\pi=\frac{\sinh\pi}\pi+\frac{2\sinh\pi}\pi S$$ $$S=\frac{\pi e^\pi}{2\sinh\pi}-\frac12$$ Pero eso no está cerca de estar correcto. ¿Qué hice mal, y cómo puedo probar la identidad? Gracias.	real-analysis,sequences-and-series
A.44	For $A,B \in \mathscr{M}_{2\times2}(\mathbb{Q}) $ of finite order, show that $AB$ has infinite order	Let $G$ be the group $ ( \mathscr{M}_{2\times2}(\mathbb{Q}) , \times ) $ of nonsingular matrices. Let $ A = \left ( \begin{matrix}  0 & -1 \\   1 & 0  \end{matrix} \right ) $, the order of $A$ is $4$; Let $ B = \left ( \begin{matrix}  0 & 1 \\   -1 & -1  \end{matrix} \right ) $, the order of $B$ is $3$. Show that $AB$ has infinite order.  The only reasoning possible here is by contradiction as $G$ is not abelian. And so I tried, but I got stuck before any concrete development. Any hints are welcome, Thanks.	matrices,group-theory,cyclic-groups
A.44	Para $A,B \in \mathscr{M}_{2\times2}(\mathbb{Q}) $ de orden finito, muestre que $AB$ tiene orden infinito	Que $G$ sea el grupo $ ( \mathscr{M}_{2\times2}(\mathbb{Q}) , \times ) $ de matrices no singulares. Que $ A = \left ( \begin{matrix}  0 & -1 \\   1 & 0  \end{matrix} \right ) $, el orden de $A$ es $4$; Que $ B = \left ( \begin{matrix}  0 & 1 \\   -1 & -1  \end{matrix} \right ) $, el orden de $B$ es $3$. Muestre que $AB$ tiene un orden infinito. El único razonamiento posible aquí es por contradicción ya que $G$ no es abeliano. Y así lo intenté, pero me quedé atascado antes de cualquier desarrollo concreto. Cualquier sugerencia es bienvenida, gracias.	matrices,group-theory,cyclic-groups
A.45	How to prove that {$\sin(x) , \sin(2x) , \sin(3x) ,...,\sin(nx)$} is independent in $\mathbb{R}$?	Prove that {$\sin(x) , \sin(2x) , \sin(3x) ,...,\sin(nx)$} is independent in $\mathbb{R}$  my trial : we know that the Wronsekian shouldn't be $0$ to get the trivial solution and thus they are independent. its not trivial to show that $ W \not = 0$ W =  $\begin{vmatrix} (1)\sin(x) & (1)\sin(2x) & (1)\sin(3x) &  ... &   (1)\sin(nx) \\ (1)\cos(x) & (2)\cos(2x) & (3)\cos(3x) &  ... &   (n)\cos(nx) \\  -(1)^2\sin(x) & -(2)^2\sin(2x) & -(3)^2\sin(3x) &  ... &   -(n)^2\sin(nx) \\ -(1)^3\cos(x) & -(2)^3\cos(2x) & -(3)^3\cos(3x) &  ... &   -(n)^3\cos(nx) \\ \end{vmatrix}$ and so on. it looks like Vandermonde matrix but i cant prove that and so we conclude that its $W\not =0$	ordinary-differential-equations
A.45	¿Cómo demostrar que {$\sin(x) , \sin(2x) , \sin(3x) ,...,\sin(nx)$} es independiente en $\mathbb{R}$?	Pruebe que {$\sin(x) , \sin(2x) , \sin(3x) ,...,\sin(nx)$} es independiente en $\mathbb{R}$ mi ensayo: sabemos que el Wronsekian no debería ser $0$ para obtener la solución trivial y por lo tanto son independientes. no es trivial para mostrar que $ W \not = 0$ W = $\begin{vmatrix} (1)\sin(x) & (1)\sin(2x) & (1)\sin(3x) &  ... &   (1)\sin(nx) \\ (1)\cos(x) & (2)\cos(2x) & (3)\cos(3x) &  ... &   (n)\cos(nx) \\  -(1)^2\sin(x) & -(2)^2\sin(2x) & -(3)^2\sin(3x) &  ... &   -(n)^2\sin(nx) \\ -(1)^3\cos(x) & -(2)^3\cos(2x) & -(3)^3\cos(3x) &  ... &   -(n)^3\cos(nx) \\ \end{vmatrix}$ y así sucesivamente. se parece a la matriz de Vandermonde pero no puedo probar eso y así concluimos que su $W\not =0$	ordinary-differential-equations
A.46	Suppose $f$ is a Lebesgue integrable function on$[0,1]$ which satisfies $\int x^k f(x) dx=0$ , prove $f=0 \text{ a.e.}$	Suppose $f$ is an indefinitely differentiable real valued function on$[0,1]$ which satisfies $\int_0^1 x^k f(x)\, dx=0$ for $k=\{0,1,2,3,.. .\}$, prove $f=0$ . My attempt : To prove this assertion , it suffice to prove $$\int_0^1 f^2 (x) \,dx=0$$  Then by approximation theorem , we can find a polynomial such that for every $\epsilon \gt 0$ $$\int_0^1 f^2 (x) \,dx= \int_0^1 (f(x)-\sum_{n=0}^N a_n x^n)f(x) ,dx+\int_0^1 \sum_{n=0}^N f(x) a_n x^n\,dx \le \epsilon M$$ where $M=\sup_{x \in [0,1]}|f(x)|$ . It seems that we do not need the condition that $f$ is indefinitely differentiable , if $f$ is continuous then the conclution may hold . My question : a) Suppose $f$ is a Lebesgue integrable real valued function on$[0,1]$ which satisfies $\int_0^1 x^k f(x)\ dx=0$ for $k=\{0,1,2,3,.. .\}$, can we prove $f=0$ except on a set of measure $0$ ?      b) If a) is not true , suppose $f$ is a Riemann integrable real valued function on$[0,1]$ which satisfies $\int_0^1 x^k f(x)\ dx=0$ for $k=\{0,1,2,3,.. .\}$, can we prove $f=0$ except on a set of measure $0$ ?   EDIT: To prove $f=0 \text{  a.e.}$ , it suffice to prove all the fourier coefficient of $f$ equal to $0$ , then  $$\int_0^1 f(x) \cos(2 \pi nx) \,dx \le \int_0^1 |f(x)||\cos(2 \pi nx)-\sum_{n=0}^{N}a_n x^n| \le \epsilon||f||_{L^1}$$ and the proof is complete.	real-analysis
A.46	Supongamos que $f$ es una función integrable de Lebesgue en $[0,1]$ que satisface $\int x^k f(x) dx=0$ , prueba $f=0 \text{ a.e.}$	Supongamos que $f$ es una función real valorada indefinidamente diferenciable en $[0,1]$ que satisface a $\int_0^1 x^k f(x)\, dx=0$ para $k=\{0,1,2,3,.. .\}$, prueba $f=0$ . Mi intento: Para probar esta afirmación , basta con probar $$\int_0^1 f^2 (x) \,dx=0$$ Entonces , mediante el teorema de aproximación , podemos encontrar un polinomio tal que para cada $\epsilon \gt 0$ $$\int_0^1 f^2 (x) \,dx= \int_0^1 (f(x)-\sum_{n=0}^N a_n x^n)f(x) ,dx+\int_0^1 \sum_{n=0}^N f(x) a_n x^n\,dx \le \epsilon M$$ donde $M=\sup_{x \in [0,1]}|f(x)|$ . Parece que no necesitamos la condición de que $f$ sea indefinidamente diferenciable , si $f$ es continua entonces la conclusión puede ser válida . Mi pregunta: a) Supongamos que $f$ es una función real valorada integrable de Lebesgue en $[0,1]$ que satisface a $f$0 para $k=\{0,1,2,3,.. .\}$, ¿ podemos probar $f=0$ excepto en un conjunto de medidas de $0$ ? b) Si a) no es cierto , $f$ es una función integrable de Riemann valorada en XX$[0,1]$ que satisface a $\int_0^1 x^k f(x)\ dx=0$ for $k=\{0,1,2,3,.. .\}$, ¿ podemos probar que $f=0$ es igual a una prueba completa de la medida de $f$1 y que el coeficiente de $0$ es suficiente para probar que el conjunto de $f$1 es igual a todo el conjunto de $f$1 ? EDIT: Para probar $f=0 \text{  a.e.}$ , basta con probar todos los coeficientes fourier de $f$ igual a $0$ , entonces $$\int_0^1 f(x) \cos(2 \pi nx) \,dx \le \int_0^1 |f(x)||\cos(2 \pi nx)-\sum_{n=0}^{N}a_n x^n| \le \epsilon||f||_{L^1}$$ y la prueba esta completa.	real-analysis
A.47	Prove that for a given prime $p$ and each $0 < r < p-1$, there exists a $q$ such that $rq \equiv 1 \bmod p$	Prove that for a given prime $p$ and each $0 < r < p-1$, there exists a $q$ such that  $$rq \equiv 1 \bmod p$$ I've only taken one intro number theory course (years ago), and this just popped up in a computer science class (homework). I was assuming that this proof would be elementary since my current class in an algorithm cours, but after the few basic attempts I've tried it didn't look promising. Here's a couple approaches I thought of:  (reverse engineer) To arrive at the conclusion we would need $$rq - 1 = kp$$ for some $k$. A little manipulation: $$qr - kp = 1$$ That looks familiar, but I can't see anything from it.  (sum on $r$) $$\sum_{r=1}^{p-2} r = \frac{(p-2)(p-1)}{2} = p\frac{p - 3}{2} + 1 \equiv 1 \bmod p$$ which looks good but I don't know how to incorporate $r$ int0 the final equality.    (Wilson's Theorem—proved by Lagrange)   I vaguely recall this theorem, but I was looking at it in an old book and it wasn't easy to see how we arrived there. Anyways, $p$ is prime iff $$(p-1)! \equiv -1 \bmod p$$ Here the $r$ multiplier is built in to the factorial expression so I was thinking of adding $2$ to either side $$(p-1)! + 2 \equiv 1 \bmod p$$ which is a dead end (pretty sure). But then I was thinking, maybe multiplying Wilson't Thm by $(p+1)$? Then getting $$(p+1)(p-1)! = -(p+1) \bmod p$$ which I think results in $$(p+1)(p-1)! = 1 \bmod p$$ of which $r$ is a multiple and $q$ is obvious. But I'm not sure if that's valid.	elementary-number-theory,proof-verification
A.47	Demostrar que para un dado primo $p$ y cada $0 < r < p-1$, existe un $q$ tal que $rq \equiv 1 \bmod p$	Demuestre que para un primo dado $p$ y cada $0 < r < p-1$, existe un $q$ tal que $$rq \equiv 1 \bmod p$$ Solo he tomado un curso de introducción a la teoría de números ( hace años), y esto acaba de surgir en una clase de informática (tarea). Estaba asumiendo que esta prueba sería elemental ya que mi clase actual en un curso de algoritmos, pero después de los pocos intentos básicos que probé, no parecía prometedora. Aquí hay un par de enfoques en los que pensé: (ingeniería inversa) Para llegar a la conclusión necesitaríamos $$rq - 1 = kp$$ para obtener algo de $k$. Una pequeña manipulación: $$qr - kp = 1$$ Eso me resulta familiar, pero no puedo ver nada en él. (suma en $r$) $$\sum_{r=1}^{p-2} r = \frac{(p-2)(p-1)}{2} = p\frac{p - 3} {2} + 1 \equiv 1 \bmod p$$ que se ve bien pero no sé cómo incorporar $r$ int0 la igualdad final. (Teorema de Wilson, demostrado por Lagrange) Recuerdo vagamente este teorema, pero lo estaba mirando en un libro antiguo y no fue fácil ver cómo llegamos allí. De todos modos, ¡$p$ es primo si $$(p-1)! \equiv -1 \bmod p$$ ¡Aquí el multiplicador $r$ está integrado en la expresión factorial, así que estaba pensando en sumar $2$ a cada lado $$(p-1)! + 2 \equiv 1 \bmod p$$ que es un callejón sin salida (bastante seguro). Pero luego pensé, ¿tal vez multiplicar Wilson't Thm por $(p+1)$? ¡Entonces obteniendo $$(p+1)(p-1)! = -(p+1) \bmod p$$ que creo que da como resultado $$(p+1)(p-1)! = 1 \bmod p$$ del cual $r$ es un múltiplo y $q$ es obvio. Pero no estoy seguro de si eso es válido.	elementary-number-theory,proof-verification
A.48	Hints for showing that if $x,y \geq 0$, then $(x+y)^k \geq x^k + y^k$ for all $k \geq 1$	I'm trying to show the following:   If $x,y \geq 0$, then $(x+y)^k \geq x^k + y^k$ for all $k \in \mathbb{R_{\geq 1}}$   So, far I've tried a few things, but nothing seems to stick. Clearly $x \leq x+ y$ and since both sides of the inequality are positive, the inequality $x^k \leq (x + y)^k$ will hold for $k \geq 1$. Similarly, $y^k \leq (x+y)^k$. Adding both inequalities together, we obtain: $x^k + y^k \leq 2(x+y)^k$. While this is close, of course, it is not what we want to show. Alternatively, I was thinking that if we fix $x,y \geq 0$, let $f(k) = (x+y)^k$, and let $g(k) = x^k + y^k$, we can show using the binomial theorem that $(x+y)^r \geq x^r + y^r$ for all $r \in \mathbb{Z^+}$. Then, if we can show both $f$ and $g$ intersect only when $k =1 $, we might have better luck proving the statement since then we would have $(x+y)^r > x^r + y^r$ for all positive integers $r \geq 2$. A real number $m \notin \mathbb{Z}$ with the property that $f(m) = g(m)$ could not then exist since then that would violate the fact that $k=1$ is the only intersection. We could then invoke the continuity of $f$ and $g$ together with the fact that $(x+y)^r > x^r + y^r$ for all positive integers $r \geq 2$ to obtain our result. Of course, all of this is dependent on rigorously showing that $f$ and $g$ intersect only when $k=1$. Otherwise, I'm running low on ideas. Any hints would be greatly appreciated.	real-analysis,functions,inequality
A.48	Indicios para mostrar que si $x,y \geq 0$, entonces $(x+y)^k \geq x^k + y^k$ para todos los $k \geq 1$	Estoy tratando de mostrar lo siguiente: Si $x,y \geq 0$, entonces $(x+y)^k \geq x^k + y^k$ para todos $k \in \mathbb{R_{\ geq 1}}$ Hasta ahora he probado algunas cosas, pero nada parece funcionar. Claramente $x \leq x+ y$ y dado que ambos lados de la desigualdad son positivos, la desigualdad $x^k \leq (x + y)^k$ se mantendrá para $k \geq 1$. De manera similar, $y^k \leq (x+y)^k$. Sumando ambas desigualdades, obtenemos: $x^k + y^k \leq 2(x+y)^k$. Si bien esto está cerca, por supuesto, no es lo que queremos mostrar. Alternativamente, estaba pensando que si arreglamos $x,y \geq 0$, sea $f(k) = (x+y)^k$, y sea $g(k) = x^k + y^k$ , podemos demostrar usando el teorema del binomio que $(x+y)^r \geq x^r + y^r$ para todo $r \in \mathbb{Z^+}$. Entonces, si podemos mostrar que $f$ y $g$ se cruzan solo cuando $k =1 $, podríamos tener más suerte probando la afirmación ya que entonces tendríamos $(x+y)^r > x^r + y ^r$ para todos los números enteros positivos $r \geq 2$. Un número real $m \notin \mathbb{Z}$ con la propiedad de que $f(m) = g(m)$ no podría existir desde entonces, eso violaría el hecho de que $k=1$ es la única intersección. Entonces podríamos invocar la continuidad de $f$ y $g$ junto con el hecho de que $(x+y)^r > x^r + y^r$ para todos los enteros positivos $r \geq 2$ para obtener nuestro resultado. . Por supuesto, todo esto depende de demostrar rigurosamente que $f$ y $g$ se cruzan solo cuando $k=1$. De lo contrario, me estoy quedando sin ideas. Cualquier sugerencia sería muy apreciada.	real-analysis,functions,inequality
A.49	Is there a simple combinatoric interpretation of this identity?	I came across an exercise in which we are asked to prove the identity: $${2n\choose n}=\sum_{k=0}^n{n\choose k}^2$$ The exercise gives the hint: $$\left(1+x\right)^{2n}=\left[(1+x)^n\right]^2$$ It's not too difficult to use the hint to prove the identity (the expressions in the identity are the coefficients of $x^n$ in the respective expansions of the expressions in the hint, which of course must be the same number), but I was wondering whether there is a neater equivalent-counting interpretation... It's clear that ${2n \choose n}$ is the number of ways in which we can choose half the elements in a set (where this is possible): how can we interpret $\sum_{k=0}^n{n\choose k}^2$ equivalently?	combinatorics,binomial-coefficients
A.49	¿Existe una interpretación combinatoria de esta identidad?	Me encontré con un ejercicio en el que se nos pide que probemos la identidad: $${2n\choose n}=\sum_{k=0}^n{n\choose k}^2$$ El ejercicio da la pista: $$\left(1+x\right)^{2n}=\left[(1+x)^n\right]^2$$ No es demasiado difícil usar la pista para probar la identidad (las expresiones en la identidad son los coeficientes de $x^n$ en las respectivas expansiones de las expresiones en la pista, que por supuesto deben ser el mismo número), pero me preguntaba si hay una interpretación equivalente más precisa de contar... Está claro que ${2n \choose n}$ es el número de formas en que podemos elegir la mitad de los elementos en un conjunto (cuando esto sea posible): ¿cómo podemos interpretar $\sum_{k=0}^n{n\choose k}^2$ de manera equivalente?	combinatorics,binomial-coefficients
A.50	Divergent series $\sum{\frac{1}{n^{2+\cos{n}}}}$	Bonjour.  Show that  $$\sum{\frac{1}{n^{2+\cos{n}}}}$$ is a divergent serie. $$\\$$ My main problem is: If $\epsilon$ is “infinitely small positive real number” define $A_{\epsilon}$ as the set of all $n, |2+\cos n|\leq 1+\epsilon$ $(n \in A_{\epsilon}\iff-1\leq \cos n \leq -1+\epsilon)$. The divergence should come from the sum over $A_{\epsilon}$ but I have no idea to how to handle this. $$\\$$	real-analysis,integration,sequences-and-series,analysis
A.50	Serie de divergentes $\sum{\frac{1}{n^{2+\cos{n}}}}$	Bonjour. Muestra que $$\sum{\frac{1}{n^{2+\cos{n}}}}$$ es una serie divergente. $$\\$$ Mi principal problema es: si $\epsilon$ es un número real positivo infinitamente pequeño define $A_{\epsilon}$ como el conjunto de todos los $n, |2+\cos n|\leq 1+\epsilon$ $(n \in A_{\epsilon}\iff-1\leq \cos n \leq -1+\epsilon)$. La divergencia debe provenir de la suma sobre $A_{\epsilon}$ pero no tengo idea de cómo manejar esto. $$\\$$	real-analysis,integration,sequences-and-series,analysis
A.51	Sum of series having binomial coefficients	Prove that $\displaystyle \sum_{r=0}^n {n+r\choose r} \frac{1}{2^{r}}= 2^{n}$ what i try $$\binom{n}{n}+\binom{n+1}{1}\frac{1}{2}+\binom{n+2}{2}\frac{1}{2^2}+\binom{n+3}{3}\frac{1}{2^3}+\cdots +\binom{n+n}{n}\frac{1}{2^n}$$ $$\binom{n}{n}+\binom{n+1}{n}\frac{1}{2}+\binom{n+2}{n}\frac{1}{2^2}+\binom{n+3}{n}\frac{1}{2^3}+\cdots +\binom{n+n}{n}\frac{1}{2^n}.$$ coefficient of $x^n$ in  $$(1+x)^n+(1+x)^{n+1}\frac{1}{2}+(1+x)^{n+2}\frac{1}{2^2}+\cdots\cdots +(1+x)^{2n}\frac{1}{2^n}.$$ How do i solve ithelp me plesse	binomial-coefficients
A.51	Sumas de series con coeficientes binomial	Probar que $\displaystyle \sum_{r=0}^n {n+r\choose r} \frac{1}{2^{r}}= 2^{n}$ lo que intento $$\binom{n}{n}+\binom{n+1}{1}\frac{1}{2}+\binom{n+2}{2}\frac{1}{2^2}+\binom{n+3}{3}\frac{1}{2^3}+\cdots +\binom{n+n}{n}\frac{1}{2^n}$$ $$\binom{n}{n}+\binom{n+1}{n}\frac{1}{2}+\binom{n+2}{n}\frac{1}{2^2}+\binom{n+3}{n}\frac{1}{2^3}+\cdots +\binom{n+n}{n}\frac{1}{2^n}.$$ coeficiente de $x^n$ en $$(1+x)^n+(1+x)^{n+1}\frac{1}{2}+(1+x)^{n+2}\frac{1}{2^2}+\cdots\cdots +(1+x)^{2n}\frac{1}{2^n}.$$ Cómo puedo resolver esto ayúdame por favor.	binomial-coefficients
A.52	Prove $\forall n\in\mathbb{N}$, $\exists m\in\mathbb{N}$ s.t. $m>n$ and $m$ is prime	There are two parts I am having trouble getting started. A. Prove that $n_1, n_2,...,n_k\in\mathbb{N}$ are each at least $2$ then $n=n_1n_2...n_k+1$ is not divisible by any numbers $n_1, n_2,...,n_k$.  B. Prove that the truth of the negation leads to a contradiction. (Use theorem: For all $a,b\in\mathbb{N}$ there exist a unique quotient $q$ and remainder $r$ in $\mathbb{Z^+}$ such that we have both $a=qb+r$ and $0\leq r<q$.) For part A, I started with, given $k\in\mathbb{N}$ and $n_1, n_2,...,n_k\geq1$, I'll show that $\forall i$, $n_i \nmid n=n_1n_2...n_k+1$ to set it up, but I'm not sure how to actually go about starting it. For part B, I know that the negation is $\exists n\in\mathbb{N}$ s.t. $\forall m\in\mathbb{N}$ either $m\leq n$ or $m$ is not prime, but again I'm not sure what I should do to start the proof or exactly how to incorporate that theorem. 	proof-writing,prime-numbers
A.52	Prueba $\forall n\in\mathbb{N}$, $\exists m\in\mathbb{N}$ s.t. $m>n$ y $m$ es primo	Hay dos partes en las que tengo problemas para comenzar. A. Demuestre que $n_1, n_2,...,n_k\in\mathbb{N}$ son cada uno de ellos al menos $2$, entonces $n=n_1n_2...n_k+1$ no es divisible por ningún número $n_1, n_2 ,...,n_k$. B. Demostrar que la verdad de la negación conduce a una contradicción. (Use el teorema: Para todo $a,b\in\mathbb{N}$ existe un cociente único $q$ y el resto $r$ en $\mathbb{Z^+}$ tal que tenemos ambos $a=qb +r$ y $0\leq r<q$.) Para la parte A, comencé con, dado $k\in\mathbb{N}$ y $n_1, n_2,...,n_k\geq1$, muestra que $\forall i$, $n_i \nmid n=n_1n_2...n_k+1$ para configurarlo, pero no estoy seguro de cómo iniciarlo. Para la parte B, sé que la negación es $\exists n\in\mathbb{N}$ s.t. $\forall m\in\mathbb{N}$ $m\leq n$ o $m$ no es primo, pero nuevamente no estoy seguro de qué debo hacer para comenzar la demostración o exactamente cómo incorporar ese teorema.	proof-writing,prime-numbers
A.53	Show that one-sided inverse of a square matrix is a true inverse	We know that for a group element $g\in G$, $gh=1$ does not necessarily mean that $hg = 1$. In the case for matrices (linear maps between vector spaces), it is also true that $AB = 1 \nRightarrow BA = 1$. This happens when the $A$ and $B$ are not square matrices (in which case they do not even form a group under multiplication). However if we restrict the square matrices, $AB = 1 \Rightarrow BA = 1$. What is simple proof of this that avoids chasing the entries, and makes use simply the vector space structure of linear transformations? (In fact if we could prove this, I think this might imply that for a group to have one-sided(but not two-sided) inverses, it has to be infinite, since every finite group admits a finite dimensional representation.	linear-algebra,group-theory
A.53	Muestre que la inversa unilateral de una matriz cuadrada es una inversa verdadera	Sabemos que para un elemento de grupo $g\in G$, $gh=1$ no significa necesariamente que $hg = 1$. En el caso de matrices (mapas lineales entre espacios vectoriales), también es cierto que $AB = 1 \nRightarrow BA = 1$. Esto sucede cuando los $A$ y $B$ no son matrices cuadradas (en cuyo caso ni siquiera forman un grupo bajo multiplicación). Sin embargo, si restringimos las matrices cuadradas, $AB = 1 \Rightarrow BA = 1$. ¿Cuál es la prueba simple de esto que evita perseguir las entradas, y utiliza simplemente la estructura del espacio vectorial de las transformaciones lineales? (De hecho, si pudiéramos probar esto, creo que esto podría implicar que para un grupo tener inversos unilaterales, pero no bidimensionales, tiene que ser infinito, ya que cada grupo finito admite una representación dimensional finita.	linear-algebra,group-theory
A.54	By using a diagonal argument, show that the powerset $P(N) = (S|S ⊆ N)$ is uncountable.	Any tips or solutions for this one? By using a diagonal argument, show that the powerset $P(N) = (S|S ⊆ N)$ is uncountable.	discrete-mathematics,elementary-set-theory
A.54	Usando un argumento diagonal, muestra que el conjunto de potencias $P(N) = (S|S ⊆ N)$ es incontable.	¿Alguna sugerencia o solución para este? Usando un argumento diagonal, muestre que el conjunto de poderes $P(N) = (S|S ⊆ N)$ es incontable.	discrete-mathematics,elementary-set-theory
A.55	$\frac{1}{\sqrt{-1}}=\sqrt{-1}$?	I have trouble to comprehend what my mistake is in the following calculation: If we set $\sqrt{-1}$ to be the new number with the property that $(\sqrt{-1})^2 = -1$ then I can write $$\frac{1}{\sqrt{-1}}=\sqrt{\frac{1}{-1}}=\sqrt{-1}.$$ But we also have (and I know this is the correct result) $$ \frac{1}{\sqrt{-1}}\cdot\frac{\sqrt{-1}}{\sqrt{-1}}=\frac{\sqrt{-1}}{-1}=-\sqrt{-1}$$ What am I missing? Thanks.	complex-numbers,definition
A.55	¿$\frac{1}{\sqrt{-1}}=\sqrt{-1}$?	Tengo problemas para entender cuál es mi error en el siguiente cálculo: si establecemos $\sqrt{-1}$ como el nuevo número con la propiedad de que $(\sqrt{-1})^2 = -1$ entonces puedo escribir $$\frac{1}{\sqrt{-1}}=\sqrt{\frac{1}{-1}}=\sqrt{-1}.$$ Pero también tenemos (y sé que este es el resultado correcto) $$ \frac{1}{\sqrt{-1}}\cdot\frac{\sqrt{-1}}{\sqrt{-1}}=\frac{\sqrt{-1}}{-1}=-\sqrt{-1}$$ ¿Qué me falta? Gracias.	complex-numbers,definition
A.56	A curious logical formula involving prime numbers	Let $S$ be a nonempty set of natural numbers. Is the following formula $$ \exists p\ \bigl(\text{$p$ is prime } \rightarrow \forall x  \text{ ($x$ is prime)}\bigr)  $$ true or false on $S$? I know the answer to this question, but what would be the shortest way to arrive to the conclusion using some deduction system?	logic,first-order-logic
A.56	Una curiosa fórmula lógica que involucra números primos	Si $S$ es un conjunto de números naturales no vacíos, ¿es verdadera o falsa la siguiente fórmula $$ \exists p\ \bigl(\text{$p$ is prime } \rightarrow \forall x  \text{ ($x$ is prime)}\bigr)  $$ en $S$? Conozco la respuesta a esta pregunta, pero ¿cuál sería el camino más corto para llegar a la conclusión utilizando algún sistema de deducción?	logic,first-order-logic
A.57	Preimage of continuous one-to-one function on connected domain is not continuous.	I know that given $B$, a compact subset of $\mathbb{R}^n$, and $f : B \to \mathbb{R}^m$, a continuous injective (one-to-one) function, $f^{-1}$ is continuous on $f(B)$. (This true). I also know that image $f(X)$ of a connected subset $X$ is connected under a continuous function. Now let $X$ be a connected (non-compact) subset of $\mathbb{R}^n$, and $f : X \to \mathbb{R}^m$ be a continuous injective (one-to-one) function. I am trying (and struggling) to provide a counterexample in which mapping $f^{-1} : f(X) \mapsto X$ is not continuous on $f(X)$. (A rigorous, parametrized example). Thank you in advance!	real-analysis,general-topology,analysis,continuity,metric-spaces
A.57	La imagen previa de la función continua uno a uno en un dominio conectado no es continua.	Sé que dado $B$, un subconjunto compacto de $\mathbb{R}^n$ y $f : B \to \mathbb{R}^m$, una función inyectiva continua (uno a uno), $f^{-1}$ es continua en $f(B)$. (esto es cierto). También sé que la imagen $f(X)$ de un subconjunto conectado $X$ está conectada bajo una función continua. Ahora, que $X$ sea un subconjunto conectado (no compacto) de $\mathbb{R}^n$, y $f : X \to \mathbb{R}^m$ sea una función inyectiva continua (uno a uno). Estoy tratando (y luchando) para proporcionar un contraejemplo en el que el mapeo de $f^{-1} : f(X) \mapsto X$ no es continuo en $f(X)$. (Un ejemplo riguroso y parámétrico). Gracias por adelantado!	real-analysis,general-topology,analysis,continuity,metric-spaces
A.58	Prove that $3\arcsin \frac{1}{4} + \arccos \frac {11}{16} = \frac {\pi}{2}$	Can someone help me with this exercise? I honestly don't know where to start and how to prove it. You don't have to answer it fully, just give me a hint or something. Thank you in advance.  Exercise 1. Prove that  $3\arcsin \frac{1}{4} + \arccos \frac {11}{16} = \frac {\pi}{2}$  Thanks.	trigonometry
A.58	Prueba que $3\arcsin \frac{1}{4} + \arccos \frac {11}{16} = \frac {\pi}{2}$	¿Puede alguien ayudarme con este ejercicio? Sinceramente no sé por dónde empezar y cómo demostrarlo. No tienes que contestarlo completamente, sólo dame una pista o algo así. Gracias por adelantado. Ejercicio 1. Demuestra que $3\arcsin \frac{1}{4} + \arccos \frac {11}{16} = \frac {\pi}{2}$ Gracias.	trigonometry
A.59	Multiple proofs of $\sum_{d|n}{\phi(d)}=n$	I am looking for multiple proofs of that statement: here $\phi(n)$ denotes the Euler’s totient  $$\sum_{d|n}{\phi(d)}=n$$  Here’s one:  By unique factorisation theorem: $n=\prod_{k=1}^{m}{p_k^{\alpha_k}}$ and $d=\prod_{k=1}^{m}{p_k^{\beta_k}}$ where $0\leq \beta_k\leq \alpha_k$ so:  $\begin{align} \sum_{d|n}{\phi(d)}&=\sum_{0\leq \beta_k\leq \alpha_k}{\phi\left(\prod_{k=1}^{m}{p_k^{\beta_k}}\right)}\\ &= \sum_{0\leq \beta_k\leq \alpha_k}{\prod_{k=1}^{m}\phi({p_k^{\beta_k})}}\\ &=\sum_{0\leq \beta_k\leq \alpha_k}{\prod_{k=1}^{m}{(p_k^{\beta_k}-p_k^{\beta_k-1}})}\\ &=\prod_{k=1}^{m}{\sum_{0\leq \beta_k\leq \alpha_k}{(p_k^{\beta_k}-p_k^{\beta_k-1}}})\\ &= \prod_{k=1}^{m}{p_k^{\alpha_k}}\\ &=n. \end{align}$	group-theory,number-theory,alternative-proof,big-list
A.59	Pruebas múltiples de $\sum_{d|n}{\phi(d)}=n$	Estoy buscando múltiples pruebas de esa afirmación: aquí $\phi(n)$ denota el totiente de Euler $$\sum_{d|n}{\phi(d)}=n$$ Aquí uno: por teorema de factorizamiento único: $n=\prod_{k=1}^{m}{p_k^{\alpha_k}}$ y $d=\prod_{k=1}^{m}{p_k^{\beta_k}}$ donde $0\leq \beta_k\leq \alpha_k$ así: $\begin{align} \sum_{d|n}{\phi(d)}&=\sum_{0\leq \beta_k\leq \alpha_k}{\phi\left(\prod_{k=1}^{m}{p_k^{\beta_k}}\right)}\\ &= \sum_{0\leq \beta_k\leq \alpha_k}{\prod_{k=1}^{m}\phi({p_k^{\beta_k})}}\\ &=\sum_{0\leq \beta_k\leq \alpha_k}{\prod_{k=1}^{m}{(p_k^{\beta_k}-p_k^{\beta_k-1}})}\\ &=\prod_{k=1}^{m}{\sum_{0\leq \beta_k\leq \alpha_k}{(p_k^{\beta_k}-p_k^{\beta_k-1}}})\\ &= \prod_{k=1}^{m}{p_k^{\alpha_k}}\\ &=n. \end{align}$	group-theory,number-theory,alternative-proof,big-list
A.60	Limiting value of a sequence when n tends to infinity	Q) Let, $a_{n} \;=\; \left ( 1-\frac{1}{\sqrt{2}} \right ) ... \left ( 1- \frac{1}{\sqrt{n+1}} \right )$ , $n \geq 1$. Then $\lim_{n\rightarrow \infty } a_{n}$ (A) equals $1$ (B) does not exist (C) equals $\frac{1}{\sqrt{\pi }}$ (D) equals $0$  My Approach :- I am not getting a particular direction or any procedure to simplify $a_{n}$ and find its value when n tends to infinity. So, I tried like this simple way to substitute values and trying to find the limiting value :- $\left ( 1-\frac{1}{\sqrt{1+1}} \right ) * \left ( 1-\frac{1}{\sqrt{2+1}} \right )*\left ( 1-\frac{1}{\sqrt{3+1}} \right )*\left ( 1-\frac{1}{\sqrt{4+1}} \right )*\left ( 1-\frac{1}{\sqrt{5+1}} \right )*\left ( 1-\frac{1}{\sqrt{6+1}} \right )*\left ( 1-\frac{1}{\sqrt{7+1}} \right )*\left ( 1-\frac{1}{\sqrt{8+1}} \right )*.........*\left ( 1-\frac{1}{\sqrt{n+1}}    \right )$  =$(0.293)*(0.423)*(0.5)*(0.553)*(0.622)*(0.647)*(0.667)* ....$ =0.009*... So, here value is tending to zero. I think option $(D)$ is correct. I have tried like this $\left ( \frac{\sqrt{2}-1}{\sqrt{2}} \right )*\left ( \frac{\sqrt{3}-1}{\sqrt{3}} \right )*\left ( \frac{\sqrt{4}-1}{\sqrt{4}} \right )*.......\left ( \frac{\sqrt{(n+1)}-1}{\sqrt{n+1}} \right )$ = $\left ( \frac{(\sqrt{2}-1)*(\sqrt{3}-1)*(\sqrt{4}-1)*.......*(\sqrt{n+1}-1)}{{\sqrt{(n+1)!}}} \right )$ Now, again I stuck how to simplify further and find the value for which $a_{n}$ converges when $n$ tends to infinity . Please help if there is any procedure to solve this question.	calculus,sequences-and-series,limits,products
A.60	Limitando el valor de una secuencia cuando n tiende a infinito	P) Dejamos que, $a_{n} \;=\; \left ( 1-\frac{1}{\sqrt{2}} \right ) ... \left ( 1- \frac{1}{\sqrt{n+1}} \right )$ , $n \geq 1$. Entonces $\lim_{n\rightarrow \infty } a_{n}$ (A) igual a $1$ (B) no existe (C) igual a $\frac{1}{\sqrt{\pi }}$ (D) igual a $0$ Mi enfoque :- No estoy recibiendo una dirección particular o ningún procedimiento para simplificar $a_{n}$ y encontrar su valor cuando n tiende a infinito. Así que, intenté esta manera simple de sustituir valores y tratar de encontrar el valor limitante :- $\left ( 1-\frac{1}{\sqrt{1+1}} \right ) * \left ( 1-\frac{1}{\sqrt{2+1}} \right )*\left ( 1-\frac{1}{\sqrt{3+1}} \right )*\left ( 1-\frac{1}{\sqrt{4+1}} \right )*\left ( 1-\frac{1}{\sqrt{5+1}} \right )*\left ( 1-\frac{1}{\sqrt{6+1}} \right )*\left ( 1-\frac{1}{\sqrt{7+1}} \right )*\left ( 1-\frac{1}{\sqrt{8+1}} \right )*.........*\left ( 1-\frac{1}{\sqrt{n+1}}    \right )$ =$(0.293)*(0.423)*(0.5)*(0.553)*(0.622)*(0.647)*(0.667)* ....$ =0.009*... Así que aquí el valor tiende a cero. Creo que la opcion $(D)$ es correcto. He intentado como esta opción $a_{n} \;=\; \left ( 1-\frac{1}{\sqrt{2}} \right ) ... \left ( 1- \frac{1}{\sqrt{n+1}} \right )$1 = $a_{n} \;=\; \left ( 1-\frac{1}{\sqrt{2}} \right ) ... \left ( 1- \frac{1}{\sqrt{n+1}} \right )$2 Ahora, otra vez me he atascado cómo simplificar más y encontrar el valor para el cual $a_{n}$ converge cuando $a_{n} \;=\; \left ( 1-\frac{1}{\sqrt{2}} \right ) ... \left ( 1- \frac{1}{\sqrt{n+1}} \right )$3 tiende a infinito . Por favor, ayude si hay algún procedimiento para resolver esta pregunta.	calculus,sequences-and-series,limits,products
A.61	There exists $i, j \in \mathbb{N}$ such that $n=3i+5j$ for $n\ge 8$	Prove that there exists $i, j \in \mathbb{N}$ such that $n=3i+5j$ for $n\ge 8$  I'm having a hard time with this exercise, I'm trying to prove it by induction: Basis step:  $n=8 \implies 8=3\cdot1+5\cdot 1$ $n=9 \implies 9=3\cdot3+5\cdot0$ $n=10 \implies 10=3\cdot0+5\cdot2$  Induction step: If it's true for $n=h$ then it must be true for $n=h+1$. So now, I don't know how to begin proving that $k+1=3i+5j$.	elementary-number-theory,discrete-mathematics,induction,diophantine-equations
A.61	Existe $i, j \in \mathbb{N}$ tal que $n=3i+5j$ para $n\ge 8$	Pruebe que existe $i, j \in \mathbb{N}$ tal que $n=3i+5j$ para $n\ge 8$ Estoy teniendo un tiempo difícil con este ejercicio, estoy tratando de probarlo por inducción: Paso base: $n=8 \implies 8=3\cdot1+5\cdot 1$ $n=9 \implies 9=3\cdot3+5\cdot0$ $n=10 \implies 10=3\cdot0+5\cdot2$ Paso de inducción: Si es cierto para $n=h$ entonces debe ser cierto para $n=h+1$. Así que ahora, no sé cómo empezar a probar que $k+1=3i+5j$.	elementary-number-theory,discrete-mathematics,induction,diophantine-equations
A.62	Prove that the cardinality of the set of rational numbers and the set of integers is equal	I just learned about cardinality in my discrete class a few days ago, and this is in the homework. This is all fairly confusing to me, and I'm not entirely sure where to even start. Here's the full question: Let $\mathbb{Q}$ denote the set of rational numbers and $\mathbb{Z}$ denote the set of integers. Prove that $|\mathbb{Q}| = |\mathbb{Z}|$. I thought about saying that every element in $\mathbb{Q}$ can be written as some element in $\mathbb{Z} \times \mathbb{Z}$, but I still don't know how to prove that that is a bijection, or even how to prove that $|\mathbb{Z} \times \mathbb{Z}| = |\mathbb{Z}|$. Any help would be greatly appreciated.	discrete-mathematics
A.62	Demostrar que la cardinalidad del conjunto de números racionales y el conjunto de números enteros es igual	Hace unos días aprendí sobre la cardinalidad en mi clase discreta, y esto está en la tarea. Esto es bastante confuso para mí, y no estoy completamente seguro de dónde empezar. Aquí está la pregunta completa: Dejamos que $\mathbb{Q}$ denotar el conjunto de números racionales y $\mathbb{Z}$ denotar el conjunto de números enteros. Pruebe que $|\mathbb{Q}| = |\mathbb{Z}|$. Pensé en decir que cada elemento en $\mathbb{Q}$ se puede escribir como algún elemento en $\mathbb{Z} \times \mathbb{Z}$, pero todavía no sé cómo demostrar que es una bijección, o incluso cómo probar que $|\mathbb{Z} \times \mathbb{Z}| = |\mathbb{Z}|$. Cualquier ayuda sería muy apreciada.	discrete-mathematics
A.63	$\gcd$ and $\text{lcm}$ of more than $2$ positive integers	For any two positive integers ${n_1,n_2}$, the relationship between their greatest common divisor and their least common multiple is given by $$\text{lcm}(n_1,n_2)=\frac{n_1 n_2}{\gcd(n_1,n_2)}$$ If I have a set of $r$ positive integers ${n_1,n_2,n_3,...,n_r}$, does the same relationship hold? Is it true that $$\text{lcm}(n_1,n_2,n_3,...,n_r)=\frac{\prod_{i=1}^r n_i}{gcd(n_1,n2,n_3,...,n_r)}$$ I feel like this should be easy to prove, but I'm struggling to get a handle on it.	proof-explanation,greatest-common-divisor,least-common-multiple
A.63	$\gcd$ y $\text{lcm}$ de más de $2$ numeros positivos	Para cualquier dos números enteros positivos ${n_1,n_2}$, la relación entre su divisor común más grande y su múltiplo común menor es dada por $$\text{lcm}(n_1,n_2)=\frac{n_1 n_2}{\gcd(n_1,n_2)}$$ Si tengo un conjunto de $r$ números enteros positivos ${n_1,n_2,n_3,...,n_r}$, ¿se mantiene la relación? ¿es verdad que $$\text{lcm}(n_1,n_2,n_3,...,n_r)=\frac{\prod_{i=1}^r n_i}{gcd(n_1,n2,n_3,...,n_r)}$$ siento que esto debería ser fácil de probar, pero estoy luchando para conseguir un comprendimiento en él.	proof-explanation,greatest-common-divisor,least-common-multiple
A.64	Suppose that f : [a, b] → R is continuous and that f([a, b]) ⊂ [a, b]. Prove that there exists a point c ∈ [a, b] satisfying f(c) = c.	Suppose that $f : [a, b] \to \mathbb{R}$ is continuous and that $f([a, b]) \subset [a, b]$. Prove that there exists a point $c \in [a, b]$ satisfying $f(c) = c$.  (If either $f(a) = a$ or $f(b) = b$ there is nothing left to show, so you might as well assume that $f(a) = a$ and $f(b) = b$. Since $f$ takes its values in $[a, b]$ this is the same as assuming that $f(a) > a$ and $f(b) < b$.) So far, I have: Pf. Assume $f(a)>a$ and $f(b)< b$.  Let $x, y \in [a,b]$ such that $f(a)=x$ and $f(b)=y$ which means $f[a,b]=[x,y]$. Notice $[x,y]\subset [a, b]$. Since f is continuous on $[x,y]$, there exists some $c \in [a,b]$ such that $x$ is less than or equal to $c$ is less than or equal to $y$... This is where I am stuck because I don't think I can just assume by Intermediate Value Theorem that some $f(c)=c$?	real-analysis,continuity,proof-explanation
A.64	Supongamos que f: [a, b] → R es continua y que f([a, b])  [a, b]. Pruebe que existe un punto c ∈ [a, b] satisfactorio f(c) = c.	Supongamos que $f : [a, b] \to \mathbb{R}$ es continuo y que $f([a, b]) \subset [a, b]$. Pruebe que existe un punto $c \in [a, b]$ que satisface $f(c) = c$. (Si o $f(a) = a$ o $f(b) = b$ no queda nada que mostrar, entonces también puede asumir que $f(a) = a$ y $f(b) = b$. Dado que $f$ toma sus valores en $[a, b]$, esto es lo mismo que suponer que $f(a) > a$ y $f(b) < b$.) Hasta ahora, tengo: Pf. Supongamos $f(a)>a$ y $f(b)<b$. Sea $x, y \in [a,b]$ tal que $f(a)=x$ y $f(b)=y$ lo que significa $f[a,b]=[x,y]$. Observe $[x,y]\subset [a, b]$. Dado que f es continua en $[x,y]$, existe algún $c \en [a,b]$ tal que $x$ es menor o igual que $c$ es menor o igual a $y$. .. Aquí es donde estoy estancado porque no creo que pueda asumir mediante el teorema del valor intermedio que algún $f(c)=c$?	real-analysis,continuity,proof-explanation
A.65	How can we show that $e^{-2\lambda t}\lambda^2\le\frac1{e^2t^2}$ for all $\lambda,t\ge0$?	How can we show that $$e^{-2\lambda t}\lambda^2\le\frac1{e^2t^2}\tag1$$ for all $\lambda,t\ge0$? Applying $\ln$ to both sides yields that $(1)$ should be equivalent to $$t\lambda\le e^{t\lambda-1}\tag2.$$ So, if I did no mistake, it should suffice to show $x\le e^{x-1}$ for all $x\ge0$. How can we do this?	calculus,inequality,exponential-function
A.65	¿Cómo podemos mostrar que $e^{-2\lambda t}\lambda^2\le\frac1{e^2t^2}$ para todos los $\lambda,t\ge0$?	¿Cómo podemos mostrar que $$e^{-2\lambda t}\lambda^2\le\frac1{e^2t^2}\tag1$$ para todos los $\lambda,t\ge0$? Aplicando $\ln$ a ambos lados da que $(1)$ debe ser equivalente a $$t\lambda\le e^{t\lambda-1}\tag2.$$ Así que, si no me equivoqué, debería ser suficiente para mostrar $x\le e^{x-1}$ para todos los $x\ge0$. ¿Como podemos hacer esto?	calculus,inequality,exponential-function
A.66	if $x,h \in \mathbb{R}^d$ and $A \in \mathbb{R}^{d\times d}$ is it possible to justify that $(x^TAh)^T = h^TA^Tx$?	if $x,h \in \mathbb{R}^d$ and $A \in \mathbb{R}^{d\times d}$ is it possible to justify that $(x^TAh)^T = h^TA^Tx$?	linear-algebra,transpose
A.66	si $x,h \in \mathbb{R}^d$ y $A \in \mathbb{R}^{d\times d}$ es posible justificar que $(x^TAh)^T = h^TA^Tx$?	si $x,h \in \mathbb{R}^d$ y $A \in \mathbb{R}^{d\times d}$ es posible justificar que $(x^TAh)^T = h^TA^Tx$?	linear-algebra,transpose
A.67	Combination of matrixes	If A is a $k\times k$ matrix,B is a $k\times l$ matrix and C is a $l\times l$ matrix prove that: $\det{\begin{bmatrix}A&B\\O&C\end{bmatrix}}=\det(A)\det(C)$ O is the matrix that all it's elements are equal to zero. I know some rules for calculating determinants but I don't know how to begin in this question. 	calculus,determinant
A.67	Combinación de matrices	Si A es una matriz $k\times k$, B es una matriz $k\times l$ y C es una matriz $l\times l$ prueba que: $\det{\begin{bmatrix}A&B\\O&C\end{bmatrix}}=\det(A)\det(C)$ O es la matriz que todos sus elementos son iguales a cero. Conozco algunas reglas para calcular los determinantes pero no sé cómo empezar con esta pregunta.	calculus,determinant
A.68	Prove $a^n+1$ is divisible by $a + 1$ if $n$ is odd	Prove $a^n+1$ is divisible by $a + 1$ if $n$ is odd: We know $a$ cannot be $-1$ and the $n \in \mathbb{N}$. Since $n$ must be odd, we can rewrite $n$ as $2k+1$. Now we assume it holds for prove that it holds for the next term. $$a^{2(k+1)+1}+1$$ $$=a^{2k+3}+1$$ $$=a^3\cdot a^{2k}+1$$ $$=(a^3+1)\cdot a^{2k} -a^{2k}+1$$ Im not sure on what to do next. Since $a^{2k}$ means that the exponential term will be even and thus you cant use the fact that $a^n+1$ is divisible by $a + 1$ if $n$ is odd.	polynomials,induction,divisibility
A.68	Prueba que $a^n+1$ es divisible por $a + 1$ si $n$ es impar	Prueba que $a^n+1$ es divisible por $a + 1$ si $n$ es impar: sabemos que $a$ no puede ser $-1$ y el $n \in \mathbb{N}$. ya que $n$ debe ser impar, podemos reescribir $n$ como $2k+1$. Ahora asumimos que se mantiene para probar que se mantiene para el siguiente término. $$a^{2(k+1)+1}+1$$ $$=a^{2k+3}+1$$ $a^n+1$0 $a^n+1$1 No estoy seguro de qué hacer a continuación. Dado que $a^{2k}$ significa que el término exponencial será par y, por lo tanto, no puedes usar el hecho de que $a^n+1$ es divisible por $a + 1$ si $n$ es impar.	polynomials,induction,divisibility
A.69	Induction with two variable parameters	So I was assigned this homework problem: $$\ {s \choose s} + {s+1 \choose s} +...+ {n \choose s} = {n+1 \choose s+1}$$ for all s and all $n \geq s$ I've tried to email both my professor and my TA and their explanations seem contradictory. My professor responded saying the statement I need to prove is "The formula is correct for $0 \leq s \leq n$." Whereas my TA told me I need to use induction on both variables and I'm not sure how to do that. Any help is appreciated!	combinatorics
A.69	Inducción con dos parámetros variables	Así que me asignaron este problema de tarea: $$\ {s \choose s} + {s+1 \choose s} +...+ {n \choose s} = {n+1 \choose s+1}$$ para todas las s y todas las $n \geq s$ he intentado enviar por correo electrónico a mi profesor y a mi TA y sus explicaciones parecen contradictorias. Mi profesor respondió diciendo que la declaración que necesito probar es "La fórmula es correcta para $0 \leq s \leq n$." Mientras que mi TA me dijo que necesito usar inducción en ambas variables y no estoy seguro de cómo hacerlo. Cualquier ayuda es apreciada!	combinatorics
A.70	Proving $\sum_{j=0}^{N-1}\cos\frac{\left(2j+1\right)\pi}{2N}=0$	Let $l\in\mathbb{Z}$ and $N\in\mathbb{N}$. I need to prove the following: $\begin{equation} \sum_{j=0}^{N-1}\cos\left(l\frac{\left(2j+1\right)\pi}{2N} \right)=0 \end{equation}$ I tried to use Euler formula and then sum the first $N$ terms of the geometric serie I get, but it didn't work. Any ideas?	trigonometry,summation
A.70	Prueba de que $\sum_{j=0}^{N-1}\cos\frac{\left(2j+1\right)\pi}{2N}=0$	‘Dejamos que $l\in\mathbb{Z}$ y $N\in\mathbb{N}$. Necesito probar lo siguiente: $\begin{equation} \sum_{j=0}^{N-1}\cos\left(l\frac{\left(2j+1\right)\pi}{2N} \right)=0 \end{equation}$ Intenté usar la fórmula de Euler y luego sumar los primeros términos $N$ de la serie geométrica que obtuve, pero no funcionó. ¿Alguna idea?	trigonometry,summation
A.71	Show, with induction that $1^2 + 2^2 + .... + n^2 = \frac{n(n+1)(2n+1)}{6}$	Show, with induction that $1^2 + 2^2 + .... + n^2 = \frac{n(n+1)(2n+1)}{6}$ My attempt Case 1: n = 1 $LHS = 1^2$  $RHS = \frac{(1+1)(2+1)}{6} = \frac{2*3}{6} = 1$ Case 2: n = p $LHS_{p} = 1^2 + 2^2 + ... + p^2$ $RHS_{p} = \frac{p(p+1)(2p+1)}{6}$ Case 3: n = p + 1 $LHS_{p+1} = 1^2+2^2+....+p^2+(p+1)^2$ $RHS_{p+1} = \frac{(p+1)((p+1)+1)(2(p+1)+1)}{6}$ Now to show this with induction I think i need to show that $RHS_{p+1} = RHS_{p} + (p+1)^2$ $RHS_{p+1} = \frac{p(p+1)(2p+1)}{6} + (p+1)^2$ So I need to rewrite  $RHS_{p+1} = \frac{(p+1)((p+1)+1)(2(p+1)+1)}{6} $to be equal to $\frac{p(p+1)(2p+1)}{6} + (p+1)^2$  Anyone see how I can do that? Or got any other solution?	induction
A.71	Muestra, con inducción que $1^2 + 2^2 + .... + n^2 = \frac{n(n+1)(2n+1)}{6}$	Muestre, con inducción que $1^2 + 2^2 + .... + n^2 = \frac{n(n+1)(2n+1)}{6}$ Mi intento caso 1: n = 1 $LHS = 1^2$ $RHS = \frac{(1+1)(2+1)}{6} = \frac{2*3}{6} = 1$ caso 2: n = p $LHS_{p} = 1^2 + 2^2 + ... + p^2$ $RHS_{p} = \frac{p(p+1)(2p+1)}{6}$ caso 3: n = p + 1 $LHS_{p+1} = 1^2+2^2+....+p^2+(p+1)^2$ $RHS_{p+1} = \frac{(p+1)((p+1)+1)(2(p+1)+1)}{6}$ Ahora para mostrar esto con inducción creo que necesito mostrar que $RHS_{p+1} = RHS_{p} + (p+1)^2$ $RHS_{p+1} = \frac{p(p+1)(2p+1)}{6} + (p+1)^2$ Así que necesito reescribir $1^2 + 2^2 + .... + n^2 = \frac{n(n+1)(2n+1)}{6}$0 para ser igual a $1^2 + 2^2 + .... + n^2 = \frac{n(n+1)(2n+1)}{6}$1 ¿Alguien ve cómo puedo hacer eso? O alguna otra solución?	induction
A.72	Is it possible that $\mathcal{X} = \mathcal{Y}$, yet $\mathcal{X} \in \mathcal{Y}$?	Is it possible for a set to equal another set, yet the former set be an element in the latter set?  I.e.: $\mathcal{X} = \mathcal{Y}$, yet $\mathcal{X} \in \mathcal{Y}$	set-theory,axioms
A.72	¿Es posible que $\mathcal{X} = \mathcal{Y}$, pero $\mathcal{X} \in \mathcal{Y}$?	¿Es posible que un conjunto sea igual a otro conjunto, pero el conjunto anterior sea un elemento en el último conjunto? por ejemplo  I.e.: $\mathcal{X} = \mathcal{Y}$, yet $\mathcal{X} \in \mathcal{Y}$	set-theory,axioms
A.73	Help on proof of $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2 = \binom{2n}{n}$	The proof is required to be made through the binomial theorem. I will expose the demonstration I was tought, and forward my questions after exposing it. You'll see question marks like this one (?-n) on points I don't quite understand, where $n$ is the numeration of the mark. This are the doubts I have about the demonstration, the which I hope someone can clarify. Prove that $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2 = \binom{2n}{n}$. We will use the following equality, and call it $P$:  $(1+x)^n(1+x)^n=(1+x)^{2n}$ (?-1) The result will be proved finding the $x^n$ coefficient of both terms of this equality (?-2). According to the binomial theorem, the left-hand side of this equation is the product of two factors, both equal to $\binom{n}{0}1+\binom{n}{1}x+...+\binom{n}{r}x^r+...+\binom{n}{n}x^n$ When both factors multiply, a term on $x^n$ is obtained when a term of the first factor has some $x^i$ and the term of the second factor has some $x^{n-i}$. Therefor the coefficients of $x^n$ are $\binom{n}{0}\binom{n}{n}+\binom{n}{1}\binom{n}{n-1}+\binom{n}{2}\binom{n}{n-2}+...\binom{n}{n}\binom{n}{0}$ . Since $\binom{n}{n-r}=\binom{n}{r}$, the previous summation is equal to $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2$. So the left hand side of the equation we are asked to proove is a coefficient of $x^n$. When we expand the right-hand side of the equation $P$, we find that $\binom{2n}{n}$ is a coefficient of $x^n$. Therefore (?-3) the left-hand side of the equation we were asked to prove is in deed equal to $\binom{2n}{n}$. In conclussion, $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2 = \binom{2n}{n}$. This was all the demonstration. My doubt one (?-1) goes about where the heck does this equation come from? How would I know what equation to come up with if requested to prove a different equality? Doubt two (?-2) goes about why would the solution of the first equation would have anything to do with finding the $x^n$ coefficients of the one I just made up (see doubt one). Doubt three (?-3) goes about why demonstrating that $a$ is a coefficient of $x^n$ on the left hand side of the equation I made up, and that $b$ is a coefficient of $x^n$ on the right-hand side of this equation as well, would prove my original equation, the one I was supposed to prove on the first place? I know there are many doubts here, I hope you guys can help me. Sorry for the long post, it's a long demonstration.	discrete-mathematics,binomial-coefficients,binomial-theorem
A.73	Ayuda en la prueba de $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2 = \binom{2n}{n}$	La demostración debe realizarse mediante el teorema del binomio. Expondré la demostración que me enseñaron y reenviaré mis preguntas después de exponerla. Verás signos de interrogación como este (?-n) en puntos que no entiendo del todo, donde $n$ es la numeración de la marca. Estas son las dudas que tengo sobre la manifestación, las cuales espero que alguien pueda aclarar. Demuestre que $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2 = \binom{2n}{n}$. Usaremos la siguiente igualdad y la llamaremos $P$: $(1+x)^n(1+x)^n=(1+x)^{2n}$ (?-1) El resultado será probado al encontrar el coeficiente $x^n$ de ambos términos de esta igualdad (?-2). Según el teorema del binomio, el lado izquierdo de esta ecuación es el producto de dos factores, ambos iguales a $\binom{n}{0}1+\binom{n}{1}x+...+\binom {n}{r}x^r+...+\binom{n}{n}x^n$ Cuando ambos factores se multiplican, se obtiene un término de $x^n$ cuando un término del primer factor tiene algo de $ x^i$ y el término del segundo factor tiene algo de $x^{n-i}$. Por lo tanto, los coeficientes de $x^n$ son $\binom{n}{0}\binom{n}{n}+\binom{n}{1}\binom{n}{n-1}+\binom{ n}{2}\binom{n}{n-2}+...\binom{n}{n}\binom{n}{0}$ . Dado que $\binom{n}{n-r}=\binom{n}{r}$, la suma anterior es igual a $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2$. Entonces, el lado izquierdo de la ecuación que se nos pide que demostremos es un coeficiente de $x^n$. Cuando expandimos el lado derecho de la ecuación $P$, encontramos que $\binom{2n}{n}$ es un coeficiente de $x^n$. Por lo tanto (?-3) el lado izquierdo de la ecuación que se nos pidió que probáramos es de hecho igual a $\binom{2n}{n}$. En conclusión, $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2 = \binom{2n}{n}$ . Esta fue toda la manifestación. Mi duda (? -1) es ¿de dónde diablos viene esta ecuación? ¿Cómo sabría qué ecuación encontrar si me piden que demuestre una igualdad diferente? La duda dos (? -2) trata sobre por qué la solución de la primera ecuación tendría algo que ver con encontrar los coeficientes $x^n$ de la que acabo de inventar (ver duda uno). La duda tres (?-3) es sobre por qué demostrar que $a$ es un coeficiente de $x^n$ en el lado izquierdo de la ecuación que inventé, y que $b$ es un coeficiente de $x^n$. en el lado derecho de esta ecuación también, ¿probaría mi ecuación original, la que se suponía que debía probar en primer lugar? Sé que hay muchas dudas aquí, espero que puedan ayudarme. Perdón por la publicación larga, es una demostración larga.	discrete-mathematics,binomial-coefficients,binomial-theorem
A.74	Show that the image of the function $f:(0,\infty)\rightarrow \mathbb{R}$, $f(x)=x+\dfrac{1}{x}$ is the interval $[2,\infty)$.	Show that the image of the function $f:(0,\infty)\rightarrow \mathbb{R}$, $f(x)=x+\dfrac{1}{x}$ is the interval $[2,\infty)$.  If $x=1$, then $f(1)=2$. So how can I show that the mage of the function is the interval $[2,\infty)$?	functions,elementary-set-theory
A.74	Muestre que la imagen de la función $f:(0,\infty)\rightarrow \mathbb{R}$, $f(x)=x+\dfrac{1}{x}$ es el intervalo $[2,\infty)$.	Muestre que la imagen de la función $f:(0,\infty)\rightarrow \mathbb{R}$, $f(x)=x+\dfrac{1}{x}$ es el intervalo $[2,\infty)$. Si $x=1$, entonces $f(1)=2$. Entonces, ¿cómo puedo mostrar que el mago de la función es el intervalo $[2,\infty)$?	functions,elementary-set-theory
A.75	Prove that for each integer $m$, $ \lim_{u\to \infty} \frac{u^m}{e^u} = 0 $	I'm unsure how to show  that for each integer $m$, $ \lim_{u\to \infty} \frac{u^m}{e^u} = 0 $.   Looking at the solutions it starts with $e^u$ $>$ $\frac{u^{m+1}}{(m+1)!}$ but not sure how this is a logical step.	real-analysis,calculus,limits
A.75	Pruebe que para cada número entero $m$, $ \lim_{u\to \infty} \frac{u^m}{e^u} = 0 $	No estoy seguro de cómo mostrar que para cada número entero $m$, $ \lim_{u\to \infty} \frac{u^m}{e^u} = 0 $. Mirando las soluciones comienza con $e^u$ $>$ $\frac{u^{m+1}}{(m+1)!}$ pero no estoy seguro de cómo este es un paso lógico.	real-analysis,calculus,limits
A.76	Covering $\mathbb{Z}$ by arithmetic progressions	I am solving problems from an old exam (in topology, but I've translated the problem into more algebraic terms). The problem is the following:  Let $a+b\mathbb{Z}=\{z\in \mathbb{Z}\mid z = a+bk \text{ for some  }k\in \mathbb{Z}\}$ where $a\in \mathbb{Z}$ and $b\in  \mathbb{Z}-\{0\}$. Suppose we have a collection of such sets   $\{a_i+b_i\mathbb{Z}\mid i \in \mathbb{N}\}$   satisfying: $$\bigcup_{i \in \mathbb{N}}(a_i+b_i\mathbb{Z})=\mathbb{Z}$$ Show whether it is always possible to extract a finite   $I\subset \mathbb{N}$ s.t. $$\bigcup_{i \in I}(a_i+b_i\mathbb{Z})=\mathbb{Z}$$  Unfortunately, I seem to have forgotten a lot of my elementary algebra... Nevertheless, I have attempted something: Let $\{p_k\}=\{2,3,5,\dots\}$ be the set of primes. We can construct: $$\left(\bigcup_{k\in \mathbb{N}}(0+p_k\mathbb{Z})\right)\cup (-1+\ell_1 \mathbb{Z})\cup (1+\ell_2\mathbb{Z})=\mathbb{Z}$$ for some appropriate non-negative integers $\ell_1,\ell_2$. We could for instance pick $\ell_1=\ell_2=5$. Suppose there is a finite sub-collection $\{0+p_{k_j}\}$, $j=1,\dots,n$ s.t.  $$\left(\bigcup_{1\leq j\leq n}(0+p_{k_j}\mathbb{Z})\right)\cup (-1+5 \mathbb{Z})\cup (1+5\mathbb{Z})$$ Now, assume $p$ is some prime s.t. $p>\max\{p_{k_1},\dots,p_{k_n}\}$, then clearly $p\notin \bigcup_{1\leq j\leq n}(0+p_{k_j}\mathbb{Z})$. But here I run into a problem. I want $p\notin(-1+5 \mathbb{Z})\cup (1+5\mathbb{Z})$. That is, I want $5\nmid p-1$ and $5\nmid p+1$. This is of course possible if $p$ is a prime with a $7$ as its last digit. However, this approach means I have to prove that there are infinitely many primes ending on a $7$, which seems like a silly thing to prove for a simple problem like this. Surely, there is a nicer way of solving this? EDIT: I am particularily interested in a solution not relying on topology, and whether a solution like my attempted solution works.	general-topology,elementary-number-theory
A.76	Cubriendo el $\mathbb{Z}$ por progresos aritméticos	Estoy resolviendo problemas de un examen anterior (en topología, pero traduje el problema a términos más algebraicos). El problema es el siguiente: Sea $a+b\mathbb{Z}=\{z\in \mathbb{Z}\mid z = a+bk \text{ para algunos }k\in \mathbb{Z}\} $ donde $a\in \mathbb{Z}$ y $b\in \mathbb{Z}-\{0\}$. Supongamos que tenemos una colección de tales conjuntos $\{a_i+b_i\mathbb{Z}\mid i \in \mathbb{N}\}$ que satisfacen: $$\bigcup_{i \in \mathbb{N}}(a_i +b_i\mathbb{Z})=\mathbb{Z}$$ Muestra si siempre es posible extraer un $I\subconjunto finito \mathbb{N}$ s.t. $$\bigcup_{i \in I}(a_i+b_i\mathbb{Z})=\mathbb{Z}$$ Desafortunadamente, parece que he olvidado gran parte de mi álgebra elemental... Sin embargo, he intentado algo : Sea $\{p_k\}=\{2,3,5,\dots\}$ el conjunto de números primos. Podemos construir: $$\left(\bigcup_{k\in \mathbb{N}}(0+p_k\mathbb{Z})\right)\cup (-1+\ell_1 \mathbb{Z})\cup (1+\ell_2\mathbb{Z})=\mathbb{Z}$$ para algunos enteros no negativos apropiados $\ell_1,\ell_2$. Podríamos, por ejemplo, elegir $\ell_1=\ell_2=5$. Supongamos que hay una subcolección finita $\{0+p_{k_j}\}$, $j=1,\dots,n$ s.t. $$\left(\bigcup_{1\leq j\leq n}(0+p_{k_j}\mathbb{Z})\right)\cup (-1+5 \mathbb{Z})\cup (1+ 5\mathbb{Z})$$ Ahora, supongamos que $p$ es un primo s.t. $p>\max\{p_{k_1},\dots,p_{k_n}\}$, entonces claramente $p\notin \bigcup_{1\leq j\leq n}(0+p_{k_j}\mathbb{ Z})$. Pero aquí me encuentro con un problema. Quiero $p\notin(-1+5 \mathbb{Z})\cup (1+5\mathbb{Z})$. Es decir, quiero $5\nmid p-1$ y $5\nmid p+1$. Por supuesto, esto es posible si $p$ es un primo con $7$ como último dígito. Sin embargo, este enfoque significa que tengo que demostrar que hay infinitos números primos que terminan en $7$, lo que parece una tontería demostrarlo para un problema simple como este. Seguramente hay una manera mejor de resolver esto. EDIT: Estoy particularmente interesado en una solución que no dependa de la topología y en si una solución como la que intenté funciona.	general-topology,elementary-number-theory
A.77	Show that the relation $(- 1) (- 1) = 1$ is a consequence of the distributive law	Show that the relation $(- 1) (- 1) =  1$ is a consequence of the distributive law.  This question is the first problem from 'Number Theory for Beginners" by Andre Weil. I cannot get the point from where to begin. I tried using $1\cdot 1 = 1$ and $ 1\cdot x = x $, but couldn't get somewhere. Can you help me just with a hint? I would be willing to work up from there.	elementary-number-theory
A.77	Muestre que la relación $(- 1) (- 1) = 1$ es una consecuencia de la ley distributiva	Muestre que la relación $(- 1) (- 1) =  1$ es una consecuencia de la ley distributiva. Esta pregunta es el primer problema de la "Teoría de números para principiantes" de Andre Weil. No puedo entender el punto de dónde empezar. Traté de usar $1\cdot 1 = 1$ y $ 1\cdot x = x $, pero no pude llegar a algún lugar. ¿Puedes ayudarme con un solo indicio? estaría dispuesto a trabajar desde allí.	elementary-number-theory
A.79	Inequality with complex exponential	Rudin in Real and Complex Analysis uses this in a proof near the beginning of chapter 9:  $\displaystyle \left \vert{ \frac {e^{-ixu}-1}{u}}\right\vert \le \vert x \vert$ for all real $u \ne 0$  Why is this true? Edit: I believe $x$ is real	inequality,exponential-function,fourier-transform
A.79	Desigualdad con exponencial complejo	Rudin en Análisis Real y Complejo utiliza esto en una prueba cerca del comienzo del capítulo 9: $\displaystyle \left \vert{ \frac {e^{-ixu}-1}{u}}\right\vert \le \vert x \vert$ para todo el real $u \ne 0$ ¿Por qué es verdad? Edit: You creo que $x$ es real	inequality,exponential-function,fourier-transform
A.80	Why does this proof that the set of all finite subsets of N is a countable set not work for the set of all subsets of N?	I found this proof in a StackExchange thread and found it pretty understandable and simple: "The other answers give some sort of formula, like you were trying to do.  But, the simplest way to see that the set of all finite subsets of $\mathbb{N}$ is countable is probably the following. If you can list out the elements of a set, with one coming first, then the next, and so on, then that shows the set is countable.  There is an easy pattern to see here.  Just start out with the least elements. $$\emptyset, \{1\}, \{2\}, \{1, 2\}, \{3\}, \{1, 3\}, \{2, 3\}, \{1, 2, 3\}, \{4\}, \ldots$$ In other words, first comes $\{1\}$, then comes $\{2\}$.  Each time you introduce a new integer, $n$, list all the subsets of $[n] = \{1, 2, \ldots, n\}$ that contain $n$ (the ones that don't contain $n$ have already showed up).  Therefore, all subsets of $[n]$ show up in the first $2^{n}$ elements of this sequence." I understand how it applies for finite subsets of N, but I cant really pinpoint of why it would not apply to a set of all subsets of N. We could continue this scheme for ever, couldnt we?  I assume that I think in a wrong way about infinity but I am not quite sure. Any help is greatly appreciated!	analysis,elementary-set-theory,proof-explanation
A.80	¿Por qué esta prueba de que el conjunto de todos los subconjuntos finitos de N es un conjunto contable no funciona para el conjunto de todos los subconjuntos de N?	Encontré esta prueba en un hilo de StackExchange y la encontré bastante comprensible y simple: "Las otras respuestas dan algún tipo de fórmula, como lo que estabas tratando de hacer. Pero, la forma más simple de ver que el conjunto de todos los subconjuntos finitos de $\mathbb{N}$ es contable es probablemente la siguiente. Si puedes enumerar los elementos de un conjunto, con uno que viene primero, luego el siguiente, y así sucesivamente, entonces eso muestra que el conjunto es contable. Hay un patrón fácil de ver aquí. Sólo comienza con el mínimo. $$\emptyset, \{1\}, \{2\}, \{1, 2\}, \{3\}, \{1, 3\}, \{2, 3\}, \{1, 2, 3\}, \{4\}, \ldots$$ En otras palabras, primero viene $\{1\}$, luego viene $\{2\}$. Cada vez que introduzcas un nuevo número entero, $n$, lista todos los subconjuntos de $[n] = \{1, 2, \ldots, n\}$ que contienen $n$ (los que no contienen $n$ ya han aparecido). Por lo tanto, todos los subconjuntos de $[n]$ aparecen en la primera secuencia de $2^{n}$ de este. Entiendo cómo podemos continuar aplicando elementos de N, pero no estoy seguro de que podemos aplicar todos los subconjuntos finitos de una manera muy precisa, pero no puedo ayudar a entender que cualquier subconjunto de N se aplica a un esquema, pero no estoy muy seguro de que podemos aplicar todos los subconjuntos de N, pero no puedo ayudar a todo esto. Podríamos continuar con este plan para siempre, ¿no? Supongo que pienso de forma equivocada sobre el infinito, pero no estoy muy seguro. ¡Cualquier ayuda es muy apreciada!	analysis,elementary-set-theory,proof-explanation
A.81	Any infinite set contains a countable subset. Why is my proof wrong? (Axiom of Choice)	Let $M$ be an infinite set.    Proposition 1:   For any $n \in \mathbb{N}$, there exists an injection from $\{1, \cdots, n\}$ to $M$.      (1) Since $M \neq \emptyset$, there exists $x \in M$. Define $f(1)$ as $f(1) := x$. $f$ is an injection from $\{1\}$ to $M$. (2) Suppose that there exists an injection $f$ from $\{1, \cdots, n\}$ to $M$. Since $M$ is an infinite set, $M - \{f(1), \cdots, f(n)\}$ is not an empty set. So, there exists $x \in M - \{f(1), \cdots, f(n)\}$. Define $g(1), \cdots, g(n+1)$ as $g(1) := f(1), \cdots, g(n):=f(n)$ and $g(n+1) := x$. Obviously, $g$ is an injection from $\{1, \cdots, n+1\}$ to $M$.   Let $n_1$ be an arbitrary natural number. If I wanna calculate $h(n_1)$, then I get an injection $g$ from $\{1, \cdots, n_1\}$ to $M$ by Proposition 1. And I return $g(n_1)$ as the value of $h(n_1)$. And I store the pairs $(1, g(1)), \cdots, (n_1, g(n_1))$ to my database.   If I wanna calculate $h(n_2)$ for $n_2 \leq n_1$, then I search my database and I get the value $g(n_2)$ from my database and I return $g(n_2)$ as the value of $h(n_2)$. If I wanna calculate $h(n_3)$ for $n_3 > n_1$, then I add the pairs $(n_1 + 1, g(n_1+1)), \cdots, (n_3, g(n_3))$ to my database by Proposition 1 and I return $g(n_3)$ as the value of $h(n_3)$.   I can calculate $h(n)$ for any $n \in \mathbb{N}$.   From above, we get an injection $h : \mathbb{N} \to M$.   Why is my proof wrong? By the way. Suppose that a man wanna know if I have an injection $h : \mathbb{N} \to M$ or not. Then how can the man know if I have  an injection $h : \mathbb{N} \to M$ or not?	elementary-set-theory,axiom-of-choice
A.81	Cualquier conjunto infinito contiene un subconjunto contable. ¿Por qué mi prueba está equivocada? (Axioma de elección)	Sea $M$ un conjunto infinito. Proposición 1: Para cualquier $n \in \mathbb{N}$, existe una inyección de $\{1, \cdots, n\}$ a $M$. (1) Dado que $M \neq \emptyset$, existe $x \in M$. Defina $f(1)$ como $f(1) := x$. $f$ es una inyección de $\{1\}$ a $M$. (2) Supongamos que existe una inyección $f$ de $\{1, \cdots, n\}$ a $M$. Dado que $M$ es un conjunto infinito, $M - \{f(1), \cdots, f(n)\}$ no es un conjunto vacío. Entonces, existe $x \in M - \{f(1), \cdots, f(n)\}$. Defina $g(1), \cdots, g(n+1)$ como $g(1) := f(1), \cdots, g(n):=f(n)$ y $g(n+ 1) :=x$. Obviamente, $g$ es una inyección de $\{1, \cdots, n+1\}$ a $M$. Sea $n_1$ un número natural arbitrario. Si quiero calcular $h(n_1)$, entonces obtengo una inyección $g$ de $\{1, \cdots, n_1\}$ a $M$ mediante la Proposición 1. Y devuelvo $g(n_1)$ como el valor de $h(n_1)$. Y almaceno los pares $(1, g(1)), \cdots, (n_1, g(n_1))$ en mi base de datos. Si quiero calcular $h(n_2)$ para $n_2 \leq n_1$, entonces busco en mi base de datos y obtengo el valor $g(n_2)$ de mi base de datos y devuelvo $g(n_2)$ como el valor de $h(n_2)$. Si quiero calcular $h(n_3)$ para $n_3 > n_1$, entonces agrego los pares $(n_1 + 1, g(n_1+1)), \cdots, (n_3, g(n_3))$ a mi base de datos por la Proposición 1 y devuelvo $g(n_3)$ como el valor de $h(n_3)$. Puedo calcular $h(n)$ para cualquier $n \in \mathbb{N}$. Desde arriba, obtenemos una inyección $h : \mathbb{N} \to M$. ¿Por qué mi prueba es incorrecta? Por cierto. Supongamos que un hombre quiere saber si tengo una inyección $h : \mathbb{N} \to M$ o no. Entonces, ¿cómo puede saber el hombre si tengo una inyección $h : \mathbb{N} \to M$ o no?	elementary-set-theory,axiom-of-choice
A.82	All definite integrals evaluate to 0 using periodic functions.	I know that my reasoning is incorrect, I just don't know where I went wrong. I did discuss this with my Maths teacher, and even she could not find what I did wrong. Let us begin by assuming a function, $f(x)$ that is continuous and has an antiderivative in the interval $[0, 2\pi]$. Let $A$ be the area under the curve for $f(x)$ in the interval $[0, 2\pi]$ $A = \displaystyle \int_{0}^{2\pi}{f(x)\space\mathrm{d}x}$ Now there must exist a function, $g(x)$ such that:   $f(x) = g(x)\cdot \cos(x)$ Substituting the value of $f(x)$: $A = \displaystyle\int_{0}^{2\pi}{g(x)\cdot \cos(x)\space\mathrm{d}x}$ Using t substitution: Let $t = \sin(x)$ Then: $\mathrm{d}t = \cos(x)\space\mathrm{d}x$ And:  $x = \arcsin(t)$ Changing the limits: $t = \sin(x)$ $0$ becomes $\sin(0) = 0$ $2\pi$ becomes $\sin(2\pi) = 0$ Substituting in the definite integral: $A = \displaystyle \int_{0}^{0}{g(\arcsin(t))\space\mathrm{d}t}$ But Definite Integral where the lower and upper bounds are the same is $0$. So: $A = 0$, which is not possible. Thanks for the help.	calculus,integration,trigonometry,definite-integrals,inverse-function
A.82	Todos los integrals definidos se evalúan a 0 utilizando funciones periódicas.	Sé que mi razonamiento es incorrecto, sólo no sé dónde me equivoqué. Yo hablé de esto con mi profesora de matemáticas, y incluso ella no pudo encontrar lo que hice mal. Comencemos asumiendo una función, $f(x)$ que es continua y tiene un antiderivado en el intervalo $[0, 2\pi]$. Que $A$ sea el área debajo de la curva de $f(x)$ en el intervalo $[0, 2\pi]$ $A = \displaystyle \int_{0}^{2\pi}{f(x)\space\mathrm{d}x}$ Ahora debe existir una función, $g(x)$ tal que: $f(x) = g(x)\cdot \cos(x)$ Substituir el valor de $f(x)$: $A = \displaystyle\int_{0}^{2\pi}{g(x)\cdot \cos(x)\space\mathrm{d}x}$ Usando la sustitución t: Que $t = \sin(x)$ Entonces: $\mathrm{d}t = \cos(x)\space\mathrm{d}x$ Y: $f(x)$0 Cambiando los límites: $t = \sin(x)$ $0$ se convierte en $\sin(0) = 0$ $2\pi$ se convierte en $\sin(2\pi) = 0$ Sustituyendo en la integral definida: $A = \displaystyle \int_{0}^{0}{g(\arcsin(t))\space\mathrm{d}t}$ Pero la integral definida donde los límites inferior y superior son iguales es $0$. Entonces: $A = 0$, lo cual no es posible. Gracias por la ayuda.	calculus,integration,trigonometry,definite-integrals,inverse-function
A.83	Is the sequence of sums of inverse of natural numbers bounded?	I'm reading through Spivak Ch.22 (Infinite Sequences) right now. He mentioned in the written portion that it's often not a trivial matter to determine the boundedness of sequences. With that in mind, he gave us a sequence to chew on before we learn more about boundedness. That sequence is: $$1, 1+\frac{1}{2}, 1+\frac{1}{2}+\frac{1}{3}, 1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}, . . .$$ I know that a sequence is bounded above if there is a number $M$ such that $a_n\leq M$ for all $n$. Any hints here?	calculus,sequences-and-series,harmonic-numbers
A.83	¿Es la secuencia de sumas invertidas de números naturales limitada?	Estoy leyendo en Spivak Ch.22 (Sequencias Infinitas) ahora mismo. Él mencionó en la parte escrita que a menudo no es una cuestión trivial determinar la limitación de secuencias. Con eso en mente, nos dio una secuencia para masticar antes de aprender más sobre la limitación. Esa secuencia es: $$1, 1+\frac{1}{2}, 1+\frac{1}{2}+\frac{1}{3}, 1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}, . . .$$ Sé que una secuencia está limitada por encima si hay un número $M$ tal que $a_n\leq M$ para todos los $n$. ¿Alguna pista aquí?	calculus,sequences-and-series,harmonic-numbers
A.84	Is the ideal generated by ${4,x}$ a principal ideal in $Z[x]$?	I've : $I=<p,x>$ is not a principal ideal in $Z[x]$ where p is prime. My question :   Is $I=<p,x>$ a principal ideal in $Z[x]$ where p is not a prime? More particularly, is the ideal generated by ${4,x}$ a principal ideal in $Z[x]$ ?	abstract-algebra,ring-theory,ideals,principal-ideal-domains
A.84	¿El ideal generado por ${4,x}$ es un ideal principal en $Z[x]$?	Tengo: $I=<p,x>$ no es un ideal principal en $Z[x]$ donde p es primo. Mi pregunta: ¿Es $I=<p,x>$ un ideal principal en $Z[x]$ donde p no es un ideal primario? ¿Particularmente, es lo ideal generado por ${4,x}$  es un ideal principal en $Z[x]$ ?	abstract-algebra,ring-theory,ideals,principal-ideal-domains
A.85	Expected number of steps for a bug to reach position $N$	A bug starts at time $0$ at position $0$. At each step, the bug either moves to the right by $1$ step $(+1)$ with probability $1/2$, or returns to the origin with probability $1/2$. What is the expected number of steps for this bug to reach position $N$? I tried to first find the possibility that this bug reaches $N$ as the number of steps goes to infinity. The recurrence equation I find is $$p_n = \frac{1}{2}p_{n-1}$$, where $p_n$ is the possibility for the bug starting at position $n$ to reach $N$. We also have the boundary condition $p_N = 1$. Then we see that $p_{N-1}=2$, and that $p_0 = 2^N$, which doesn't make sense at all because it is greater than $1$. I think I should sort out the value of probability first, and think about the number of expected steps later. I'm sure there is something wrong with the recurrence equation, but what's wrong about it?	markov-chains,random-walk
A.85	Número de pasos esperados para que un error alcance la posición $N$	Un error comienza en el momento $0$ en la posición $0$. En cada paso, el error se mueve a la derecha por el paso $1$ $(+1)$ con probabilidad $1/2$, o regresa al origen con probabilidad $1/2$. ¿Cuál es el número esperado de pasos para que este error alcance la posición $N$? Traté de encontrar primero la posibilidad de que este error alcance $N$ a medida que el número de pasos va a infinito. La ecuación de recurrencia que encuentro es $$p_n = \frac{1}{2}p_{n-1}$$, donde $p_n$ es la posibilidad de que el error que comienza en la posición $n$ alcance $N$. También tenemos la condición límite $p_N = 1$. Entonces vemos que $p_{N-1}=2$, y que $p_0 = 2^N$, que no tiene sentido en absoluto porque es mayor que $1$. Creo que debería ordenar el valor de la probabilidad primero, y pensar sobre el número de pasos esperados más tarde. Estoy seguro de que hay algo mal con la ecuación de recurrencia, pero ¿qué es?	markov-chains,random-walk
A.86	Is it true that $\sum_{k=0}^{n}k\cdot \left(\begin{array}{l}{n}\\{k}\end{array}\right)=O\left(2 ^ {n\log _{3}n}\right)?$	Problem: Is it true that $\sum_{k=0}^{n}k\cdot \left(\begin{array}{l}{n}\\{k}\end{array}\right)=O\left( 2 ^ {n\log _{3}n}\right)?$ My start of solution: $$\sum_{k=0}^{n}k\cdot \left(\begin{array}{l}{n}\\{k}\end{array}\right)\leq \sum_{k=0}^{n}k\cdot \left(\begin{array}{l}{n}\\{\lfloor \frac{n}{2}\rfloor}\end{array}\right)\leq \frac{n\cdot(n+1)}{2}\cdot \left(\begin{array}{l}{n}\\{\lfloor \frac{n}{2}\rfloor}\end{array}\right)\leq n(n+1)! \leq nn^n \leq n^{n+1}$$ I think this upper bound is way too large and I can't seem to find a solution.	combinatorics,elementary-number-theory,discrete-mathematics
A.86	¿Es cierto que $\sum_{k=0}^{n}k\cdot \left(\begin{array}{l}{n}\\{k}\end{array}\right)=O\left(2 ^ {n\log _{3}n}\right)?$	Problema: ¿Es cierto que $\sum_{k=0}^{n}k\cdot \left(\begin{array}{l}{n}\\{k}\end{array}\right)=O\left( 2 ^ {n\log _{3}n}\right)?$ Mi inicio de solución: $$\sum_{k=0}^{n}k\cdot \left(\begin{array}{l}{n}\\{k}\end{array}\right)\leq \sum_{k=0}^{n}k\cdot \left(\begin{array}{l}{n}\\{\lfloor \frac{n}{2}\rfloor}\end{array}\right)\leq \frac{n\cdot(n+1)}{2}\cdot \left(\begin{array}{l}{n}\\{\lfloor \frac{n}{2}\rfloor}\end{array}\right)\leq n(n+1)! \leq nn^n \leq n^{n+1}$$ Creo que este límite superior es demasiado grande y no puedo encontrar una solución.	combinatorics,elementary-number-theory,discrete-mathematics
A.87	Is it true that $\forall n \in \Bbb{N} : (\sum_{i=1}^{n} a_{i} ) (\sum_{i=1}^{n} \frac{1}{a_{i}} ) \ge n^2$ , if all $a_{i}$ are positive?	If  $\forall i \in \Bbb{N}: a_{i} \in \Bbb{R}^+$ , is it true that $\forall n \in \Bbb{N} : \big(\sum_{i=1}^{n}a_{i}\big) \big(\sum_{i=1}^{n}  \frac{1}{a_{i}}\big) \ge n^2$ ?  I have been able to prove that this holds for $n=1$ , $n=2$, and $n=3$ using the following lemma:  Lemma 1:  Let $a,b \in \Bbb{R}^+$. If $ab =1$ then $a+b \ge 2$  For example, the case for $n=3$ can be proven like this: Let $a,b,c \in \Bbb{R}^+$. Then we have: $(a+b+c)\big(\frac{1}{a} + \frac{1}{b} + \frac{1}{c}\big) = 1 + \frac{a}{b} + \frac{a}{c} + \frac{b}{a} + 1 + \frac{b}{c} + \frac{c}{a} + \frac{c}{b}  + 1 $ $= 3 + \big(\frac{a}{b}  + \frac{b}{a}\big) + \big(\frac{a}{c} + \frac{c}{a}\big) + \big(\frac{b}{c} + \frac{c}{b}\big) $ By lemma 1, $\big(\frac{a}{b}  + \frac{b}{a}\big) \ge 2$,  $ \big(\frac{a}{c} + \frac{c}{a}\big) \ge 2$ and  $\big(\frac{b}{c} + \frac{c}{b}\big) \ge 2$ , therefore: $3 + \big(\frac{a}{b}  + \frac{b}{a}\big) + \big(\frac{a}{c} + \frac{c}{a}\big) + \big(\frac{b}{c} + \frac{c}{b}\big) \ge 3 + 2 + 2 +2 = 9 = 3^2 \ \blacksquare $ However I'm not sure the generalized version for all natural $n$ is true. I can't come up with a counterexample and when I try to prove it by induction I get stuck. Here is my attempt: Let $P(n)::\big(\sum_{i=1}^{n}a_{i}\big) \big(\sum_{i=1}^{n} \frac{1}{a_{i}}\big) \ge n^2$ Base case: $\big(\sum_{i=1}^{1}a_{i}\big) \big(\sum_{i=1}^{1} \frac{1}{a_{i}}\big) = a_{1} \frac{1}{a_{1}} = 1 = 1^2$ , so $P(1)$ is true. Inductive hypothesis:  I assume $P(n)$ is true. Inductive step: $$\left(\sum_{i=1}^{n+1}a_{i}\right) \left(\sum_{i=1}^{n+1} \frac{1}{a_{i}}\right) = \left[\left(\sum_{i=1}^{n}a_{i}\right) + a_{n+1}\right] \left[\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) + \frac{1}{a_{n+1}}\right]$$ $$=\left(\sum_{i=1}^{n}a_{i}\right) \left[\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) + \frac{1}{a_{n+1}}\right] + a_{n+1} \left[\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) + \frac{1}{a_{n+1}}\right]$$ $$=\left(\sum_{i=1}^{n}a_{i}\right)\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) +\left(\sum_{i=1}^{n}a_{i}\right) \frac{1}{a_{n+1}} + a_{n+1} \left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) +a_{n+1} \frac{1}{a_{n+1}}$$ $$=\left(\sum_{i=1}^{n}a_{i}\right)\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) +\left(\sum_{i=1}^{n}a_{i}\right) \frac{1}{a_{n+1}} + a_{n+1} \left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) +1 $$ $$\underbrace{\ge}_{IH} n^2 + \left(\sum_{i=1}^{n}a_{i}\right) \frac{1}{a_{n+1}} + a_{n+1} \left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) + 1$$ And here I don't know what to do with the $\big( \sum_{i=1}^{n}a_{i} \big) \frac{1}{a_{n+1}} + a_{n+1} \big(\sum_{i=1}^{n} \frac{1}{a_{i}}\big)$ term. Is this inequality true? If it is, how can I prove it? If it isn't, can anyone show me a counterexample?	algebra-precalculus,inequality
A.87	¿Es cierto que $\forall n \in \Bbb{N} : (\sum_{i=1}^{n} a_{i} ) (\sum_{i=1}^{n} \frac{1}{a_{i}} ) \ge n^2$ , si todos los $a_{i}$ son positivos?	Si $\forall i \in \Bbb{N}: a_{i} \in \Bbb{R}^+$ , ¿es cierto que $\forall n \in \Bbb{N} : \big(\sum_{ i=1}^{n}a_{i}\big) \big(\sum_{i=1}^{n} \frac{1}{a_{i}}\big) \ge n^2$? He podido demostrar que esto es válido para $n=1$, $n=2$ y $n=3$ usando el siguiente lema: Lema 1: Sea $a,b \in \Bbb{R}^+ $. Si $ab =1$ entonces $a+b \ge 2$ Por ejemplo, el caso para $n=3$ se puede demostrar así: Sea $a,b,c \in \Bbb{R}^+$. Entonces tenemos: $(a+b+c)\big(\frac{1}{a} + \frac{1}{b} + \frac{1}{c}\big) = 1 + \frac{ a}{b} + \frac{a}{c} + \frac{b}{a} + 1 + \frac{b}{c} + \frac{c}{a} + \frac{c}{ b} + 1 $ $= 3 + \big(\frac{a}{b} + \frac{b}{a}\big) + \big(\frac{a}{c} + \frac{c} {a}\big) + \big(\frac{b}{c} + \frac{c}{b}\big) $ Por el lema 1, $\big(\frac{a}{b} + \frac {b}{a}\big) \ge 2$, $ \big(\frac{a}{c} + \frac{c}{a}\big) \ge 2$ y $\big(\frac{ b}{c} + \frac{c}{b}\big) \ge 2$ , por lo tanto: $3 + \big(\frac{a}{b} + \frac{b}{a}\big) + \big(\frac{a}{c} + \frac{c}{a}\big) + \big(\frac{b}{c} + \frac{c}{b}\big) \ge 3 + 2 + 2 +2 = 9 = 3^2 \ \blacksquare $ Sin embargo, no estoy seguro de que la versión generalizada para todos los $n$ naturales sea cierta. No se me ocurre un contraejemplo y cuando intento demostrarlo por inducción me quedo atascado. Aquí está mi intento: Sea $P(n)::\big(\sum_{i=1}^{n}a_{i}\big) \big(\sum_{i=1}^{n} \frac {1}{a_{i}}\big) \ge n^2$ Caso base: $\big(\sum_{i=1}^{1}a_{i}\big) \big(\sum_{i) =1}^{1} \frac{1}{a_{i}}\big) = a_{1} \frac{1}{a_{1}} = 1 = 1^2$, entonces $P(1 )$ es cierto. Hipótesis inductiva: supongo que $ P (n) $ es cierto. Paso inductivo: $$\left(\sum_{i=1}^{n+1}a_{i}\right) \left(\sum_{i=1}^{n+1} \frac{1}{ a_{i}}\right) = \left[\left(\sum_{i=1}^{n}a_{i}\right) + a_{n+1}\right] \left[\left(\ suma_{i=1}^{n} \frac{1}{a_{i}}\right) + \frac{1}{a_{n+1}}\right]$$ $$=\left(\ suma_{i=1}^{n}a_{i}\right) \left[\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) + \ frac{1}{a_{n+1}}\right] + a_{n+1} \left[\left(\sum_{i=1}^{n} \frac{1}{a_{i}} \right) + \frac{1}{a_{n+1}}\right]$$ $$=\left(\sum_{i=1}^{n}a_{i}\right)\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\ derecha) +\left(\sum_{i=1}^{n}a_{i}\right) \frac{1}{a_{n+1}} + a_{n+1} \left(\sum_{ i=1}^{n} \frac{1}{a_{i}}\right) +a_{n+1} \frac{1}{a_{n+1}}$$ $$=\left( \sum_{i=1}^{n}a_{i}\right)\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) +\left( \sum_{i=1}^{n}a_{i}\right) \frac{1}{a_{n+1}} + a_{n+1} \left(\sum_{i=1}^{ n} \frac{1}{a_{i}}\right) +1 $$ $$\underbrace{\ge}_{IH} n^2 + \left(\sum_{i=1}^{n} a_{i}\right) \frac{1}{a_{n+1}} + a_{n+1} \left(\sum_{i=1}^{n} \frac{1}{a_{i) }}\right) + 1$$ Y aquí no sé qué hacer con el $\big( \sum_{i=1}^{n}a_{i} \big) \frac{1}{a_ {n+1}} + a_{n+1} \big(\sum_{i=1}^{n} \frac{1}{a_{i}}\big)$ término. ¿Es cierta esta desigualdad? Si es así, ¿cómo puedo probarlo? Si no es así, ¿alguien puede mostrarme un contraejemplo?	algebra-precalculus,inequality
A.88	Is the polynomial $x^4+10x^2+1$ reducible over $\mathbb{Z}[x]$?	Is the polynomial  $x^4+10x^2+1$  reducible over  $\mathbb{Z}[x]$?	abstract-algebra,ring-theory,field-theory,irreducible-polynomials
A.88	¿Es el polinomio $x^4+10x^2+1$ reducible por $\mathbb{Z}[x]$?	¿Es el polinomio $x^4+10x^2+1$ reducible por $\mathbb{Z}[x]$?	abstract-algebra,ring-theory,field-theory,irreducible-polynomials
A.89	Parametrization of pythagorean-like equation	Is there any known complete parametrization of the Diophantine equation $$ A^{2} + B^{2} = C^{2} + D^{2} $$ where $A, B, C, D$ are (positive) rational numbers, or equivalently, integers?	number-theory,diophantine-equations
A.89	Parametrización de la ecuación similar a la de Pitágoras	¿Hay alguna parametrización completa conocida de la ecuación diofantina $$ A^{2} + B^{2} = C^{2} + D^{2} $$ donde $A, B, C, D$ son números racionales (positivos) o equivalentemente, números enteros?	number-theory,diophantine-equations
A.90	Question on the definition of an Inverse matrix	By definition, if $A$ is a $ n \times n $ matrix, an inverse of $A$ is an $ n \times n $ matrix $A^{-1}$ with the property that: $$ A^{-1}A=\mathbb I_n \ \ \land \ \ AA^{-1}=\mathbb I_n \ \ \ \ (1)$$  where $ \mathbb I_n $ is the $ n \times n $ identity matrix. Are there any cases where $ A^{-1}A=\mathbb I_n$ but $AA^{-1} \neq \mathbb I_n$ or the other way around (and thus making (1) a false statement) ?	linear-algebra,matrices
A.90	Pregunta sobre la definición de una matriz inversa	Por definición, si $A$ es una matriz $ n \times n $, una inversa de $A$ es una matriz $A^{-1}$ de $ n \times n $ con la propiedad de que: $$ A^{-1}A=\mathbb I_n \ \ \land \ \ AA^{-1}=\mathbb I_n \ \ \ \ (1)$$ donde $ \mathbb I_n $ es la matriz de identidad $ n \times n $. ¿Hay casos en los que $ A^{-1}A=\mathbb I_n$ pero $AA^{-1} \neq \mathbb I_n$ o viceversa (y por lo tanto haciendo (1) una declaración falsa)?	linear-algebra,matrices
A.91	Continuous function that reaches each value on its range exactly 2 times.	Is there a continuous function from $\mathbb{R}\to\mathbb{R}$ that reaches all of its possible values (each value in it's range) exactly $2$ times (for example, $x^2$ would be perfect if it wasn't for $0$..). Also, the same question but $3$ times. I'm almost certain that there aren't such functions but who knows haha maybe there are a bunch...	real-analysis,calculus
A.91	Función continua que alcanza cada valor en su rango exactamente 2 veces.	¿Hay una función continua de $\mathbb{R}\to\mathbb{R}$ que alcanza todos sus valores posibles (cada valor en su rango) exactamente $2$ veces (por ejemplo, $x^2$ sería perfecto si no fuera para $0$..). También, la misma pregunta pero $3$ veces. Estoy casi seguro de que no hay tales funciones pero quién sabe haha tal vez hay un montón...	real-analysis,calculus
A.92	Is there a principal maximal ideal in $\mathbb F_q[X,Y]$?	Given an infinite field $K$, one can prove that any maximal ideal of $K[X,Y]$ can't be principal. In fact, every non-principal prime ideal is a maximal ideal, and can be generated by two polynomials. I am wondering whether the same result holds in $\mathbb F_q[X,Y]$. Can we find a principal ideal $I = (P(X,Y))$ for some irreducible polynomial $P$ that is a maximal ideal ?    Such a polynomial $P$ must have positive degrees in both $X$ and $Y$. Indeed, given an irreducible polynomial $Q(X)$ in only one variable, the quotient  $$\mathbb F_q[X,Y]/(Q(X))\cong (\mathbb F_q[X]/(Q(X)))[Y]$$ is a ring of polynomials over a field and as such, can never be a field.   Moreover, such a polynomial $P$ must have the form  $$P(X,Y) = \sum_{i=0}^n P_i(X)Y^i$$ where $d\geq 1$, the $P_i$'s are polynomials in $X$ and $P_n$ is a nonzero polynomial that vanishes identically on $\mathbb F_q$, thus $P_n$ must be divisible by $\prod_{\alpha \in \mathbb F_q} (X-\alpha)$. Indeed, if there were some $\alpha \in \mathbb F_q$ such that $P_n(\alpha) \not = 0$, then the ideal $I:=(P,X-\alpha)$ would contain $(P)$ strictly. If $(P)$ were to be maximal, $I$ would be the whole of $\mathbb F_q[X,Y]$. Writing down the fact that $1\in I$ and evaluating $X=\alpha$ would leads us to the conclusion $n=\deg_Y(P)=0$, which is absurd.   This is all I could infer so far. With respect to the above, I tried looking at $P(X,Y) = (X^{p-1}-1)XY - 1$ or $P(X,Y) = (X^{p-1}-1)XY - X - 1$ in $\mathbb F_p[X,Y]$ for some prime number $p$ but I have trouble determining whether the quotient is a field or not.   Would somebody know the answer of the problem, and according to it, give a proof or a counter-example ? Thank you very much in advance.	polynomials,commutative-algebra,field-theory,finite-fields
A.92	¿Hay un ideal máximo principal en $\mathbb F_q[X,Y]$?	Dado un campo infinito $K$, se puede demostrar que cualquier ideal máximo de $K[X,Y]$ no puede ser principal. De hecho, todo ideal primo no principal es un ideal máximo y puede generarse mediante dos polinomios. Me pregunto si se cumple el mismo resultado en $\mathbb F_q[X,Y]$. ¿Podemos encontrar un ideal principal $I = (P(X,Y))$ para algún polinomio irreducible $P$ que sea un ideal máximo? Tal polinomio $P$ debe tener grados positivos tanto en $X$ como en $Y$. De hecho, dado un polinomio irreducible $Q(X)$ en una sola variable, el cociente $$\mathbb F_q[X,Y]/(Q(X))\cong (\mathbb F_q[X]/(Q(X) )))[Y]$$ es un anillo de polinomios sobre un campo y, como tal, nunca puede ser un campo. Además, dicho polinomio $P$ debe tener la forma $$P(X,Y) = \sum_{i=0}^n P_i(X)Y^i$$ donde $d\geq 1$, el $P_i $ son polinomios en $X$ y $P_n$ es un polinomio distinto de cero que desaparece idénticamente en $\mathbb F_q$, por lo tanto, $P_n$ debe ser divisible por $\prod_{\alpha \in \mathbb F_q} (X- \alfa)$. De hecho, si hubiera algún $\alpha \in \mathbb F_q$ tal que $P_n(\alpha) \not = 0$, entonces el $I:=(P,X-\alpha)$ ideal contendría $(P )$ estrictamente. Si $(P)$ fuera máximo, $I$ sería el conjunto de $\mathbb F_q[X,Y]$. Escribir el hecho de que $1\in I$ y evaluar $X=\alpha$ nos llevaría a la conclusión $n=\deg_Y(P)=0$, lo cual es absurdo. Esto es todo lo que pude inferir hasta ahora. Con respecto a lo anterior, intenté mirar $P(X,Y) = (X^{p-1}-1)XY - 1$ o $P(X,Y) = (X^{p-1} -1)XY - X - 1$ en $\mathbb F_p[X,Y]$ para algún número primo $p$ pero tengo problemas para determinar si el cociente es un campo o no. ¿Alguien sabría la respuesta del problema y, según ella, daría una prueba o un contraejemplo? Muchas gracias por adelantado.	polynomials,commutative-algebra,field-theory,finite-fields
A.93	Characteristic Polynomial $AB =$ characteristic polynomial $ BA$?	Let $A,B$ matrix on $\mathbb{R}$ size $nxn$. How can I prove that $det(xI - AB) = det(xI - BA)$ if $A$ and $B$ are singular matrix	linear-algebra,polynomials
A.93	Polinomio característico $AB =$ polinomio característico $ BA$?	Dejemos que la matriz $A,B$ en el tamaño $\mathbb{R}$ $nxn$. ¿Cómo puedo probar que $det(xI - AB) = det(xI - BA)$ si $A$ y $B$ son matriz singulares	linear-algebra,polynomials
A.94	Are there $2^{\aleph_{0} }$ sets of natural numbers such that each two have finite intersection	Question: Are there $2^{\aleph_{0} }$ sets of natural numbers such that each two have finite intersection. From what I've read about infinite families, I need to ignore those who have the properpty $P$. Property $P$: the family is threadless, but whenever we take finitely many sets from the family, those sets have infinite intersection. and probably find ones with property $T$. Property $T$: the family is threadless, and it is an “almost-tower”: Whenever you pick two sets in the family, one of them is almost-contained in the other. - probably meaning that we can find such an intersection which is finite, because members are contained in each other. Then I thought.. To find them, I should think of rational numbers instead of natural numbers, remembering that rational numbers can be paired up with natural ones, so solving this problem for families of rational numbers is the same as solving it for families of natural numbers. Now, I need to consider various ways of defining the real numbers. And I'm stuck.. any help is appreciated.	cardinals,rational-numbers,natural-numbers
A.94	¿Hay $2^{\aleph_{0} }$ conjuntos de números naturales tales que cada dos tienen una intersección finita?	Pregunta: ¿Hay $2^{\aleph_{0} }$ conjuntos de números naturales tales que cada uno de los dos tenga una intersección finita? De lo que he leído sobre familias infinitas, necesito ignorar a aquellos que tienen el propio $P$. Propiedad $P$: la familia es sin hilos, pero cada vez que tomamos infinitos conjuntos de la familia, esos conjuntos tienen una intersección infinita. y probablemente encontrar aquellos con la propiedad $T$. Propiedad $T$: la familia es sin hilos, y es una  casi-torre: Cada vez que elijas dos conjuntos en la familia, uno de ellos está casi contenido en el otro. - probablemente lo que significa que podemos encontrar una intersección que es finita, porque los miembros están contenidos en el otro. Entonces pensé.. para encontrarlos, debería pensar en números racionales en lugar de números naturales, recordando que los números racionales pueden ser combinados con números naturales, así que resolver este problema de las mismas familias, es casi contenido en el otro. Ahora necesito considerar varias formas de definir los números reales. Y estoy estancado... se agradece cualquier ayuda.	cardinals,rational-numbers,natural-numbers
A.95	Length of convex paths and bounding $\sin x$	The problem:  Defining $\sin x$ as the leg $b$ of a right triangle with $\angle B=x$ (in radians) and hypotenuse $1$, prove that $$\lim_{x\to 0}\frac{\sin x}x=1$$  (The motivation is to find the derivative of $\sin x$ in a elementary, "pre-Taylor" and "pre-series" context). I have seen many times a proof that it is based in the fact that the length of the arc $x$ satisfies $$\sin x<x<\tan x$$ or sometimes $$\sin x<x<\sin x+1-\cos x$$ The lower bound is clear because the $\sin x$ is the length of a straight segment and $x$ is the length of a curved segment with the same endpoints. But I find that the upper bound is based on this intuitive fact:  If $F$ and $G$ are two convex subsets of $\Bbb R^2$ and   $F\subset G$, then $|\partial F|<|\partial G|$.  ($|\partial F|$ is the length of the boundary of $F$). But I haven't ever seen a proof of that. I tried it myself but I get stuck trying to bound $$\int_s^t\sqrt{1+y'(u)^2}du$$ provided for example that $y''$ is negative and $y(s)=y(t)$. I realize that $y'$ can't be bounded (note for example $y=\sqrt{1-x^2}$, $-1\le x\le 1$). Question Is it possible to justify the derivative of $\sin x$ or the inequality about the lengths of bounds? Is there any calculus text with this approach (or similar) to define trigonometical functions?	real-analysis,derivatives,trigonometry,reference-request,arc-length
A.95	Longitud de las vías convexas y del límite $\sin x$	El problema: Definir $\sin x$ como el cateto $b$ de un triángulo rectángulo con $\angle B=x$ (en radianes) e hipotenusa $1$, demuestra que $$\lim_{x\to 0}\frac {\sin x}x=1$$ (La motivación es encontrar la derivada de $\sin x$ en un contexto elemental, "pre-Taylor" y "pre-serie"). He visto muchas veces una prueba de que se basa en el hecho de que la longitud del arco $x$ satisface $$\sin x<x<\tan x$$ o a veces $$\sin x<x<\sin x +1-\cos x$$ El límite inferior es claro porque $\sin x$ es la longitud de un segmento recto y $x$ es la longitud de un segmento curvo con los mismos puntos finales. Pero encuentro que el límite superior se basa en este hecho intuitivo: si $F$ y $G$ son dos subconjuntos convexos de $\Bbb R^2$ y $F\subset G$, entonces $|\partial F|< |\G parcial|$. ($|\partial F|$ es la longitud del límite de $F$). Pero nunca he visto una prueba de ello. Lo intenté yo mismo pero me quedo atascado al intentar enlazar $$\int_s^t\sqrt{1+y'(u)^2}du$$ siempre que, por ejemplo, $y''$ sea negativo y $y(s) =y(t)$. Me doy cuenta de que $y'$ no puede estar limitado (tenga en cuenta, por ejemplo, $y=\sqrt{1-x^2}$, $-1\le x\le 1$). Pregunta ¿Es posible justificar la derivada de $\sin x$ o la desigualdad sobre las longitudes de los límites? ¿Existe algún texto de cálculo con este enfoque (o similar) para definir funciones trigonométricas?	real-analysis,derivatives,trigonometry,reference-request,arc-length
A.96	Let $\sum_i a_i$ be a convergent sum with positive $a_i$. Does $\sum_i \frac{a_i}{a_i+a_{i+1}+a_{i+2}+\cdots}$ always diverge?	Let $\sum_i a_i$ be a convergent sum, with all $a_i$ positive.  Let $s_n=\sum_{i=n}^{\infty}a_i$.   Does $\sum_i  \frac{a_i}{s_i}$ always diverge?  I've tried a few examples such as $a_i= r^i$ (geometric series) and $a_i=1/i^2$ and it seems to always diverge.	sequences-and-series,analysis
A.96	Que $\sum_i a_i$ sea una suma convergente con el positivo $a_i$. ¿$\sum_i \frac{a_i}{a_i+a_{i+1}+a_{i+2}+\cdots}$ siempre diverge?	Que $\sum_i a_i$ sea una suma convergente, con todos los $a_i$ positivos. Que $s_n=\sum_{i=n}^{\infty}a_i$. ¿$\sum_i  \frac{a_i}{s_i}$ siempre diverge? He intentado algunos ejemplos como $a_i= r^i$ (serie geométrica) y $a_i=1/i^2$ y parece que siempre diverge.	sequences-and-series,analysis
A.97	what is the dimension of $\mathbb{R}$ at a vector space over there field $\mathbb{Q}$?	If we look at $\mathbb{C}$ as a vector space over $\mathbb{R}$ it's dimension will be $2$, because $\mathbb{C} = span\{1,i\}$.  A question I thought of is what would be the dimension of $\mathbb{R}$ as a vector space over $\mathbb{Q}$?  I feel like the answer should be infinity, because if the dimension was finite, say $n$, then for every $m := n+1$ real numbers  $x_1,...x_m$ there was a linear combination with rational coefficients that gives $0$: $\frac{a_1}{b_1}x_1+...+\frac{a_m}{b_m}x_m = 0$. Multiplying by $lcm(b_1,...,b_m)$ we get that for every $m$ real numbers there is a linear combination with natural coefficients that gives $0$. That feels false, how do you prove it?	linear-algebra,vector-spaces
A.97	¿Cuál es la dimensión de $\mathbb{R}$ en un espacio vectorial allá en el campo $\mathbb{Q}$?	Si vemos a $\mathbb{C}$ como un espacio vectorial sobre $\mathbb{R}$ su dimensión será $2$, porque $\mathbb{C} = span\{1,i\}$. Una pregunta que pensé es cuál sería la dimensión de $\mathbb{R}$ como un espacio vectorial sobre $\mathbb{Q}$? Creo que la respuesta debería ser infinita, porque si la dimensión era finita, digamos $n$, entonces para cada $m := n+1$ números reales $x_1,...x_m$ había una combinación lineal con coeficientes racionales que da $0$: $\mathbb{C}$0. Multiplicando por $\mathbb{C}$1 obtenemos que para cada $\mathbb{C}$2 números reales hay una combinación lineal con coeficientes naturales que da $0$. Eso se siente falso, ¿cómo lo pruebas?	linear-algebra,vector-spaces
A.98	If $R:S^{1}\rightarrow S^{1}$ is a irrational rotation, $\{R^{n}([x])\}$ is dense in $S^{1}$ for all points.	Let $\alpha$ a irrational number, and $R:S^{1}\rightarrow S^{1}$ the irrational rotation, i.e., $[x]\rightarrow[x+\alpha]$. I need to prove that, for all $[x]\in S^{1}$, the set $\{R^{n}([x])\}$ is dense in $S^{1}$. First, I can write $R$ by $$R(e^{2\pi ix})=e^{2\pi i (x+\alpha)}.$$ So, I can write $R^{n}(x)$, $n\in\mathbb{Z}$ by $$R^{n}(e^{2\pi i x})=e^{2\pi i(x+n\alpha)} $$ I need to prove that, for all $e^{2\pi i x}\in S^{1}$ and for all $[y]=e^{2\pi i y}\in S^{1}$, every neighborhood $V$ of $[y]$ contains a point $[z_{y}]=e^{2\pi i z}$ such that $[z_y]=R^{n}([x])$ for some $n\in\mathbb{Z}$. That is, $e^{2\pi i z}=e^{2\pi i (x+\alpha n)}\Rightarrow 2\pi iz=2\pi i(x+\alpha n)+2ik\pi\;\textrm{for some}\;k\in\mathbb{Z}\Rightarrow z=x+\alpha n+k.$ Is my way correct? If it does, how can I proceed now? If it doesn't, what I need to do? I don't think this question is duplicate. I'm showing my attempt to proof, that is different from other proofs.	general-topology,circles,rotations,irrational-numbers
A.98	Si $R:S^{1}\rightarrow S^{1}$ es una rotación irracional, $\{R^{n}([x])\}$ es denso en $S^{1}$ para todos los puntos.	Sea $\alpha$ un número irracional y $R:S^{1}\rightarrow S^{1}$ la rotación irracional, es decir, $[x]\rightarrow[x+\alpha]$. Necesito demostrar que, para todo $[x]\in S^{1}$, el conjunto $\{R^{n}([x])\}$ es denso en $S^{1}$. Primero, puedo escribir $R$ mediante $$R(e^{2\pi ix})=e^{2\pi i (x+\alpha)}.$$ Entonces, puedo escribir $R^{n} (x)$, $n\in\mathbb{Z}$ por $$R^{n}(e^{2\pi i x})=e^{2\pi i(x+n\alpha)} $ $ Necesito demostrar que, para todo $e^{2\pi i x}\in S^{1}$ y para todo $[y]=e^{2\pi i y}\in S^{1}$ , cada vecindad $V$ de $[y]$ contiene un punto $[z_{y}]=e^{2\pi i z}$ tal que $[z_y]=R^{n}([x])$ por algunos $n\in\mathbb{Z}$. Es decir, $e^{2\pi i z}=e^{2\pi i (x+\alpha n)}\Rightarrow 2\pi iz=2\pi i(x+\alpha n)+2ik\pi\; \textrm{para algunos}\;k\in\mathbb{Z}\Rightarrow z=x+\alpha n+k.$ ¿Es correcto mi método? Si es así, ¿cómo puedo proceder ahora? Si no es así, ¿qué tengo que hacer? No creo que esta pregunta esté duplicada. Estoy mostrando mi intento de prueba, que es diferente de otras pruebas.	general-topology,circles,rotations,irrational-numbers
A.99	Rationals can be the set of continuity of a function?	Most of the functions that I have seen have their discontinuities on rationals and continuities on irrationals! I am wondering if there is any exampe of some function whose continuities are rationals? Or is other words   The set of continuities of a function $f:\mathbb{R}\to\mathbb{R}$ can be $\mathbb{Q}$?	real-analysis,calculus,continuity
A.99	¿Los racionales pueden ser el conjunto de continuidad de una función?	La mayoría de las funciones que he visto tienen sus discontinuidades en racionales y continuidades en irracionales! Me pregunto si hay algún ejemplo de alguna función cuyas continuidades son racionales? ¿O en otras palabras el conjunto de continuidades de una función $f:\mathbb{R}\to\mathbb{R}$ puede ser $\mathbb{Q}$?	real-analysis,calculus,continuity
A.100	Is this set "not closed"?	Is it correct to say that this set $E=(0,1]$ where $E\subseteq R$ (Where $R$ is the set of real numbers) is not closed?	real-analysis
A.100	¿Este conjunto no está cerrado?	¿Es correcto decir que este conjunto $E=(0,1]$ donde $E\subseteq R$ (donde $R$ es el conjunto de números reales) no está cerrado?	real-analysis
A.201	Matrix over division ring having one sided inverse is invertible	I want to see if there is any elementary way to prove the following assertion about matrices over division rings (such as not using Wedderburn's theory or tensoring techniques). If an $n\times n$ matrix over a division ring has left inverse, then it also has right inverse. The assertion has elementary proof for matrices over fields, but I am considering over division rings.  One can give some hints also.	abstract-algebra,matrices,ring-theory
A.201	Matriz sobre anillo de división que tiene un lado inverso es invertible	Quiero ver si hay alguna forma elemental de probar la siguiente afirmación sobre matrices sobre anillos de división (como no usar la teoría de Wedderburn o técnicas de tensoría). Si una matriz $n\times n$ sobre un anillo de división ha dejado el inverso, entonces también tiene el inverso derecho. La afirmación tiene prueba elemental para matrices sobre campos, pero estoy considerando sobre anillos de división. Uno puede dar algunas pistas también.	abstract-algebra,matrices,ring-theory
A.202	Rings Trapped Between Fields	Some Background and Motivation: In this question, it is shown that an integral domain $D$ such that $F \subset D \subset E$, $E$ and $F$ fields with $[E:F]$ finite, is itself a field.  However, a significantly more general result holds and seems worthy, of independent address; hence, Let $F \subset E \tag 1$ be fields with $[E:F] < \infty; \tag 2$ if $R$ is a ring such that $F \subset R \subset E, \tag 3$ show that $R$ is in fact a field.	abstract-algebra,ring-theory,field-theory,extension-field
A.202	Anillos atrapados entre campos	Algunos antecedentes y motivación: En esta pregunta, se muestra que un dominio integral $D$ tal que los campos $F \subset D \subset E$, $E$ y $F$ con $[E:F]$ finito, es en sí mismo un campo. Sin embargo, un resultado significativamente más general se mantiene y parece digno, de dirección independiente; por lo tanto, que $F \subset E \tag 1$ sean campos con $[E:F] < \infty; \tag 2$ si $R$ es un anillo tal que $F \subset R \subset E, \tag 3$ muestre que $R$ es de hecho un campo.	abstract-algebra,ring-theory,field-theory,extension-field
A.203	Why does the subtraction symbol go away? $-(-x)= x$	$+(+x) = +x$ but why is $-(-x) = +x$??? What's the reason behind the rule, it's really basic and "obvious" because a no turns a no to a yes But I don't want to reason like that, lol. So how would you explain it? Do I just say on a real number line $-x$ makes a turnaround and $-(-x)$ would turn it positive again? Also if I say for example: $-x = 5$ then I do have $-(-x) = -5$ Is it correct? It wouldn't matter, right? (Btw: I don't know if the tag "elementary-number-theory" is correct) **The question is different to $(-x)*(-x)=x$	abstract-algebra
A.203	¿Por qué desaparece el símbolo de resta? $-(-x)= x$	$+(+x) = +x$ pero ¿por qué $-(-x) = +x$??? ¿Cuál es la razón detrás de la regla? Es realmente básica y "obvia" porque un no convierte un no en un sí. Pero no quiero razonar así, jajaja. Entonces, ¿cómo lo explicarías? ¿Simplemente digo en una recta numérica real que $-x$ da un giro y $-(-x)$ lo volvería positivo nuevamente? Además, si digo, por ejemplo: $-x = 5$ entonces tengo $-(-x) = -5$ ¿Es correcto? No importaría, ¿verdad? (Por cierto: no sé si la etiqueta "teoría-de-números-elemental" es correcta) **La pregunta es diferente a $(-x)*(-x)=x$	abstract-algebra
A.204	subscheme where two morphisms agree is points where they agree on residue fields	Let $X, Y, Z$ be schemes, where $X, Y$ are $Z$ schemes.  I know the definition of "the locally closed subscheme of $X$ where two $Z$-  morphisms $\pi, \pi': X\rightarrow Y$ agree" from its universal property.  Also I can define it as the fiber product of the diagonal $$\delta :  Y\rightarrow Y\times_Z Y$$ with $$(\pi, \pi'): X\rightarrow Y\times_Z Y.$$ My question:  how to prove that the underlying set of "the locally closed subscheme where the two morphisms agree" is the same as the set of points where the two morphism agree on the residue field. It is probably clear thatthe former is contained in the latter, but why is it all of them?  That is, why is a point where $\pi, \pi'$ agree on the residue field necessarily contained in "the subscheme where $\pi, \pi'$ agree"?	algebraic-geometry,schemes
A.204	subesquema donde dos morfísmos coinciden es puntos donde coinciden en campos de residuos	Que $X, Y, Z$ sea esquema, donde $X, Y$ es esquema $Z$. Conozco la definición de "el subesquema localmente cerrado de $X$ donde dos morfismos $Z$ coinciden" desde su propiedad universal. También puedo definirlo como el producto de fibra de la diagonal $$\delta :  Y\rightarrow Y\times_Z Y$$ con $$(\pi, \pi'): X\rightarrow Y\times_Z Y.$$ Mi pregunta: ¿cómo demostrar que el conjunto subyacente de "el subesquema localmente cerrado donde los dos morfismos coinciden" es el mismo que el conjunto de puntos donde los dos morfismos coinciden en el campo de residuos. Es probable que quede claro que el primero está contenido en el último, pero ¿por qué es todo? Es decir, ¿por qué un punto en el que $\pi, \pi'$ concuerdan en el campo residuo está necesariamente contenido en "el subesquema donde $\pi, \pi'$ concuerdan"?	algebraic-geometry,schemes
A.205	How can we find x for x^n = n^x	Find values of x such that $x^n=n^x$ Here, n $\in$ I.  One solution will remain x=n But i want to find if any more solutions can exist $$x^n=n^x$$	algebra-precalculus,logarithms
A.205	¿Cómo podemos encontrar x para x^n = n^x	Encuentra valores de x tales que $x^n=n^x$ Aquí, n $\in$ I. Una solución permanecerá x=n Pero quiero encontrar si hay más soluciones pueden existir $$x^n=n^x$$	algebra-precalculus,logarithms
A.206	I'm confused on the limit of $\left(1+\frac{1}{n}\right)^n$	Okay so I read Richard Rusczyk's AoPS Volume 2 Book, and I stumbled upon the part where he informs very briefly that $\lim_{n \rightarrow \infty} \left(1+\frac{1}{n}\right)^n=e$. But he doesn't really provide a rigorous proof as to why that's true (not criticizing him or anything).. It would really help if someone could provide me with the simplest proof possible as to why $\lim_{n \rightarrow \infty} \left(1+\frac{1}{n}\right)^n=e$. Thank you in advance!	algebra-precalculus,limits,exponential-function
A.206	Estoy confundido en el límite de $\left(1+\frac{1}{n}\right)^n$	Bien, entonces leí el libro AoPS Volumen 2 de Richard Rusczyk, y me encontré con la parte donde informa muy brevemente que $\lim_{n \rightarrow \infty} \left(1+\frac{1}{n}\right)^n=e$. Pero no proporciona una prueba rigurosa de por qué eso es cierto (no criticándolo o nada).. Sería realmente útil si alguien pudiera proporcionarme la prueba más simple posible de por qué $\lim_{n \rightarrow \infty} \left(1+\frac{1}{n}\right)^n=e$. ¡Gracias de antemano!	algebra-precalculus,limits,exponential-function
A.207	What function is asymptotically eqyivalent to $\sum_{k \geq 0}k!/N^k$?	I am working on this problem to find a function $f(N)$ s.t. $$ f(N) \sim \sum_{k \geq 0}\frac{k!}{N^k} $$ where $\sim$ means that given functions $f$ and $g$, we have $f \sim g \implies f = O(g) \text{ and } f=\Omega(g)$. For instance, given the right hand side of the equation above, on input $N$ we have the following (it's a divergent series) $$ f(N) \sim 1 + \frac{1}{N} + \frac{1}{2N^2} + \frac{1}{6N^3} + O\bigg(\frac{1}{N^4}\bigg) $$ The closest function I can think of are the binomials where: $$ (N \text{ choose } r) \sim \frac{N^r}{r!} $$ But it doesnt really equal the first equation above. Any help?	algorithms,asymptotics,approximation,factorial
A.207	¿Qué función es asimptóticamente equivalente a $\sum_{k \geq 0}k!/N^k$?	Estoy trabajando en este problema para encontrar una función $f(N)$ s.t. $$ f(N) \sim \sum_{k \geq 0}\frac{k!}{N^k} $$ donde $\sim$ significa que dadas funciones $f$ y $g$, tenemos $f \sim g \implies f = O(g) \text{ and } f=\Omega(g)$. Por ejemplo, dado el lado derecho de la ecuación de arriba, en la entrada $N$ tenemos lo siguiente (es una serie divergente) $$ f(N) \sim 1 + \frac{1}{N} + \frac{1}{2N^2} + \frac{1}{6N^3} + O\bigg(\frac{1}{N^4}\bigg) $$ La función más cercana que puedo pensar son los binomios donde: $$ (N \text{ choose } r) \sim \frac{N^r}{r!} $$ Pero no es realmente igual a la primera ecuación de arriba. ¿Alguna ayuda?	algorithms,asymptotics,approximation,factorial
A.208	Where does this asymptote for $H_n^{(k)}$ come from?	@Claude Leibovici's answer to this Math Stack Exchange question (it's the second answer) gives an asymptote for the generalised harmonic number $H_n^{(k)}=\sum_{i=1}^n \frac{1}{i^k}$: $$H_n^{(k)}=n^{-k}    \left(-\frac{n}{k-1}+\frac{1}{2}-\frac{k}{12    n}+O\left(\frac{1}{n^3}\right)\right)    +\zeta (k)$$ Heuristically, this is an excellent fit. But can someone please tell me if this is a published result, and more importantly how it is derived?	asymptotics,harmonic-functions,upper-lower-bounds,harmonic-numbers
A.208	¿De dónde viene este asintoto para $H_n^{(k)}$?	@Claude Leibovici la respuesta a esta pregunta de Math Stack Exchange (es la segunda respuesta) da un asintoto para el número armónico generalizado $H_n^{(k)}=\sum_{i=1}^n \frac{1}{i^k}$: $$H_n^{(k)}=n^{-k}    \left(-\frac{n}{k-1}+\frac{1}{2}-\frac{k}{12    n}+O\left(\frac{1}{n^3}\right)\right)    +\zeta (k)$$ Heurísticamente, esto es un ajuste excelente. Pero ¿me puede decir alguien si este es un resultado publicado, y más importante, cómo se deriva?	asymptotics,harmonic-functions,upper-lower-bounds,harmonic-numbers
A.209	Evaluate the definite integral: $\int_0^\infty e^{-hx^2}\;\mathrm{d}x$	where $h>0$. Could someone explain to me how to solve it? I searched the internet and I found the result is $\frac{\sqrt{\pi}}{2\sqrt{h}}$ but I couldn't undersand Gauss error function - that is involved in solving.	calculus,integration,definite-integrals
A.209	Evaluar la integral definida: $\int_0^\infty e^{-hx^2}\;\mathrm{d}x$	Donde $h>0$. ¿Puede alguien me explicar cómo resolverlo? busqué en Internet y encontré el resultado es $\frac{\sqrt{\pi}}{2\sqrt{h}}$ pero no pude entender la función de error de Gauss - que está involucrado en la resolución.	calculus,integration,definite-integrals
A.210	what's an elegant way to show that $x(1-x) \leq \frac14$?	for $x \in \mathbb{R}$, consider $f(x) = x(1-x)$, using traditional methods of finding global extremas, we can show that the derivative has a unique zero at $x= \frac12$ and $f''(\frac12) < 0$, thus $x(1-x) \leq \frac14 = f(\frac12)$ is there a more elegant way ?	calculus,inequality
A.210	¿Cuál es una forma elegante de mostrar que $x(1-x) \leq \frac14$?	Para $x \in \mathbb{R}$, considere $f(x) = x(1-x)$, utilizando métodos tradicionales de encontrar extremos globales, podemos mostrar que la derivada tiene un cero único en $x= \frac12$ y $f''(\frac12) < 0$, por lo tanto, $x(1-x) \leq \frac14 = f(\frac12)$ hay una manera más elegante ?	calculus,inequality
A.211	$\int\sqrt{x^2\sqrt{x^3\sqrt{x^4\sqrt{x^5\sqrt{x^6\sqrt{x^7\sqrt{x^8\ldots}}}}}}}\,dx$	I was attempting to solve an MIT integration bee problem (1) when I misread the integral and wrote (2) instead.  $$\int\sqrt{x\cdot \sqrt[3]{x\cdot \sqrt[4]{x\cdot\sqrt[5]{x\ldots } }}}\,dx\tag{1}$$ $$\int\sqrt{x^2\sqrt{x^3\sqrt{x^4\sqrt{x^5\sqrt{x^6\sqrt{x^7\sqrt{x^8\ldots}}}}}}}\,dx\tag{2}$$ I was able to solve (1), as the integrand simplifies to $x^{e-2}$, however, I'm struggling with solving (2).  If we rewrite the roots as powers, we get: $$\int x^\frac{2}{2}\cdot x^\frac{3}{4}\cdot x^\frac{4}{8}\cdot x^\frac{5}{16}\ldots\,dx$$ combining the powers we get: $$\int x^{\frac{2}{2}+\frac{3}{4}+\frac{4}{8}+\frac{5}{16}+\ldots}$$ the exponent is the infinite sum $$\sum^{\infty}_{n=1}\frac{n+1}{2^n}\tag{3} $$ we can split this into:  $$\sum^{\infty}_{n=1}\frac{n}{2^n}+\sum^{\infty}_{n=1}\frac{1}{2^n} $$ The right sum is well known except here the sum begins at $n=1$, meaning that the right sum evaluates to 1. Messing around with desmos, the integrand appears to be $x^3,x>0$ implying that (3) converges to 3 and the $\sum^{\infty}_{n=1}\frac{n}{2^n}$ converges to 2. Which is part I'm struggling with. Any ideas?  $$\sum^{\infty}_{n=1}\frac{n}{2^n}$$	calculus,sequences-and-series
A.211	$\int\sqrt{x^2\sqrt{x^3\sqrt{x^4\sqrt{x^5\sqrt{x^6\sqrt{x^7\sqrt{x^8\ldots}}}}}}}\,dx$	Estaba tratando de resolver un problema de abeja de integración del MIT (1) cuando mal leí la integral y escribí (2) en su lugar. $$\int\sqrt{x\cdot \sqrt[3]{x\cdot \sqrt[4]{x\cdot\sqrt[5]{x\ldots } }}}\,dx\tag{1}$$ $$\int\sqrt{x^2\sqrt{x^3\sqrt{x^4\sqrt{x^5\sqrt{x^6\sqrt{x^7\sqrt{x^8\ldots}}}}}}}\,dx\tag{2}$$ pude resolver (1), ya que el integrando se simplifica a $x^{e-2}$, sin embargo, estoy luchando con la solución (2). Si reescribemos las raíces como poderes, obtenemos: $$\int x^\frac{2}{2}\cdot x^\frac{3}{4}\cdot x^\frac{4}{8}\cdot x^\frac{5}{16}\ldots\,dx$$ combinando los poderes que obtenemos: $$\int x^{\frac{2}{2}+\frac{3}{4}+\frac{4}{8}+\frac{5}{16}+\ldots}$$ el exponente es la suma infinita $$\sum^{\infty}_{n=1}\frac{n+1}{2^n}\tag{3} $$ podemos dividir esto en: $$\sum^{\infty}_{n=1}\frac{n}{2^n}+\sum^{\infty}_{n=1}\frac{1}{2^n} $$ La suma correcta es bien conocida excepto aquí la suma comienza en $n=1$, lo que significa que la suma correcta se evalúa a 1. Testeando con desmos, el integrando parece ser $x^3,x>0$, lo que implica que (3) converge a 3 y $\sum^{\infty}_{n=1}\frac{n}{2 ^n}$ converge a 2. Que es la parte con la que estoy luchando. ¿Algunas ideas? $$\sum^{\infty}_{n=1}\frac{n}{2^n}$$	calculus,sequences-and-series
A.212	Evaluating an infinite series	I've been given the function $$f(x)=\sum_{n=0}^{\infty}(2n+1)(2x)^{2n}$$ And I have to evaluate $f(1/4)$ so find the value of $$f(1/4)=\sum_{n=0}^{\infty}\frac{2n+1}{2^{2n}}$$ I would appreciate any help with this as I am pretty lost.	calculus,sequences-and-series,power-series,taylor-expansion
A.212	Evaluación de una serie infinita	Me han dado la función $$f(x)=\sum_{n=0}^{\infty}(2n+1)(2x)^{2n}$$ y tengo que evaluar $f(1/4)$ así que encontrar el valor de $$f(1/4)=\sum_{n=0}^{\infty}\frac{2n+1}{2^{2n}}$$ agradecería cualquier ayuda con esto ya que estoy bastante perdido.	calculus,sequences-and-series,power-series,taylor-expansion
A.213	Calculate $\sum_{x=1}^{\infty} \frac{(x-1)}{2^{x}}$	I can prove it converges but I don't know at what value it converges. $\sum_{x=1}^{\infty} \frac{(x-1)}{2^{x}}$	calculus,power-series
A.213	Calculación de $\sum_{x=1}^{\infty} \frac{(x-1)}{2^{x}}$	Puedo demostrar que converge pero no sé a qué valor converge. $\sum_{x=1}^{\infty} \frac{(x-1)}{2^{x}}$	calculus,power-series
A.214	Show that $\int\limits_{-\infty}^\infty e^{-\pi x^2}dx = 1$	I want to show that $\int\limits_{-\infty}^\infty e^{-\pi x^2}dx = 1$. By definition $$\int\limits_{-\infty}^\infty e^{-\pi x^2}dx = \lim\limits_{t\to\infty}\int\limits_{-t}^t e^{-\pi x^2}dx$$ and since the integrand $e^{-\pi x^2}$ is an even function $$\int\limits_{-\infty}^\infty e^{-\pi x^2}dx = \lim\limits_{t\to\infty}\int\limits_{-t}^t e^{-\pi x^2}dx = 2\lim\limits_{t\to\infty}\int\limits_0^t e^{-\pi x^2}dx$$ i.e. we can equivalently show that $\lim\limits_{t\to\infty}\int\limits_0^t e^{-\pi x^2}dx=\frac{1}{2}$. Since the antiderivative of $e^{-x^2}$ is given by the error function we can't straightforwardly evaluate the integral, so I tried to use the power series expansion, hoping to be able to see that the resulting series will converge to $\frac{1}{2}$: $$|\int\limits_0^t e^{-\pi x^2}dx-\frac{1}{2}| = |\int\limits_0^t\sum\limits_{n=0}^\infty\frac{\pi^n\cdot x^{2n}}{n!}dx - \frac{1}{2}| = |\sum\limits_{n=0}^\infty\frac{\pi^n\cdot t^{2n+1}}{n!\cdot(n+1)}-\frac{1}{2}|$$ However, I'm in a doubt that it converges and a quick check in Wolfram Mathematica shows indeed that with $t\to\infty$ the resulting series will diverge. What am I doing wrong? Can anybody help me with a proof for this problem? Any help will be really appreciated.	calculus,integration,improper-integrals
A.214	Demuestra que $\int\limits_{-\infty}^\infty e^{-\pi x^2}dx = 1$	Quiero mostrar que $\int\limits_{-\infty}^\infty e^{-\pi x^2}dx = 1$. Por definición $$\int\limits_{-\infty}^\infty e^{-\pi x^2}dx = \lim\limits_{t\to\infty}\int\limits_{-t}^t e^{-\pi x^2}dx$$ y ya que el integrand $e^{-\pi x^2}$ es una función par $$\int\limits_{-\infty}^\infty e^{-\pi x^2}dx = \lim\limits_{t\to\infty}\int\limits_{-t}^t e^{-\pi x^2}dx = 2\lim\limits_{t\to\infty}\int\limits_0^t e^{-\pi x^2}dx$$ es decir, podemos mostrar equivalentemente que $\lim\limits_{t\to\infty}\int\limits_0^t e^{-\pi x^2}dx=\frac{1}{2}$. Dado que la antiderivada de $e^{-x^2}$ es dada por la función de error no podemos evaluar directamente la integral, así que intenté usar la expansión de la serie de potencia, con la esperanza de poder ver que la serie resultante convergerá a $\frac{1}{2}$: $$|\int\limits_0^t e^{-\pi x^2}dx-\frac{1}{2}| = |\int\limits_0^t\sum\limits_{n=0}^\infty\frac{\pi^n\cdot x^{2n}}{n!}dx - \frac{1}{2}| = |\sum\limits_{n=0}^\infty\frac{\pi^n\cdot t^{2n+1}}{n!\cdot(n+1)}-\frac{1}{2}|$$ Sin embargo, tengo dudas de que converge y una rápida comprobación en Wolfram Mathematica muestra que con $t\to\infty$ las series resultantes divergirán. ¿Qué estoy haciendo mal? ¿Puede alguien realmente ayudarme con una prueba para este problema? Cualquier ayuda será realmente apreciada.	calculus,integration,improper-integrals
A.215	Set Of Discontinuities Of A Derivative	Prove that the set of discontinuities of a derivative of an everywhere differentiable function $f(x)$ is of 1st category. Let $f'(x)$ be a derivative of an everywhere differentiable function $f(x)$. Now as the set of discontinuities of any arbitrary functions can be written as a countable union of closed sets. So let $A$ be the set of discontinuities of $f'(x)$, then we can write $$A=\bigcup_{n=1}^{\infty}A_n$$ where all the $A_n$ are closed set. Now suppose that for $n=n_0$ the set $A_{n_0}$ is not nowhere dense then there exists an open interval $(p, q)$ such that for any interval $I$ in that open interval $(p, q)$ we have $$I \cap A_{n_0} \neq \phi$$ and hence $A_{n_0}$ is dense in the open interval $(p, q)$ and as $A_{n_0}$ is closed so it contains the interval $(p, q)$ and hence $f'(x)$ is entirely discontinuous on the open interval $(p, q)$, but as the derivative of an everywhere differentiable function cannot be entirely discontinuous on an interval, so a contradiction. Is My Proof Correct??	calculus
A.215	Conjunto de discontinuidades de un derivado	Demostrar que el conjunto de discontinuidades de una derivada de una función $f(x)$ diferenciable en todas partes es de 1a categoría. Que $f'(x)$ sea un derivado de una función $f(x)$ diferenciable en todas partes. Ahora como el conjunto de discontinuidades de cualquier función arbitraria se puede escribir como una unión contable de conjuntos cerrados. Así que que $A$ sea el conjunto de discontinuidades de $f'(x)$, entonces podemos escribir $$A=\bigcup_{n=1}^{\infty}A_n$$ donde todos los $A_n$ son conjuntos cerrados. Ahora supongamos que para $n=n_0$ el conjunto $A_{n_0}$ no es en ninguna parte denso, entonces existe un intervalo abierto $(p, q)$ de tal manera que para cualquier intervalo $I$ en ese intervalo abierto $(p, q)$ tenemos $$I \cap A_{n_0} \neq \phi$$ y por lo tanto $A_{n_0}$ es denso en el intervalo abierto $(p, q)$ y como $A_{n_0}$ está cerrado, por lo que contiene el intervalo $(p, q)$ y por lo tanto $f'(x)$ es completamente discontinuo en el intervalo abierto, pero la derivada de una función no puede ser discontinuo en cualquier lugar, ¿Mi prueba es correcta?	calculus
A.216	Compute the limit $\lim\limits_{t \to + \infty} \int_0^{+ \infty} \frac{ \mathrm d x}{e^x+ \sin tx} $	I have been working on this limit for days, but I am not getting it. The question is  Compute the limit $$\lim_{t \to + \infty} \int_0^{+ \infty} \frac{ \mathrm d x}{e^x+ \sin (tx)}$$  Note that the integral is well defined and convergent for every $t >0$. Indeed the integrand function is a positive function for every $t >0$ since $$e^x + \sin tx > e^x-1 > x>0$$ And as $x \to + \infty$ the integrand function behaves like $e^{-x}$. WHAT I TRIED: I consider $t=2n \pi$ a multiple of $2 \pi$, and see what happens: $$\int_0^{+ \infty} \frac{ \mathrm d x}{e^x+ \sin (2n \pi x)} = \sum_{k=0}^\infty \int_{k /n}^{(k+1) /n} \frac{ \mathrm d x}{e^x+ \sin (2n \pi x)}$$ Making the change of variables $u = 2n \pi x$ I get \begin{align}\sum_{k=0}^\infty \frac{1}{2n \pi} \int_{2k \pi}^{(2k+2) \pi} \frac{ \mathrm d u}{e^{u/2n \pi}+ \sin (u)} &\ge \sum_{k=0}^\infty \frac{1}{2n \pi} \int_{2k \pi}^{(2k+2) \pi} \frac{ \mathrm d u}{e^{(2k+2) \pi/2n \pi}+ \sin (u)} \\&= \sum_{k=0}^\infty \frac{1}{2n \pi} \int_{2k \pi}^{(2k+2) \pi} \frac{ \mathrm d u}{e^{(k+1)/n}+ \sin (u)}\end{align} where I write the lower bound with the minimum of the function at $u=(2k+2) \pi$. Now I use the fact that the integrand function does is integrated over a period of $2 \pi$, and using the result for $C>1$ $$\int_0^{2 \pi} \frac{ \mathrm d u}{C+ \sin (u)} = \frac{2 \pi}{\sqrt{C^2-1}}$$ I get the estimate \begin{align}\sum_{k=0}^\infty \frac{1}{2n \pi} \int_{2k \pi}^{(2k+2) \pi} \frac{ \mathrm d u}{e^{(k+1)/n}+ \sin (u)} &= \sum_{k=0}^\infty \frac{1}{2n \pi} \frac{2 \pi}{\sqrt{e^{2(k+1)/n} -1 }} \\&= \frac{1}{n} \sum_{k=0}^\infty \frac{1}{\sqrt{e^{2(k+1)/n} -1 }}\end{align} Summing all up, I got that $$\int_0^{+ \infty} \frac{ \mathrm d x}{e^x+ \sin (2n \pi x)} \ge \frac{1}{n} \sum_{k=0}^\infty \frac{1}{\sqrt{e^{2(k+1)/n} -1 }}$$ As $n \to \infty$ the series converges to the Riemann integral $$\int_0^{+ \infty} \frac{\mathrm d y}{\sqrt{e^{2y}-1}} = \frac{\pi}{2}$$ Hence the limit should be a number larger than $\pi/2$, or $+ \infty$. Using WA I got for large values of $t$ that the integral is between $1$ and $2$, thus $\pi/2$ could be the actual limit.	calculus,integration,limits,definite-integrals
A.216	Calcule el límite $\lim\limits_{t \to + \infty} \int_0^{+ \infty} \frac{ \mathrm d x}{e^x+ \sin tx} $	Llevo días trabajando en este límite, pero no lo consigo. La pregunta es Calcular el límite $$\lim_{t \to + \infty} \int_0^{+ \infty} \frac{ \mathrm d x}{e^x+ \sin (tx)}$$ Tenga en cuenta que la integral es bien definido y convergente para cada $t >0$. De hecho, la función integrando es una función positiva para cada $t >0$ ya que $$e^x + \sin tx > e^x-1 > x>0$$ Y como $x \to + \infty$ la función integrando se comporta como $e^{-x}$. LO QUE INTENTÉ: Considero $t=2n \pi$ un múltiplo de $2 \pi$, y veo qué sucede: $$\int_0^{+ \infty} \frac{ \mathrm d x}{e^x+ \sin ( 2n \pi x)} = \sum_{k=0}^\infty \int_{k /n}^{(k+1) /n} \frac{ \mathrm d x}{e^x+ \sin (2n \ pi x)}$$ Haciendo el cambio de variables $u = 2n \pi x$ obtengo \begin{align}\sum_{k=0}^\infty \frac{1}{2n \pi} \int_{2k \pi}^{(2k+2) \pi} \frac{ \mathrm d u}{e^{u/2n \pi}+ \sin (u)} &\ge \sum_{k=0}^\infty \frac{1}{2n \pi} \int_{2k \pi}^{(2k+2) \pi} \frac{ \mathrm d u}{e^{(2k+2) \pi/2n \pi} + \sin (u)} \\&= \sum_{k=0}^\infty \frac{1}{2n \pi} \int_{2k \pi}^{(2k+2) \pi} \frac { \mathrm d u}{e^{(k+1)/n}+ \sin (u)}\end{align} donde escribo el límite inferior con el mínimo de la función en $u=(2k+2) \pi$. Ahora uso el hecho de que la función integrando se integra durante un período de $2 \pi$, y uso el resultado para $C>1$ $$\int_0^{2 \pi} \frac{ \mathrm d u}{C+ \sin (u)} = \frac{2 \pi}{\sqrt{C^2-1}}$$ Obtengo la estimación \begin{align}\sum_{k=0}^\infty \frac{1 }{2n \pi} \int_{2k \pi}^{(2k+2) \pi} \frac{ \mathrm d u}{e^{(k+1)/n}+ \sin (u)} & = \sum_{k=0}^\infty \frac{1}{2n \pi} \frac{2 \pi}{\sqrt{e^{2(k+1)/n} -1 }} \\ &= \frac{1}{n} \sum_{k=0}^\infty \frac{1}{\sqrt{e^{2(k+1)/n} -1 }}\end{align} Resumiendo todo, obtuve que $$\int_0^{+ \infty} \frac{ \mathrm d x}{e^x+ \sin (2n \pi x)} \ge \frac{1}{n} \sum_{ k=0}^\infty \frac{1}{\sqrt{e^{2(k+1)/n} -1 }}$$ Como $n \to \infty$ la serie converge a la integral de Riemann $ $\int_0^{+ \infty} \frac{\mathrm d y}{\sqrt{e^{2y}-1}} = \frac{\pi}{2}$$ Por lo tanto, el límite debe ser un número mayor que $\pi/2$, o $+ \infty$. Usando WA obtuve para valores grandes de $t$ que la integral está entre $1$ y $2$, por lo que $\pi/2$ podría ser el límite real.	calculus,integration,limits,definite-integrals
A.217	An easy Calculus Problem	Here's the question- Find the maximum area of an isosceles triangle inscribed in the ellipse $x^2/a^2 + y^2/b^2 = 1$. My teacher solved it by considering two arbitrary points on the ellipse to be vertices of the triangle, being $(a\cos\theta, b\sin \theta)$ and $(a\cos\theta, -b\sin \theta)$. (Let's just say $\theta$ is theta) and then proceeded with the derivative tests(which i understood) But, he didn't indicate what our $\theta$ was,and declared that these points always lie on an ellipse. Why so? And even if they do, what's the guarantee that points of such a form will be our required vertices? One more thing, I'd appreciate it if you could suggest another way of solving this problem. Thank you!	calculus,derivatives
A.217	Un problema de cálculo fácil	Aquí está la pregunta: encontrar el área máxima de un triángulo de isosceles inscrito en la elipse $x^2/a^2 + y^2/b^2 = 1$. Mi profesor lo resolvió considerando dos puntos arbitrarios en la elipse para ser vértices del triángulo, siendo $(a\cos\theta, b\sin \theta)$ y $(a\cos\theta, -b\sin \theta)$. (Sólo digamos que $\theta$ es theta) y luego continuó con las pruebas derivadas(que entendí) Pero, no indicó lo que nuestro $\theta$ era, y declaró que estos puntos siempre están en una elipse. ¿Por qué? E incluso si lo hicieran, ¿cuál es la garantía de que los puntos de esa forma serán nuestros vértices requeridos? Una cosa más, le agradecería que pudiera sugerir otra forma de resolver este problema. ¡Gracias!	calculus,derivatives
A.218	Problem involving recursion of binomial coefficients	Wrt Ramsey numbers I have the following identity given to me: $ R(m, n) \leq R(m-1, n)+R(m, n-1) $ And i have the following bases cases: $R(m,2)=m$ and $R(2,n)=n$. One has to prove that: $R(m, n) \leq\left(\begin{array}{c}{m+n-2} \\ {m-1}\end{array}\right)$ It is obvious that we have to go on splitting the two terms on the RHS into pairs of terms decrementing the indices by 1 each time. But the appearance of the combination is non-obvious.	combinatorics,combinations,binomial-coefficients,ramsey-theory
A.218	Problema que implica la recursión de los coeficientes binomial	Wrt números Ramsey tengo la siguiente identidad que me dieron: $ R(m, n) \leq R(m-1, n)+R(m, n-1) $ y tengo los siguientes casos de base: $R(m,2)=m$ y $R(2,n)=n$. Uno tiene que probar que: $R(m, n) \leq\left(\begin{array}{c}{m+n-2} \\ {m-1}\end{array}\right)$ Es obvio que tenemos que seguir dividiendo los dos términos en el RHS en pares de términos que disminuyen los índices por 1 cada vez. Pero la apariencia de la combinación no es obvia.	combinatorics,combinations,binomial-coefficients,ramsey-theory
A.219	How to prove a combinatorial identity with a combinatorial argument	I am trying to prove the following identity using a a combinatorial argument: $$\dbinom{n+r+1}{r}=\sum_{k=0}^{r}\dbinom{n+k}{k}$$	combinatorics,discrete-mathematics,combinations,combinatorial-proofs
A.219	Cómo probar una identidad combinatoria con un argumento combinatorial	Estoy tratando de probar la siguiente identidad usando un argumento combinatorial: $$\dbinom{n+r+1}{r}=\sum_{k=0}^{r}\dbinom{n+k}{k}$$	combinatorics,discrete-mathematics,combinations,combinatorial-proofs
A.220	How can i prove the identity $\sum_{k=0}^{n} \binom{x+k}{k}=\binom{x+n+1}{n}$	I'm having a difficult time understanding how to give a combinatorics proof of the identity $$\sum_{k=0}^{n} \binom{x+k}{k}=\binom{x+n+1}{n}$$	combinatorics,summation,binomial-coefficients
A.220	¿Cómo puedo probar la identidad de $\sum_{k=0}^{n} \binom{x+k}{k}=\binom{x+n+1}{n}$	Estoy teniendo dificultades para entender cómo dar una prueba combinatoria de la identidad $$\sum_{k=0}^{n} \binom{x+k}{k}=\binom{x+n+1}{n}$$	combinatorics,summation,binomial-coefficients
A.221	What's the minimum number of $2$s needed to write a positive integer?	This is just for fun and inspired by Estimating pi, using only 2s. For a positive integer $n$, let $f(n)$ denote the minimum number of $2$s needed to express $n$ using addition, subtraction, multiplication, division, and exponentiation, together with the ability to concatenate $2$s, so for example $2 \times 22^2 + \frac{222}{2}$ is a valid expression. Other variants involving different sets of allowed operations are possible, of course. This function is very far from monotonic, so to smooth it out let's also consider $$g(n) = \text{max}_{1 \le m \le n} f(m).$$ For example,  $f(1) = 2$ ($1 = \frac 22$) $f(11) = 3$ ($11 = \frac{22}{2}$)   Question: What can you say about $f(n)$ and $g(n)$? Can you give exact values for small values of $n$? Can you give (asymptotic or exact) upper bounds? Lower bounds?  As a simple example we can write any positive integer $n$ in the form $2^k + n'$ where $n' < 2^k$ ($2^k$ is just the leading digit in the binary expansion of $n$), which gives $f(n) \le f(k) + 1 + f(n')$. If we write $\ell(n) = \lfloor \log_2 n \rfloor$ then iterating this gives something like $$g(n) \le \sum_{k=1}^{\ell(n)} \left( g(k) + 1 \right).$$ This gives an upper bound growing something like $\ell(n) \ell^2(n) \ell^3(n) \dots$ which I think is pessimistic. For example, in my answer to the linked question I show that $$f(14885392687) \le 36$$ and $\ell(14885392687) = 33$ so maybe we can expect something as good as $g(n) = O(\log n)$ for an upper bound. I have no idea about a lower bound, other than to write down an upper bound on the number of possible expressions that can be made with a given number of $2$s. Edit: A related question involving $4$s and more allowed operations: How many fours are needed to represent numbers up to $N$?	combinatorics,optimization,recreational-mathematics
A.221	¿Cuál es el número mínimo de $2$s necesario para escribir un número entero positivo?	Esto es sólo por diversión y está inspirado en la Estimación de pi, usando solo 2. Para un entero positivo $n$, sea $f(n)$ el número mínimo de $2$s necesarios para expresar $n$ usando suma, resta, multiplicación, división y exponenciación, junto con la capacidad de concatenar $2$s. , entonces, por ejemplo, $2 \times 22^2 + \frac{222}{2}$ es una expresión válida. Por supuesto, son posibles otras variantes que implican diferentes conjuntos de operaciones permitidas. Esta función está muy lejos de ser monótona, así que para suavizarla consideremos también $$g(n) = \text{max}_{1 \le m \le n} f(m).$$ Por ejemplo, $f (1) = 2$ ($1 = \frac 22$) $f(11) = 3$ ($11 = \frac{22}{2}$) Pregunta: ¿Qué puedes decir sobre $f(n)$ y $ g(n)$? ¿Puedes dar valores exactos para valores pequeños de $n$? ¿Puedes dar límites superiores (asintóticos o exactos)? ¿Límites inferiores? Como ejemplo simple, podemos escribir cualquier número entero positivo $n$ en la forma $2^k + n'$ donde $n' < 2^k$ ($2^k$ es solo el dígito principal en la expansión binaria de $n$ ), lo que da $f(n) \le f(k) + 1 + f(n')$. Si escribimos $\ell(n) = \lfloor \log_2 n \rfloor$ entonces iterar esto da algo como $$g(n) \le \sum_{k=1}^{\ell(n)} \left( g(k) + 1 \right).$$ Esto da un límite superior que crece algo como $\ell(n) \ell^2(n) \ell^3(n) \dots$ que creo que es pesimista. Por ejemplo, en mi respuesta a la pregunta vinculada muestro que $$f(14885392687) \le 36$$ y $\ell(14885392687) = 33$ entonces tal vez podamos esperar algo tan bueno como $g(n) = O (\log n)$ para un límite superior. No tengo idea acerca de un límite inferior, aparte de escribir un límite superior en el número de expresiones posibles que se pueden hacer con un número determinado de $2$. Edit: Una pregunta relacionada que involucra $4$s y más operaciones permitidas: ¿Cuántos cuatros se necesitan para representar números hasta $N$?	combinatorics,optimization,recreational-mathematics
A.222	A company hires $11$ new employees, and they will be assigned to four different departments, A, B, C, D	A company hires $11$ new employees, and they will be assigned to four different departments, A, B, C, D. Each department has at least one new employee. In how many ways can these assignments be done?  I know that for each section (A,B,C,D) I should add a () and as long as every section must get a new employee we should start like this: $$(x+x^2/2!+x^3/3!+...)^4$$ then if we look  it's $(e^x-1)^4$. After this step I don't know what to do.	combinatorics,generating-functions
A.222	Una empresa contrata a $11$ nuevos empleados, y serán asignados a cuatro departamentos diferentes: A, B, C, D	Una empresa contrata a $11$ nuevos empleados, y se asignarán a cuatro departamentos diferentes, A, B, C, D. Cada departamento tiene al menos un nuevo empleado. ¿De cuántas maneras se pueden hacer estas asignaciones? Sé que para cada sección (A, B, C, D) debo agregar un () y mientras cada sección tenga que conseguir un nuevo empleado deberíamos comenzar así: $$(x+x^2/2!+x^3/3!+...)^4$$ entonces si miramos es $(e^x-1)^4$. Después de este paso no sé qué hacer.	combinatorics,generating-functions
A.223	combinatorial proof that $\sum_{i=0}^n {n+i\choose i}\frac{1}{2^i} = 2^n$	Give a combinatorial proof that $\displaystyle\sum_{i=0}^n {n+i\choose i}\frac{1}{2^i} = 2^n$.  I'm not sure if Pascal's identity is useful here. Or perhaps there is a way involving binary strings? $2^n$ is the number of binary strings of length $n$, so if there was some way to decompose these strings into disjoint sets $B_i$ with cardinality ${n+i\choose i}\frac{1}{2^i}$, a proof using that method might work. ${n+i\choose i}$ is the number of binary strings of length $n+i$ with exactly $i$ ones, since one can choose $i$ of the $n+i$ positions to be ones in ${n+i\choose i}$ ways and make the rest zeroes in one way. Then we divide by the number of binary strings of length $i$, though I'm not sure how to deduce the combinatorial significance of this. I'm most likely thinking of this the wrong way.	combinatorics,elementary-set-theory,summation,combinatorial-proofs
A.223	prueba combinatoria de que $\sum_{i=0}^n {n+i\choose i}\frac{1}{2^i} = 2^n$	Da una prueba combinatoria de que $\displaystyle\sum_{i=0}^n {n+i\choose i}\frac{1}{2^i} = 2^n$. No estoy seguro de si la identidad de Pascal es útil aquí. ¿O tal vez hay una manera de involucrar cadenas binarias? $2^n$ es el número de cadenas binarias de longitud $n$, por lo que si hubiera alguna forma de descomponer estas cadenas en conjuntos disjuntos $B_i$ con cardinalidad ${n+i\choose i}\frac{1}{2 ^i}$, una prueba que utilice ese método podría funcionar. ${n+i\choose i}$ es el número de cadenas binarias de longitud $n+i$ con exactamente $i$ unos, ya que uno puede elegir $i$ de las posiciones $n+i$ para que sean unos en $ {n+i\choose i}$ formas y haga que el resto sean ceros en una forma. Luego dividimos por el número de cadenas binarias de longitud $i$, aunque no estoy seguro de cómo deducir el significado combinatorio de esto. Lo más probable es que esté pensando en esto de manera incorrecta.	combinatorics,elementary-set-theory,summation,combinatorial-proofs
A.224	I think I found a flaw in the $\varepsilon$-$\delta$ definition of continuity.	If I have a function $f(x)$ defined as follows.  $f(x) = 1$ for all $x<1$ and $x>2$; $f(x) = 100$ for $x = 1.5$; $f(x)$ is undefined anywhere else.  According to the $\varepsilon$-$\delta$ definition of continuity, if I take $\delta$ as any positive number smaller than $0.5$, then $f(x)$ by definition is continuous at $x = 1.5$ because within the $\delta$-neighborhood there is only one point defined, but $f(x)$ is obviously not continuous at $x = 1.5$. Below is the $\varepsilon$-$\delta$ definition of continuity: The function $f(x)$ is continuous at a point $x_0$ of its domain if for every positive $\varepsilon$ we can find a positive number $\delta$ such that $$|f(x) - f(x_0)|<\varepsilon$$ for all values $x$ in the domain of $f$ for which $|x-x_0|<\delta$.	continuity
A.224	Creo que encontré un fallo en la definición de continuidad de $\varepsilon$-$\delta$.	Si tengo una función $f(x)$ definida de la siguiente manera. $f(x) = 1$ para todos los $x<1$ y $x>2$; $f(x) = 100$ para $x = 1.5$; $f(x)$ es indefinido en cualquier otro lugar. Según la definición de continuidad $\varepsilon$-$\delta$, si tomo $\delta$ como cualquier número positivo menor que $0.5$, entonces $f(x)$ por definición es continuo en $x = 1.5$ porque dentro del vecindario $\delta$-neighborhood sólo un punto definido, pero $f(x)$ obviamente no es continuo en $x = 1.5$. Debajo se encuentra la definición de continuidad $\varepsilon$-$\delta$: La función $f(x)$ es continua en un punto $x_0$ de su dominio si para cada $\varepsilon$ positivo podemos encontrar un número positivo $\delta$ tal que $$|f(x) - f(x_0)|<\varepsilon$$ para todos los valores $f(x)$2 en el dominio de $f$ para el cual $|x-x_0|<\delta$.	continuity
A.225	Finding the sum of non-unique roots of cubic equations	The real numbers $\alpha,\beta$ satisfy $$\alpha^3-3\alpha^2+5\alpha-17=0\tag{1}$$ $$\beta^3-3\beta^2+5\beta+11=0\tag{2}$$ Find $\alpha+\beta$  Are the three roots of both cubic equations unique, or is there only one root? How can you prove it? What's the best approach to this problem?  I tried using Vieta's formulas, where the sum of three roots of (1) and (2) are: $$\alpha_1+\alpha_2+\alpha_3=3$$ $$\beta_1+\beta_2+\beta_3=3$$ Summing both, $$\alpha_1+\alpha_2+\alpha_3+\beta_1+\beta_2+\beta_3=6$$ Assuming there is only one root for each of (1) and (2), we are done, but what if there isn't?	cubic-equations
A.225	Encontrar la suma de las raíces no únicas de las ecuaciones cúbicas	Los números reales $\alpha,\beta$ satisfagan $$\alpha^3-3\alpha^2+5\alpha-17=0\tag{1}$$ $$\beta^3-3\beta^2+5\beta+11=0\tag{2}$$ Encontrar $\alpha+\beta$ ¿Son las tres raíces de ambas ecuaciones cúbicas únicas, o sólo hay una raíz? ¿Cómo puede probarlo? ¿Cuál es el mejor enfoque para este problema? He intentado usar las fórmulas de Vieta, donde la suma de las tres raíces de (1) y (2) son: $$\alpha_1+\alpha_2+\alpha_3=3$$ $$\beta_1+\beta_2+\beta_3=3$$ Sumando ambas, $$\alpha_1+\alpha_2+\alpha_3+\beta_1+\beta_2+\beta_3=6$$ Suponiendo que hay sólo una raíz para cada uno de (1) y (2), estamos hechos, pero ¿qué pasa si no hay?	cubic-equations
A.226	Parametrization of the curve $x^{x^y}=y$	I was looking at the graph of the equation $x^{x^y}=y$ (Desmos link). This graph has two components that cross at the point $(1/e^e,1/e)=(e^{-e},e^{-1})$. Component 1 (as I'll call it) is the component $x^y=y$ which has the simple parametrization $$(x,y)=\left(t^{1/t},t\right),\qquad0 Component 2 is a path between the points $(0,0)$ and $(0,1)$.  Does component 2 also admit a parameterization?  To clarify: Component 2 is a path so of course it abstractly admits a parameterization, but I'm asking if there is a parametrization that we can actually write down algebraically in terms of elementary functions.  My motivation for this question is from the limiting behavior of the sequence $0,1,x,x^x,x^{x^x},x^{x^{x^x}},\ldots$, whose behavior is closely related to the solutions to $x^{x^y}=y$. In particular, if $x$ is less than $e^{-e}$ then this sequence alternates between the upper and lower parts of component 2.	curves,parametric,plane-curves,parametrization
A.226	Parametrización de la curva $x^{x^y}=y$	Estaba mirando el gráfico de la ecuación $x^{x^y}=y$ (enlace Desmos). Este gráfico tiene dos componentes que se cruzan en el punto $(1/e^e,1/e)=(e^{-e},e^{-1})$. Componente 1 (como lo llamaré) es el componente $x^y=y$ que tiene la parámetriz simple $$(x,y)=\left(t^{1/t},t\right),\qquad0 Component 2 is a path between the points $(0,0)$ y $(0,1) $.  ¿El componente 2 también admite una parametrización? Para aclarar: el componente 2 es una ruta, por lo que, por supuesto, admite de manera abstracta una parametrización, pero pregunto si hay una parametrización que realmente podamos escribir algebraicamente en términos de funciones elementales. Mi motivación para esta pregunta proviene del comportamiento limitante de la secuencia $0,1,x,x^x,x^{x^x},x^{x^x} ,\ldots$, cuyo comportamiento está estrechamente relacionado con el soluciones a $x^{x^y}=y$. En particular, si $x$ es menor que $e^{-e}$ entonces esta secuencia alterna entre las partes superiores e inferiores del componente 2.	curves,parametric,plane-curves,parametrization
A.227	$1+2+3... =-\frac{1}{12}$ - Question regarding this	So, I'm not a big expert in this subject but I know $1+2+3...=-\dfrac{1}{12}$ isn't to do with 'real' maths but it's all to do with the zeta function; however I was watching a maths video and the equation: $$ \frac{x(x+1)}{2} $$ ... is actually a perfect equation for the series $1+2+3...$ etc. where $x$ represents $n$ in a series and $y$ is the sum of the series up to $n$. So, you can conclude that: $$ \sum^{n}_{i=1}1+2+3...=\frac{x(x+1)}{2} $$ However, this is where it gets weird; as you have probably guessed, the roots of the equation is $x=0,-1$  but if I want to find the integral of the roots from $-1$ to $0$ which is under the $x$ axis, I get the following: $$ \int_{-1}^{0} \frac{x(x+1)}{2}\:dx=-\frac{1}{12} $$ So, my question is why is this the case; what connection is there between the value of the integral under the $x$ axis that this graph has compared to the summation of the series? Link to Desmos graph for more clarity	definite-integrals,summation,quadratics,zeta-functions
A.227	$1+2+3... =-\frac{1}{12}$ - Pregunta sobre este asunto	Así que, no soy un gran experto en este tema, pero sé que $1+2+3...=-\dfrac{1}{12}$ no tiene que ver con las matemáticas 'reales', pero todo tiene que ver con la función zeta; sin embargo, estaba viendo un video de matemáticas y la ecuación: $$ \frac{x(x+1)}{2} $$ ... es en realidad una ecuación perfecta para la serie $1+2+3...$ etc. donde $x$ representa $n$ en una serie y $y$ es la suma de la serie hasta $n$. Así que, se puede concluir que: $$ \sum^{n}_{i=1}1+2+3...=\frac{x(x+1)}{2} $$ Sin embargo, aquí es donde se pone raro; como probablemente has adivinado, las raíces de la ecuación es $x=0,-1$ pero si quiero encontrar la integral de las raíces de $-1$ a $0$ que está debajo del eje $x$ obtengo lo siguiente: $$ \int_{-1}^{0} \frac{x(x+1)}{2}\:dx=-\frac{1}{12} $$ Así que, la pregunta es por qué es este caso; ¿Qué conexión hay entre el valor de la integral bajo el eje $x$ que tiene esta gráfica comparado con la sumatoria de la serie? Enlace al gráfico de Desmos para mayor claridad	definite-integrals,summation,quadratics,zeta-functions
A.228	Terrible integral with parameter	Let be   $\quad f(x)=\int_{0 }^{+\infty}cos\left(\frac{t^3}{3}+xt\right) d t$  Find the integral $$F(x, y)=\int_{-\infty}^{+\infty} f(t+x) f(t+y) d t$$ I tried exploring f(x), took it in parts, got that it converges. F(x,y) is difficult to investigate, since the product of integrals is there, I don't know what to do with it.	definite-integrals,improper-integrals
A.228	Terrible integral con parámetro	Dejemos que $\quad f(x)=\int_{0 }^{+\infty}cos\left(\frac{t^3}{3}+xt\right) d t$ Encontrar la integral $$F(x, y)=\int_{-\infty}^{+\infty} f(t+x) f(t+y) d t$$ intenté explorar f(x), lo tome en partes, se que converge. F(x,y) es difícil de investigar, ya que el producto de integrales está allí, no sé qué hacer con él.	definite-integrals,improper-integrals
A.229	Finding integral of function involving fractional part of x	The integral given is: $$\int_{0}^{1}\big\lbrace\frac{1}{x}\big\rbrace \big\lbrace\frac{1}{1-x}\big\rbrace \big\lbrace1-\frac{1}{x}\big\rbrace dx$$ where  $\big\lbrace x\big\rbrace$ represents the fractional part of $x$ I first tried breaking it using a piecewise definition but I couldn't figure out how to do it as there wasn't any consistent pattern that I could spot.  I tried graphing it to get an idea but that also didn't get me anywhere. Finally, I tried using the property of definite integral that $\int_{0}^{a}f(x)dx=\int_{0}^{a}f(a-x)dx$ but the first and seccond terms remained the same and the last term changed but not it did not lead to any noticeable changes. I am stuck now. Any help would be appreciated.	definite-integrals,fractional-part
A.229	Encontrar la integral de una función que involucra una parte fraccionaria de x	La integral dada es: $$\int_{0}^{1}\big\lbrace\frac{1}{x}\big\rbrace \big\lbrace\frac{1}{1-x}\big\rbrace \big\lbrace1-\frac{1}{x}\big\rbrace dx$$ donde $\big\lbrace x\big\rbrace$ representa la parte fraccionaria de $x$ primero intenté romperlo usando una definición pieza pero no pude encontrar cómo hacerlo ya que no había ningún patrón consistente que pudiera detectar. Intenté graficarlo para obtener una idea pero eso también no me llevó a ninguna parte. Finalmente, intenté usar la propiedad de integral definida que $\int_{0}^{a}f(x)dx=\int_{0}^{a}f(a-x)dx$ pero los primeros y segundos términos permanecieron los mismos y el último término cambió pero no no llevó a ningún cambio notable. Estoy atascado ahora. Cualquier ayuda sería apreciada.	definite-integrals,fractional-part
A.230	Question about definition of Ramsey number	So I went through the definition of Ramsey number and I have a basic question. Definition: For any given number of colours, $c$, and any given integers $n_1, …, n_c$, there is a number, $R(n_1, …, n_c)$, such that if the edges of a complete graph of order $R(n_1, ..., n_c)$ are coloured with c different colours, then for some i between 1 and c, it must contain a complete subgraph of order ni whose edges are all colour i. Question: Is the multicolour Ramsey number $R(n_1,n_2,..n_c) $same as $R(n_2,n_1,..n_c) $or any other permutation of$ {n_1,n_2,..n_c}$? The definition seems to imply so, I just want to verify if I'm thinking right. I have read that $R(m,n)=R(n,m)$ but nothing about symmetricity of multicolour Ramsey numbers.	definition,ramsey-theory
A.230	Pregunta sobre la definición del número Ramsey	Así que pasé por la definición del número Ramsey y tengo una pregunta básica. definición: para cualquier número dado de colores, $c$, y cualquier número entero dado $n_1, …, n_c$, hay un número, $R(n_1, …, n_c)$, de tal manera que si los bordes de un gráfico completo de orden $R(n_1, ..., n_c)$ están coloreados con c colores diferentes, entonces para algunos i entre 1 y c, debe contener un subgrafo completo de orden ni cuyos bordes son todos colores i. pregunta: ¿Es el número multicolor Ramsey $R(n_1,n_2,..n_c) $ el mismo que $R(n_2,n_1,..n_c) $ o cualquier otra permutación de $ {n_1,n_2,..n_c}$? la definición parece implicar eso, sólo quiero verificar si estoy pensando bien. He leído que $R(m,n)=R(n,m)$ pero nada sobre la simetría de los números multicolores Ramsey.	definition,ramsey-theory
A.231	Why is 1 divided by aleph null undefined?	So recently I have been thinking about infinity, and one of the things that I thought of was if you were able to get a defined value for the reciprocal of a transfinite (cardinal) number. So, I plugged $\frac{1}{א_0}$ into WolframAlpha and it said the following: img1  Why is this the case? Shouldn't this be similar to this case? $$\lim_{x\to\infty} \frac{1}{x} =0$$ Aren't $\infty$ and $א_0$ equal to the same value in this context? What am I missing here?	definition
A.231	¿Por qué 1 dividido por aleph nulo no está definido?	Así que recientemente he estado pensando en el infinito, y una de las cosas que pensé era si se podía obtener un valor definido para la reciprocidad de un número transfinito (cardenal). Así que, conecté $\frac{1}{א_0}$ en WolframAlpha y dijo lo siguiente: img1 ¿Por qué es este el caso? ¿No debería ser similar a este caso? $$\lim_{x\to\infty} \frac{1}{x} =0$$ ¿No son $\infty$ y $א_0$ igual al mismo valor en este contexto? ¿Qué me falta aquí?	definition
A.232	Definition of Induced Matrix Norm	I'm getting confused between 2 variants of definition of induced matrix norm. Given a norm $||\cdot||$ on $\mathbb{R}^n$, the induced matrix norm is defined by $$ \left\lVert A \right\rVert = \max_{\mathbf v\not =0}\frac{\left\lVert A\mathbf v \right\rVert}{\left\lVert \mathbf v \right\rVert} \qquad for \quad A \in \mathbb{R}^{n\times n}.$$  I'm trying to deduce the second variant from this definition i.e. $$\left\lVert A \right\rVert =\max_{\Vert \mathbf w\Vert = 1}\Vert A \mathbf w\Vert.$$ Consider $\mathbf v= \frac{\left\Vert\mathbf v\right\rVert\mathbf v}{\left\Vert\mathbf v\right\rVert}=\mathbf w\left\Vert\mathbf v\right\rVert $ where $\mathbf w= \frac{\mathbf v}{||\mathbf v||}$ and $||\mathbf w||=1$. Therefore, $$ \left\lVert A \right\rVert = \max_{v\not =0}\frac{\left\lVert A\mathbf v \right\rVert}{\left\lVert \mathbf v \right\rVert}= \max_{v\not =0}\frac{\left\lVert A(\mathbf w\left\Vert\mathbf v\right\rVert )\right\rVert}{\left\lVert \mathbf v \right\rVert}=\max_{\mathbf v\not=0}\frac{||\mathbf v||}{||\mathbf v||}\Vert A \mathbf w\Vert.$$ I don't know how to continue from here on.  Also, I 'm reading textbooks where they say : $$\left\lVert A \right\rVert =\max_{\Vert \mathbf w\Vert = 1}\Vert A \mathbf w\Vert \quad for \quad \mathbf w \in \mathbb{R}^n .$$ My question is shouldn't it be  $$\left\lVert A \right\rVert =\max_{\Vert \mathbf w\Vert = 1}\Vert A \mathbf w\Vert \quad for \quad \mathbf w \in \mathbb{R}^n \setminus \{\mathbf 0\} .$$ Because $\mathbf w=\mathbf 0$ means $||\mathbf w||=0$ which is ruled out because $||\mathbf w||=1.$	definition,norm
A.232	Definición de la norma de matriz inducida	Estoy confundido entre 2 variantes de la definición de la norma de matriz inducida. Dado que una norma $||\cdot||$ en $\mathbb{R}^n$, la norma de matriz inducida se define por $$ \left\lVert A \right\rVert = \max_{\mathbf v\not =0}\frac{\left\lVert A\mathbf v \right\rVert}{\left\lVert \mathbf v \right\rVert} \qquad for \quad A \in \mathbb{R}^{n\times n}.$$ Estoy tratando de deducir la segunda variante de esta definición es decir $$\left\lVert A \right\rVert =\max_{\Vert \mathbf w\Vert = 1}\Vert A \mathbf w\Vert.$$ Considere $\mathbf v= \frac{\left\Vert\mathbf v\right\rVert\mathbf v}{\left\Vert\mathbf v\right\rVert}=\mathbf w\left\Vert\mathbf v\right\rVert $ donde $\mathbf w= \frac{\mathbf v}{||\mathbf v||}$ y $||\mathbf w||=1$. Por lo tanto, $$ \left\lVert A \right\rVert = \max_{v\not =0}\frac{\left\lVert A\mathbf v \right\rVert}{\left\lVert \mathbf v \right\rVert}= \max_{v\not =0}\frac{\left\lVert A(\mathbf w\left\Vert\mathbf v\right\rVert )\right\rVert}{\left\lVert \mathbf v \right\rVert}=\max_{\mathbf v\not=0}\frac{||\mathbf v||}{||\mathbf v||}\Vert A \mathbf w\Vert.$$ no sé cómo continuar desde aquí. También, estoy leyendo libros de texto donde dicen: $$\left\lVert A \right\rVert =\max_{\Vert \mathbf w\Vert = 1}\Vert A \mathbf w\Vert \quad for \quad \mathbf w \in \mathbb{R}^n .$$ Mi pregunta es ¿no debería ser $$\left\lVert A \right\rVert =\max_{\Vert \mathbf w\Vert = 1}\Vert A \mathbf w\Vert \quad para \quad \mathbf w \in \mathbb{R}^n \setminus \{\mathbf 0\} .$$ Porque $\mathbf w=\mathbf 0$ significa $||\mathbf w||=0$ lo cual se descarta porque $||\mathbf w||=1. $	definition,norm
A.233	Motivation for defining tangent vectors with derivations and why they should act on $f\in C^\infty(M)$	I'm revisiting the definition for tangent spaces in Lee's Introduction to Smooth Manifolds and I'm trying to convince myself why we might define tangent vectors as derivations at a point $p\in M$:  Let $M$ be a smooth manifold, and let $p\in M$. A linear map $v:C^\infty(M)\to \mathbb{R}$ is called a derivation at $p$ if \begin{align*} v(fg) = f(p)vg + g(p)vf \end{align*} for all $f,g\in C^\infty(M)$.  So far, I know that if $M=\mathbb{R}^n$, then each derivation can be given as a directional derivative in some direction in $\mathbb{R}^n$. After reading the parts on the differential and its computation in coordinates, I'm still wondering why we would be interested in defining a tangent vector as a map that acts on functions on the manifold and the benefits from acting on smooth functions. The main reason that I can think of is that the collection of derivations at a point forms a vector space, which is we what want for a tangent space. I have also looked at the approach of defining tangent vectors with equivalence classes of curves, but it seems that there's also an action on $f\in C^\infty(M)$ going on; we call curves $\gamma:J\to M$ the tangent vectors, and they have a directional-derivative-like operators that act on $f\in C^\infty(M)$ by \begin{align*} \left.\frac{d}{dt}(f\circ \gamma)(t)\right|_{t=0}. \end{align*} This seems really similar to how a vector in $\mathbb{R}^n$ defines its own directional derivative, but again, I'm not sure why the action on $f\in C^\infty(M)$ would be useful/significant.	differential-geometry,differential-topology,smooth-manifolds,tangent-spaces
A.233	Motivación para definir vectores tangentes con derivadas y por qué deberían actuar sobre $f\in C^\infty(M)$	Estoy revisando la definición de espacios tangentes en la Introducción de Lee a Maniflitos Limos y estoy tratando de convencerme de por qué podríamos definir vectores tangentes como derivadas en un punto $p\in M$: Dejemos que $M$ sea un varietal liso, y dejemos que $p\in M$. Un mapa lineal $v:C^\infty(M)\to \mathbb{R}$ se llama una derivación en $p$ si \begin{align*} v(fg) = f(p) vg + g) p) vf \end{align*} para todos los $f,g\in C^\infty(M)$. Hasta ahora, sé que si $M=\mathbb{R}^n$, entonces cada derivado tangente puede ser dado como una derivación direccional en algún punto $\mathbb{R}^n$. Después de leer las partes sobre la colección tangente y su cálculo en coordenadas tangentes, estoy preguntandome por qué todavía estaríamos interesados en definir una derivación tangente como una colección tangente que actúa en la dirección de las funciones tangentes y que actúa en la dirección de la función tangente. La razón principal que se me ocurre es que la colección de derivaciones en un punto forma un espacio vectorial, que es lo que queremos para un espacio tangente. También he analizado el enfoque de definir vectores tangentes con clases de equivalencia de curvas, pero parece que también hay una acción en $f\in C^\infty(M)$; A las curvas las llamamos $\gamma:J\to M$ los vectores tangentes, y tienen operadores tipo derivada direccional que actúan sobre $f\in C^\infty(M)$ por \begin{align*} \left .\frac{d}{dt}(f\circ \gamma)(t)\right|_{t=0}. \end{align*} Esto parece muy similar a cómo un vector en $\mathbb{R}^n$ define su propia derivada direccional, pero nuevamente, no estoy seguro de por qué la acción en $f\in C^\infty (M)$ sería útil/significativo.	differential-geometry,differential-topology,smooth-manifolds,tangent-spaces
A.234	Sards theorem for polynomial	I'm having some struggles with an aspect about something apparently trivial about Sard's theorem, but couldn't find anything online. Let $f$ be a polynomial. According to Sard's theorem, the image $f(Z)$ of the set of critical values  $$Z = \{a \in X : f'(a) = 0\}$$ has measure zero. What if I want to show that the set $Z$ itself has measure zero in the domain of $f$? I feel like it's so simple but i just can't get behind it.	differential-topology
A.234	Teorema de Sards para el polinomio	Estoy teniendo algunas dificultades con un aspecto sobre algo aparentemente trivial sobre el teorema de Sard, pero no pude encontrar nada en línea. Dejamos que $f$ ser un polinomio. Según el teorema de Sard, la imagen $f(Z)$ del conjunto de valores críticos $$Z = \{a \in X : f'(a) = 0\}$$ tiene la medida de cero. ¿Qué pasa si quiero mostrar que el conjunto $Z$ en sí tiene la medida de cero en el dominio de $f$? Siento que es tan simple pero no puedo entenderlo.	differential-topology
A.235	Method for solving Diophantine equation $ax^2 + bx + c = y^2$	How do I solve the Diophantine equation $ax^2 + bx + c = y^2$? The approach I have so far is to use the transformation $X = 2ax + b$ and $Y = 2y$. Applying this, we get, $X^2 - dY^2 = n$, where $n = b^2 - 4ac$ and $d = a$. $X^2 - dY^2 = n$ is a Pell equation. Questions:  Is there any other method? What is the complexity of the algorithm for finding the solution to the Pell equation?	diophantine-equations,computational-complexity,pell-type-equations
A.235	Método para resolver la ecuación diofantina $ax^2 + bx + c = y^2$	¿Cómo resolver la ecuación de Diofantino $ax^2 + bx + c = y^2$? El enfoque que tengo hasta ahora es usar la transformación $X = 2ax + b$ y $Y = 2y$. Aplicando esto, obtenemos, $X^2 - dY^2 = n$, donde $n = b^2 - 4ac$ y $d = a$. $X^2 - dY^2 = n$ es una ecuación de Pell. Preguntas: ¿Existe algún otro método? ¿Cuál es la complejidad del algoritmo para encontrar la solución a la ecuación de Pell?	diophantine-equations,computational-complexity,pell-type-equations
A.236	Normalizing constant in Dirichlet distribution	According to references (e.g. Wikipedia and elsewhere), the Dirichlet distribution, parametrized by $\boldsymbol{\alpha}=(\alpha_1,\ldots,\alpha_K)$, is $$ D(x_1, \ldots, x_K) = \frac{1}{\mathrm{B}(\boldsymbol\alpha)} \prod_{i=1}^K x_i^{\alpha_i - 1} $$ where $$ \mathrm{B}(\boldsymbol\alpha) = \frac{\prod_{i=1}^K \Gamma(\alpha_i)}{\Gamma\left(\sum_{i=1}^K \alpha_i\right)}. $$ So, if $K = 2$ and $\alpha_1 = \alpha_2 = 1$ then this gives $ D(x_1, x_2) = 1/\mathrm{B(\boldsymbol\alpha)} $ where $$ \mathrm{B}(\boldsymbol\alpha) = \Gamma(1)^2 / \Gamma(2) = 1 $$ so, $D(x_1, x_2) = 1$ for all $x_1, x_2$.  However, $D(x_1, x_2)$ is defined on the standard $1$-simplex defined in $R^2$ by $x_i \ge 0$ and $x_1 + x_2 = 1$.  This is the span (or affine hull) of the two points $(0, 1)$ and $(1, 0)$.  Since this is a line segment of length $\sqrt{2}$, the integral of the Dirichlet distribution over this simplex is $\sqrt{2}$, not $1$ as expected.  What am I missing here? The same problem comes in higher dimensions.  For instance, for $K=3$, the simplex is a triangle with side $\sqrt{2}$, but the normalization constant becomes $B(\boldsymbol\alpha) = 1/\Gamma(3) = 1/2$, which is not the area of this triangle. What is wrong here?	dirichlet-series
A.236	Normalizando la constante en la distribución de Dirichlet	Según referencias (por ejemplo Wikipedia y en otros lugares), la distribución de Dirichlet, parametrizada por $\boldsymbol{\alpha}=(\alpha_1,\ldots,\alpha_K)$, es $$ D(x_1, \ldots, x_K) = \frac{1}{\mathrm{B}(\boldsymbol\alpha)} \prod_{i=1}^K x_i^{\alpha_i - 1} $$ donde $$ \mathrm{B}(\boldsymbol\alpha) = \frac{\prod_{i=1}^K \Gamma(\alpha_i)}{\Gamma\left(\sum_{i=1}^K \alpha_i\right)}. $$ Así que, si $K = 2$ y $\alpha_1 = \alpha_2 = 1$ entonces esto da $ D(x_1, x_2) = 1/\mathrm{B(\boldsymbol\alpha)} $ donde $$ \mathrm{B}(\boldsymbol\alpha) = \Gamma(1)^2 / \Gamma(2) = 1 $$ así, $D(x_1, x_2) = 1$ para todos $x_1, x_2$. Sin embargo, $D(x_1, x_2)$ se define en el estándar $1$ -simplex definido en $R^2$ por $x_i \ge 0$ y $x_1 + x_2 = 1$. Este es el tramo (o casco afín) de los dos puntos $(0, 1)$ y $(1, 0)$. Dado que este es un segmento de línea de longitud $\sqrt{2}$, la integral de la distribución de Dirichlet sobre este simplex es $\sqrt{2}$, no $1$ como se esperaba. ¿Que me estoy perdiendo aqui? El mismo problema se presenta en dimensiones superiores. Por ejemplo, para $K=3$, el simplex es un triángulo con lado $\sqrt{2}$, pero la constante de normalización se convierte en $B(\boldsymbol\alpha) = 1/\Gamma(3) = 1/2 $, que no es el área de este triángulo. ¿Que esta mal aquí?	dirichlet-series
A.237	Rewrite the propositions without implication	I want to rewrite the following types of propositions without the simple or double implication:  $p \land \lnot q \to r$ $p \land \lnot q \to r \land q$ $(p \to r) \leftrightarrow (q \to r)$  So we have to write these propositions without any implication, for example the first proposition like $p \land \lnot q \land r$ or is something else meant?	discrete-mathematics,logic
A.237	Reescribir las propuestas sin implicaciones	Quiero reescribir los siguientes tipos de proposiciones sin la implicación simple o doble: $p \land \lnot q \to r$ $p \land \lnot q \to r \land q$ $(p \to r) \leftrightarrow (q \to r)$ Así que tenemos que escribir estas proposiciones sin ninguna implicación, por ejemplo la primera proposición como $p \land \lnot q \land r$ o se quiere decir algo más?	discrete-mathematics,logic
A.238	Definition of Equivalence Relation	I was going through the text "Discrete Mathematics and its Application" by Kenneth Rosen (5th Edition) where I am across the definition of equivalence relation and felt that it is one sided.  Definition: A relation on a set A is called an equivalence relation if it is reflexive, symmetric, and transitive.  Now let us analyze the situation of what equivalence is meant to us intuitively. Let there be a binary relation $R$ defined on a set $A$. Now we suppose that $R$ be reflexive, symmetric and transitive. So we have for $a,b,c \in A$   $a R a$ (by the reflexive property of R) if $a R b$ then $b R a$  (by the symmetric property of R) if $a R b$ and $b R c$ then $aRc$ (by the transitive property of R)  Intuitively we can satisfy ourselves with the fact that the above are the necessary conditions for $R$ to be equivalent. So "if $R$ is reflexive, symmetric and transitive, then $R$ is an equivalence relation" Now working our intuition for equivalence relation $\sim$ we note the following. Let $\sim$ be an equivalence relation on a set A, then for $a,b,c \in A$ we have,  $a \sim a$ (by the intuitive knowledge of what $\sim$ means) if $a\sim b$ then $b \sim a$ (by the intuitive knowledge of what $\sim$ means) if $a\sim b$ and $b \sim c$ then $a\sim c$ (by the intuitive knowledge of what $\sim$ means)  Now we see that (1) implies $\sim$ is reflexive, (2) implies that $\sim$ is symmetric and (3) implies that $\sim$ is transitive. So we have "if $\sim$ is an equivalent relation then $\sim$ is reflexive, symmetric and transitive" From the two intuitive implications we can conclude that A relation on a set A is called an equivalence relation if and only if it is reflexive, symmetric, and transitive. and not what the book says. This definition makes quite sense unlike the book definition which says that if $R$ fails to be either reflexive or symmetric or transitive then $R$ may or may not be an equivalence relation, which after all gives a weird feeling. Correct me if my logic is wrong.	discrete-mathematics,elementary-set-theory,proof-writing,definition,relations
A.238	Definición de la relación de equivalencia	Estaba leyendo el texto "Matemáticas discretas y su aplicación" de Kenneth Rosen (quinta edición), donde encontré la definición de relación de equivalencia y sentí que es unilateral. Definición: Una relación en un conjunto A se llama relación de equivalencia si es reflexiva, simétrica y transitiva. Analicemos ahora la situación de lo que para nosotros significa intuitivamente equivalencia. Sea una relación binaria $R$ definida sobre un conjunto $A$. Ahora suponemos que $R$ es reflexivo, simétrico y transitivo. Entonces tenemos para $a,b,c \in A$ $a R a$ (por la propiedad reflexiva de R) si $a R b$ entonces $b R a$ (por la propiedad simétrica de R) si $a R b$ y $b R c$ luego $aRc$ (por la propiedad transitiva de R) Intuitivamente podemos satisfacernos con el hecho de que las anteriores son las condiciones necesarias para que $R$ sea equivalente. Entonces "si $R$ es reflexivo, simétrico y transitivo, entonces $R$ es una relación de equivalencia" Ahora, trabajando nuestra intuición para la relación de equivalencia $\sim$, observamos lo siguiente. Sea $\sim$ una relación de equivalencia en un conjunto A, entonces para $a,b,c \in A$ tenemos, $a \sim a$ (por el conocimiento intuitivo de lo que significa $\sim$) si $ a\sim b$ entonces $b \sim a$ (por el conocimiento intuitivo de lo que significa $\sim$) si $a\sim b$ y $b \sim c$ entonces $a\sim c$ (por el conocimiento intuitivo conocimiento de lo que significa $\sim$) Ahora vemos que (1) implica que $\sim$ es reflexivo, (2) implica que $\sim$ es simétrico y (3) implica que $\sim$ es transitivo. Entonces tenemos "si $\sim$ es una relación equivalente entonces $\sim$ es reflexiva, simétrica y transitiva". De las dos implicaciones intuitivas podemos concluir que una relación en un conjunto A se llama relación de equivalencia si y sólo si es reflexivo, simétrico y transitivo. y no lo que dice el libro. Esta definición tiene bastante sentido a diferencia de la definición del libro que dice que si $R$ no es reflexivo, simétrico o transitivo, entonces $R$ puede o no ser una relación de equivalencia, lo que después de todo da una sensación extraña. Corrígeme si mi lógica es incorrecta.	discrete-mathematics,elementary-set-theory,proof-writing,definition,relations
A.239	Fake proof, symmetric and transitive relation is already reflexive	Let $R$ be a symmetric, transitive relation. If $(x, y) \in R$ then the symmetric property implies that $(y, x) \in R$. Using the the transitive property upon $(x, y)$ and $(y, x)$ we can conclude $(x, x) \in R$. Is this fair logic or is it flawed?	discrete-mathematics,relations,fake-proofs
A.239	La prueba es falsa, la relación simétrica y transitiva ya es reflexiva	Sea $R$ unauna relación simétrica, transitiva. si $(x, y) \in R$ entonces la propiedad simétrica implica que $(y, x) \in R$. usando la propiedad transitiva sobre $(x, y)$ y $(y, x)$ podemos concluir $(x, x) \in R$. ¿Es esta lógica justa o es defectuosa?	discrete-mathematics,relations,fake-proofs
A.240	What do the constants "4" and "2" in Bhaskara mean and where did they come from?	In bhaskar, the way to get the result, is to get the $\Delta  = b^2 – 4ac$, and then the $X = (–b \pm \sqrt\Delta)/2a)$. But from where come these constants?	education
A.240	¿Qué significan las constantes "4" y "2" en Bhaskara y de dónde provienen?	En bhaskar, la forma de obtener el resultado es obtener el $\Delta  = b^2 – 4ac$, y luego el $X = (–b \pm \sqrt\Delta)/2a)$. Pero ¿de dónde vienen estas constantes?	education
A.241	Divisibility of $n^3 +6n^2-7n$	Let $n = 2, 3, 4, ...$ be an integer. Show that $n^3 +6n^2-7n$ is divisible by $6$.  How should one approach this? Using modular arithmetic or some other approach? ¿Usando aritmética modular o algún otro enfoque?	elementary-number-theory
A.241	Divisibilidad de $n^3 +6n^2-7n$	Si los número $n = 2, 3, 4, ...$ es un número entero, muestra que el número $n^3 +6n^2-7n$ es divisible por el número $6$. ¿Cómo se debe abordar esto?	elementary-number-theory
A.242	Show that for all prime numbers $p$ greater than $3$, $24$ divides $p^2-1$ evenly.	Show that for all prime numbers $p$ greater than $3$, $24$ divides $p^2-1$ evenly.  Since $(p+1)(p-1) = p^2-1$ we have that $\frac{(p+1)(p-1)}{24}=k$, where $k \in \Bbb Z.$ Now since $24 = 2^3 \cdot 3$ and the numerator contains always at least one even factor(?) we have that $24=2^3\cdot3\vert(p+1)(p-1).$ Is my reasoning here correct or am I missing something here?	elementary-number-theory
A.242	Muestre que para todos los números primos $p$ mayor que $3$, $24$ divide $p^2-1$ de manera uniforme.	Muestre que para todos los números primos $p$ mayor que $3$, $24$ divide $p^2-1$ de manera uniforme. Dado que $(p+1)(p-1) = p^2-1$ tenemos que $\frac{(p+1)(p-1)}{24}=k$, donde $k \in \ Bbb Z.$ Ahora bien, dado que $24 = 2^3 \cdot 3$ y el numerador siempre contiene al menos un factor par (?), tenemos que $24=2^3\cdot3\vert(p+1)(p-1) .$ ¿Mi razonamiento aquí es correcto o me falta algo?	elementary-number-theory
A.243	If $2^{2k}-x^2\bigm|2^{2k}-1$ then $x=1$	This is the $y=2^k$ case of this question. Suppose that $k\geq1$ and $0 and $2^{2k}-x^2\bigm|2^{2k}-1$. ¿Es necesariamente el caso de que $x=1$? De manera equivalente: supongamos que hay dos divisores positivos de $2^{2k}-1$ cuyo promedio es $2^k$. ¿Es necesariamente el caso de que estos dos divisores sean $2^k-1$ y $2^k+1$?	elementary-number-theory,divisibility
A.243	Si $2^{2k}-x^2\bigm|2^{2k}-1$ entonces $x=1$	Este es el caso $y=2^k$ de esta pregunta. Supongamos que $k\geq1$ y $0 y $2^{2k}-x^2\bigm d2^{2k}-1$. Is it necessarily the case that $x=1$? Equivalently: Suppose that there are two positive divisors of $2^{2k}-1$ which average to $2^k$. Is it necessarily the case that these two divisors are $2^k-1$ and $2^k+1$?	elementary-number-theory,divisibility
A.244	Compute the Cardinality of a quotient set	Let $R$ be a an Equivalence relation on $\Bbb{R}$ defined by: $$aRb \Leftrightarrow (a-b)\in \Bbb{Z}$$ What is the cardinality of $|\Bbb{R}/R|$?   where $\Bbb{R}/R$ is the quotient set of $\Bbb{R}$ under $R$.	elementary-set-theory
A.244	Calcule la cardinalidad de un conjunto de cuotientes	Que $R$ sea una relación de equivalencia en $\Bbb{R}$ definida por: $$aRb \Leftrightarrow (a-b)\in \Bbb{Z}$$ ¿Cuál es la cardinalidad de $|\Bbb{R}/R|$? donde $\Bbb{R}/R$ es el conjunto de cuotientes de $\Bbb{R}$ bajo $R$.	elementary-set-theory
A.245	Is there a known set of closed form solutions to the functional equation f(f(z)) = sin z?	That is $$ f(f(z)) = \sin z $$ where $$ z \in \mathbb{Z} $$	functional-equations
A.245	¿Existe un conjunto conocido de soluciones de forma cerrada a la ecuación funcional f ((f ((z)) = sin z?	Eso es $$ f(f(z)) = \sin z $$ donde $$ z \in \mathbb{Z} $$	functional-equations
A.246	Finding $n$ such that in a regular $n$-gon $A_1A_2\ldots A_n$ we have $\frac1{A_1A_2}=\frac1{A_1A_3}+\frac1{A_1A_4}$	INMO '92 Question 9:  Find $n$ such that in a regular $n$-gon $A_1A_2 ...A_n$ we have $$\frac{1}{A_1A_2}=\frac{1}{A_1A_3}+\frac{1}{A_1A_4}$$  I tried the following Assume it is inscribed in a circle. Then length of chord is $2\sin(\theta)$ where $\theta$ is half the angle subtended at the center between consecutive points. So, $\theta=\frac{180^\circ}{n}$. Then we get $$\csc(\theta)=\csc(2\theta)+\csc(3\theta)$$ Not sure quite how to proceed from there- using double and triple angle formulae doesn't seem to work	geometry,trigonometry,contest-math,polygons
A.246	Encontrando $n$ tal que en un regular $n$-gon $A_1A_2\ldots A_n$ tenemos $\frac1{A_1A_2}=\frac1{A_1A_3}+\frac1{A_1A_4}$	INMO '92 Pregunta 9: Encontrar $n$ tal que en un regular $n$-gon $A_1A_2 ...A_n$ tenemos $$\frac{1}{A_1A_2}=\frac{1}{A_1A_3}+\frac{1}{A_1A_4}$$ He intentado lo siguiente Supongamos que está inscrito en un círculo. Entonces la longitud del acorde es $2\sin(\theta)$ donde $\theta$ es la mitad del ángulo subtendido en el centro entre puntos consecutivos. Así que, $\theta=\frac{180^\circ}{n}$. Entonces obtenemos $$\csc(\theta)=\csc(2\theta)+\csc(3\theta)$$ No estamos seguros de cómo proceder de allí- usando fórmulas de ángulo doble y triple no parece funcionar	geometry,trigonometry,contest-math,polygons
A.247	Finding the endpoints of the maximal arc of circle $x^2+(y-8)^2=25$ visible from $(0,-5)$.	The equation of circle $C$ is $x^2 + (y − 8)^2 = 25$. The eye is located at $E = (0, −5)$. The maximal circular arc visible to the eye is $AB$, which is then being projected on to the one-dimensional "screen" as $A'B'$. What are the co-ordinates of points $A$ and $B$?  I came this far: point $P$ on circle $C$ has the coordinates $x = 5 \cos\theta$, $y = 8 + 5 \sin \theta$. Now I should use this to find points $A$ and $B$, but I don't know how to proceed.	geometry,analytic-geometry
A.247	Encontrar los puntos finales del arco máximo del círculo $x^2+(y-8)^2=25$ visible desde el $(0,-5)$.	La ecuación del círculo $C$ es $x^2 + (y − 8)^2 = 25$. El ojo se encuentra en $E = (0, −5)$. El arco circular máximo visible al ojo es $AB$, que luego se proyecta en la pantalla unidimensional como $A'B'$. ¿Cuáles son las coordenadas de los puntos $A$ y $B$? Llegué hasta aquí: el punto $P$ en el círculo $C$ tiene las coordenadas $x = 5 \cos\theta$, $y = 8 + 5 \sin \theta$. Ahora debería usar esto para encontrar los puntos $A$ y $B$, pero no sé cómo proceder.	geometry,analytic-geometry
A.248	product of elements $\ne e$ implies only one element with order $2$	Let $G$ be an abelian group with even order and $M:=\{g\in G:g^2=e\}$. It is easy to show that $M$ is a sugroup of $G$ and the number of elements of $M$ must be a power of $2$.  I want to prove that the product of the elements of $G$ (which in this case is equal to the product of the elements of $M$) is not the identity element $e$, if and only if #$M=2$ (this means that there is only one element with order $2$). I found this claim when I studied the Wilson criterion for the primality of a number.  The case #$M=4$ is easy. If $a,b\in M$, we can show that $ab$ is not $a,b,e$, hence must be some other element $c$ and we get $abc=c^2=e$. But what about #$M=8$? If we have the elements $e,a,b,c,d,f,g,h$ and $ab=c$ and $df=g$, the product would be $h$ which is impossible considering my claim.  Who can complete my proof ?	group-theory,finite-groups
A.248	producto de elementos $\ne e$ implica sólo un elemento con orden $2$	Si $G$ es un grupo abeliano con orden par y $M:=\{g\in G:g^2=e\}$. Es fácil demostrar que $M$ es un sub grupo de $G$ y el número de elementos de $M$ debe ser una potencia de $2$. Quiero demostrar que el producto de los elementos de $G$ (que en este caso es igual al producto de los elementos de $M$) no es el elemento de identidad $e$, si y sólo si #$M=2$ (esto significa que sólo hay un elemento con orden $2$). Encontré esta afirmación cuando estudié el criterio de Wilson para la primalidad de un número. Si el caso #$M=4$ es fácil. $a,b\in M$, podemos demostrar que $ab$ no es $a,b,e$, por lo tanto debe haber algún otro elemento $c$ y obtenemos $abc=c^2=e$. Pero ¿qué pasa con #$M=8$? Si tenemos los elementos $e,a,b,c,d,f,g,h$ y $ab=c$ y $df=g$, el producto sería $h$, lo cual es imposible considerando mi afirmación. ¿Quién puede completar mi prueba?	group-theory,finite-groups
A.249	An abelian group proof with $g*g=e$ for all $g$.	I have to show that the following group $$ (G, * , e) $$ with its operation $*$, which is defined through $ g*g = e$  for every $g \in G $ is an abelian group. In order to do that one have only to show that the group is commutative. How can one prove it whereas the operation is defined always between an Element and itself? I reckon it is not so simple as it seems Thanks in advance for your help :)	group-theory,abelian-groups,monoid
A.249	Una prueba de grupo abeliano con $g*g=e$ para todos los $g$.	Tengo que demostrar que el siguiente grupo $$ (G, * , e) $$ con su operación $*$, que se define a través de $ g*g = e$ para cada $g \in G $ es un grupo abeliano. Para hacer eso uno sólo tiene que demostrar que el grupo es commutativo. ¿Cómo se puede demostrar que la operación se define siempre entre un elemento y sí mismo? Creo que no es tan simple como parece. Gracias de antemano por tu ayuda :)	group-theory,abelian-groups,monoid
A.250	How to show $(a_1a_2\ldots a_n)^{\frac{1}{n}}\leq \frac{\sum_{i=1}^{n}a_i}{n}$	How to show $(a_1a_2\ldots a_n)^{\frac{1}{n}}\leq \frac{\sum_{i=1}^{n}a_i}{n}$ with $a_i$ positive. Well, I tried by induction: with $n=2$ then $\sqrt{ab}\leq \frac{a+b}{2}$ is equivalent to say (elevate square in both side) $4ab\leq a^2 +2ab + b^2$ and this is equivalent $0\leq(a-b)^2$ and this is true. I suppose it is true for some $n$. But with $n+1$, I don't know how to do, Please can help me with a hint or other way, thank you so much.	inequality,a.m.-g.m.-inequality
A.250	Cómo mostrar $(a_1a_2\ldots a_n)^{\frac{1}{n}}\leq \frac{\sum_{i=1}^{n}a_i}{n}$	¿Cómo mostrar $(a_1a_2\ldots a_n)^{\frac{1}{n}}\leq \frac{\sum_{i=1}^{n}a_i}{n}$ con $a_i$ positivo? Bueno, lo intenté por inducción: con $n=2$ entonces $\sqrt{ab}\leq \frac{a+b}{2}$ es equivalente a decir (elevar cuadrado en ambos lados) $4ab\leq a^2 +2ab + b^2$ y esto es equivalente a $0\leq(a-b)^2$ y esto es cierto. Supongo que es cierto para algunos $n$. Pero con $n+1$, no sé cómo hacer, por favor pueden ayudarme con una pista u otra manera, muchas gracias.	inequality,a.m.-g.m.-inequality
A.251	How to prove that $(k+1)^{\frac{1}{k+1}} < k^{\frac{1}{k}} (k >= 3)$ using mathematical induction?	I've been trying to solve this for hours, but I just can't seem to do it. And yes, I know I have to show that $(k+2)^{\frac{1}{k+2}} < ... < (k + 1)^{\frac{1}{k+1}}$ from the starting assumption that $(k+1)^{\frac{1}{k+1}} < k^{\frac{1}{k}}$. Any hints would be highly welcome.	inequality,induction
A.251	¿Cómo probar que $(k+1)^{\frac{1}{k+1}} < k^{\frac{1}{k}} (k >= 3)$ usando inducción matemática?	He estado tratando de resolver esto durante horas, pero no puedo hacerlo. Y sí, sé que tengo que mostrar que $(k+2)^{\frac{1}{k+2}} < ... < (k + 1)^{\frac{1}{k+1}}$ desde el supuesto inicial que $(k+1)^{\frac{1}{k+1}} < k^{\frac{1}{k}}$. Cualquier sugerencia sería muy bienvenida.	inequality,induction
A.252	Unexpected appearances of $\pi^2 /~6$.	The number $\frac 16 \pi^2$ turns up surprisingly often and frequently in unexpected places. - Julian Havil, Gamma: Exploring Euler's Constant.   It is well-known, especially in 'pop math,' that $$\zeta(2)=\frac1{1^2}+\frac1{2^2}+\frac1{3^2}+\cdots = \frac{\pi^2}{6}.$$ Euler's proof of which is quite nice. I would like to know where else this constant appears non-trivially. This is a bit broad, so here are the specifics of my question:  We can fiddle with the zeta function at arbitrary even integer values to eek out a $\zeta(2)$. I would consider these 'appearances' of $\frac 16 \pi^2$ to be redundant and ask that they not be mentioned unless you have some wickedly compelling reason to include it. By 'non-trivially,' I mean that I do not want converging series, integrals, etc. where it is obvious that $c\pi$ or $c\pi^2$ with $c \in \mathbb{Q}$ can simply be 'factored out' in some way such that it looks like $c\pi^2$ was included after-the-fact so that said series, integral, etc. would equal $\frac 16 \pi^2$. For instance, $\sum \frac{\pi^2}{6\cdot2^n} = \frac 16 \pi^2$, but clearly the appearance of $\frac 16\pi^2$ here is contrived. (But, if you have an answer that seems very interesting but you're unsure if it fits the 'non-trivial' bill, keep in mind that nobody will actually stop you from posting it.)  I hope this is specific enough. This was my attempt at formally saying 'I want to see all the interesting ways we can make $\frac 16 \pi^2$.' With all that being said, I will give my favorite example as an answer below! :$)$  There used to be a chunk of text explaining why this question should be reopened here. It was reopened, so I removed it.	integration,sequences-and-series,riemann-zeta,big-list,pi
A.252	Apariciones inesperadas de $\pi^2 /~6$.	El número $\frac 16 \pi^2$ aparece sorprendentemente a menudo y con frecuencia en lugares inesperados. - Julian Havil, Gamma: Explorando la constante de Euler. Es bien conocido, especialmente en las 'matemáticas populares', que $$\zeta(2)=\frac1{1^2}+\frac1{2^2}+\frac1{3^2}+\cdots = \ frac{\pi^2}{6}.$$ La prueba de Euler es bastante buena. Me gustaría saber dónde más aparece esta constante de manera no trivial. Esto es un poco amplio, así que aquí están los detalles de mi pregunta: podemos probar con la función zeta en valores enteros pares arbitrarios para obtener un $\zeta(2)$. Consideraría que estas 'apariciones' de $\frac 16 \pi^2$ son redundantes y pediría que no se mencionen a menos que tenga alguna razón perversamente convincente para incluirlas. Por 'no trivialmente' quiero decir que no quiero series convergentes, integrales, etc. donde sea obvio que $c\pi$ o $c\pi^2$ con $c \in \mathbb{Q}$ simplemente se puede 'descomponer' de alguna manera de manera que parezca que $c\pi^2$ se incluyó después del hecho para que dicha serie, integral, etc. fuera igual a $\frac 16 \pi^2$. Por ejemplo, $\sum \frac{\pi^2}{6\cdot2^n} = \frac 16 \pi^2$, pero claramente la apariencia de $\frac 16\pi^2$ aquí es artificial. (Pero, si tiene una respuesta que parece muy interesante pero no está seguro de si se ajusta al perfil "no trivial", tenga en cuenta que nadie le impedirá publicarla). Espero que esto sea lo suficientemente específico. Este fue mi intento de decir formalmente 'Quiero ver todas las formas interesantes en las que podemos hacer $\frac 16 \pi^2$'. Dicho todo esto, ¡daré mi ejemplo favorito como respuesta a continuación! :$)$ Solía ​​haber un fragmento de texto que explicaba por qué esta pregunta debería reabrirse aquí. Se volvió a abrir, así que lo eliminé.	integration,sequences-and-series,riemann-zeta,big-list,pi
A.253	How would I show the result below using contour integration?	How would I show the result below using contour integration? $$\int_{-\infty}^{\infty} \frac{\cos bx - \cos ax}{x^2} dx = \pi (a-b)$$ where a>b>0 using contour integration. Any help would be greatly appreciated, thanks!	integration,complex-analysis,trigonometry,fourier-analysis,contour-integration
A.253	¿Cómo mostrar el resultado de abajo usando la integración de contornos?	¿Cómo mostraría el resultado a continuación usando la integración de contornos? $$\int_{-\infty}^{\infty} \frac{\cos bx - \cos ax}{x^2} dx = \pi (a-b)$$ donde a>b>0 usando la integración de contornos. ¡Cualquier ayuda sería muy apreciada, gracias!	integration,complex-analysis,trigonometry,fourier-analysis,contour-integration
A.254	Integration question hard	Can I get some hints on how to solve this integral? $$ I=\int_0^\pi \frac{x \ dx}{1-sinx \ cosx} $$	integration,definite-integrals
A.254	La cuestión de la integración es difícil	¿Puedo obtener algunas sugerencias sobre cómo resolver esta integral? $$ I=\int_0^\pi \frac{x \ dx}{1-sinx \ cosx} $$	integration,definite-integrals
A.255	Integral of $e^{-\frac{u^2}{2}}$	This is the first time I came across the problem of finding integral of $\propto$. I have a joint distribution $$f_{X,Y}(x,y) \propto \exp\left(13xy - 94x^2 - \frac{1}{2}y^2\right)$$  where $ -\infty< x <\infty, -\infty< y <\infty $ I attempted to find $f_X(x)$ as follows:  \begin{align*} f_X(x) &\propto \int_{-\infty}^\infty e^{13xy - 94x^2 - \frac{1}{2}y^2}{\rm d}y\\ &\propto \int_{-\infty}^\infty e^{-\frac{1}{2}(y - 13x)^2 - \frac{19x^2}{2}}{\rm d}y\\ &\propto \frac{1}{e^{\frac{19x^2}{2}}} \int_{-\infty}^\infty e^{-\frac{u^2}{2}}{\rm d}u \end{align*} where $ u = (y - 13x)^2 $ Similarly, I derived $$ f_Y(y) \propto \frac{1}{e^{\frac{19x^2}{376}}} \int_{-\infty}^\infty e^{-u^2}{\rm d}u $$ where $ u = \sqrt{94}x - \frac{13y}{2\sqrt{94}} $ Could you please show me how to proceed to the destination solutions? Thanks in advance.	integration,probability-distributions,definite-integrals
A.255	Integral de $e^{-\frac{u^2}{2}}$	Esta es la primera vez que me encuentro con el problema de encontrar la integral de $\propto$. Tengo una distribución conjunta $$f_{X,Y}(x,y) \propto \exp\left(13xy - 94x^2 - \frac{1}{2}y^2\right)$$ donde $ - \infty< x <\infty, -\infty< y <\infty $ Intenté encontrar $f_X(x)$ de la siguiente manera: \begin{align*} f_X(x) &\propto \int_{-\infty} ^\infty e^{13xy - 94x^2 - \frac{1}{2}y^2}{\rm d}y\\ &\propto \int_{-\infty}^\infty e^{-\ frac{1}{2}(y - 13x)^2 - \frac{19x^2}{2}}{\rm d}y\\ &\propto \frac{1}{e^{\frac{19x ^2}{2}}} \int_{-\infty}^\infty e^{-\frac{u^2}{2}}{\rm d}u \end{align*} donde $ u = ( y - 13x)^2 $ De manera similar, deduje $$ f_Y(y) \propto \frac{1}{e^{\frac{19x^2}{376}}} \int_{-\infty}^\infty e^{-u^2}{\rm d}u $$ donde $ u = \sqrt{94}x - \frac{13y}{2\sqrt{94}} $ ¿Podría mostrarme cómo proceder con las soluciones de destino? Gracias de antemano.	integration,probability-distributions,definite-integrals
A.256	Why do we have to factorize the function before taking its limit	I am learning about limits and there is something that I cant quite understand: If we have the function:  $$ f(x) =\frac{x^2-1}{x-1} $$  Let's say that we want to see which value for y (image) the function approaches as x (domain) gets closer to 1. On a nutshell, we have to take this following limit:  $$ \lim_{x\to1}\frac{x^2-1}{x-1} $$  As soon as we look to this function, we realize that the function is not continuous at x = 1 (By the way, can I say that?).  I know the algorithm to figure out the solution of the limit: First, there is the need of eliminating the function discontinuity. Usually, it is just a matter of factorizing the function into a new function which the exactly same image as the one before with one crucial difference: The function is continuous for all real numbers  My doubts:Is my way to think about it correct? Can I think like that? Take the example above:    $$ f(x) = \frac{x^2-1}{x-1} $$  After factorizing, we get: $$ f(x) = {x+1} $$ If we plot both functions, they are the same, although the second one has its continuity all along the real numbers domain Thanks in advance	limits
A.256	¿Por qué tenemos que factorizar la función antes de tomar su límite	Estoy aprendiendo sobre límites y hay algo que no entiendo: si tenemos la función: $$ f(x) =\frac{x^2-1}{x-1} $$ Digamos que queremos ver qué valor para y (imagen) la función se acerca a x (dominio) se acerca a 1. En pocas palabras, tenemos que tomar este límite: $$ \lim_{x\to1}\frac{x^2-1}{x-1} $$ Tan pronto como miramos a esta función, nos damos cuenta de que la función no es continua en x = 1 (Por cierto, ¿puedo decir eso?). Conozco el algoritmo para encontrar la solución del límite: Primero, hay que eliminar la función discontinuidad. Por lo general, es sólo una cuestión de factorizar la función en una nueva función que la misma imagen que la anterior con una diferencia crucial: La función es continua para todos los números Mi duda: ¿Es mi manera de pensar en ello? ¿Puedo pensar que? Tomar el ejemplo anterior:  $$ f(x) = \frac{x^2-1}{x-1} $$ Después de factorizar, obtenemos: $$ f(x) = {x+1} $$ Si trazamos ambas funciones, son iguales, aunque el segundo tiene su continuidad en todo el dominio de números reales. Gracias de antemano.	limits
A.257	Is "taking a limit" a function? Is it a procedure? A ternary operation?	I was sitting in analysis yesterday and, naturally, we took the limit of some expression. It occurred to me that "taking the limit" of some expression abides the rules of a linear transformation $$\lim_{x \rightarrow k}\ c(f(x)+g(x)) = c \lim_{x \rightarrow k} f(x) + c\ \lim_{x \rightarrow k} g(x),$$ and (my group theory is virtually non existent) appears also to be a homomorphism: $$\lim_{x \rightarrow k} (fg)(x) = \lim_{x \rightarrow k} f(x)g(x), $$ etc. Anyway, my real question is, what mathematical construct is the limit?	limits,terminology
A.257	¿Es "tomar un límite" una función? ¿Es un procedimiento? ¿Una operación ternaria?	Ayer estuve sentado en análisis y, naturalmente, tomamos el límite de alguna expresión. Me ocurrió que "tomar el límite" de alguna expresión se ajusta a las reglas de una transformación lineal $$\lim_{x \rightarrow k}\ c(f(x)+g(x)) = c \lim_{x \rightarrow k} f(x) + c\ \lim_{x \rightarrow k} g(x),$$ y (mi teoría de grupos es prácticamente inexistente) parece también ser un homomorfismo: $$\lim_{x \rightarrow k} (fg)(x) = \lim_{x \rightarrow k} f(x)g(x), $$ etc. De todos modos, mi verdadera pregunta es, ¿qué construcción matemática es el límite?	limits,terminology
A.258	How to prove $\lim_{x \to \infty} \frac{x^k}{e^x}=0$ ? (k is any positive number)	Originally, i was trying to find the value of $$\lim_{h \to 0} \frac{e^{-\frac{1}{h^2}}}{h}$$ to find out differentiabiltiy of  $f(x) = \begin{cases} e^{-1/x^2} & \text{ if } x \ne 0 \\ 0 & \text{ if } x = 0 \end{cases}$  at 0. In this case,  $$\lim_{h \to 0} \frac{e^{-\frac{1}{h^2}}}{h}=\lim_{x \to \infty} \frac{x}{e^{x^2}}=\lim_{x \to -\infty} \frac{x}{e^{x^2}}$$ So when I applied L'Hospital's rule, then its value was 0. And I thought that No matter how big $k$ is,  $$\lim_{x \to \infty} \frac{x^k}{e^x}$$ will be equal to zero because exponential's increase speed is much faster than polynomial's. Definitely, we can also apply L'hospital's rule at here, but I think it is not proper qualitative explanation for the fact that exponential is much bigger than polynomial. Is there any other approach which explains why  about this problem? (In fact, I tried to use $\epsilon-\delta$ but how can i show that there exist some $M$ s.t. for every $M then  $x^k<\epsilon e^x$?) (And I also tried to use inequality like $e^x>x+1$  for all $x>0$ but it only worked for $k<1$)	limits,derivatives,exponential-function,epsilon-delta,differential
A.258	¿ Cómo probar que $\lim_{x \to \infty} \frac{x^k}{e^x}=0$ ? (k es cualquier número positivo)	Originalmente, estaba tratando de encontrar el valor de $$\lim_{h \to 0} \frac{e^{-\frac{1}{h^2}}}{h}$$ para descubrir la diferenciabilidad de $ f(x) = \begin{cases} e^{-1/x^2} & \text{ if } x \ne 0 \\ 0 & \text{ if } x = 0 \end{cases}$ en 0 . En este caso, $$\lim_{h \to 0} \frac{e^{-\frac{1}{h^2}}}{h}=\lim_{x \to \infty} \frac{x}{e^{x^2}}=\lim_{x \to -\infty} \frac{x}{e^{x^2}}$$ Así que cuando aplicé la regla de L'Hospital, entonces su valor era 0. Y pensé que no importa lo grande que sea $k$ es, $$\lim_{x \to \infty} \frac{x^k}{e^x}$$ será igual a cero porque la velocidad de aumento de la exponencial es mucho más rápida que la del polinomio. Definitivamente, también podemos aplicar la regla de L'Hospital aquí, pero creo que no es una explicación cualitativa adecuada para el hecho de que la exponencial es mucho más grande que el polinomio. ¿Hay algún otro enfoque que explique por qué sobre este problema? (De hecho, traté de usar $\epsilon-\delta$ pero cómo puedo mostrar que hay algún $M$ s.t. para cada $M luego $x^k<\epsilon e^x$?) (Y también intenté usar la desigualdad como $e^x>x+1$ para todos los $x>0$ pero solo funcionó para $k<1$)	limits,derivatives,exponential-function,epsilon-delta,differential
A.259	Why does $n^c$ grow faster than $2^n$?	For every finite case, I can find a $c$ where $2^n = n^c$, so why is this true? $$\lim_{n \rightarrow \infty} \frac{2^n}{n^c} = 0$$ From the finite cases it seems like $2^n$ grows faster because we can find a $c$ to match it at any $n$.	limits
A.259	¿Por qué el $n^c$ crece más rápido que el $2^n$?	Para cada caso finito, puedo encontrar un $c$ donde $2^n = n^c$, así que ¿por qué es esto cierto? $$\lim_{n \rightarrow \infty} \frac{2^n}{n^c} = 0$$ De los casos finitos parece que $2^n$ crece más rápido porque podemos encontrar un $c$ para coincidir con él en cualquier $n$.	limits
A.260	limit with two variables	The limit I need to calculate is $\lim_{(x,y)\rightarrow (0,0)}\frac{xy^{2}}{x^{4}+y^{2}}$. Using polar coordinates I get: $lim_{r\rightarrow 0}\frac{r\cos(\theta)\sin^{2}(\theta)}{r^{2}\cos^{4}(\theta)+\sin^{2}(\theta)}$. Now if $\sin(\theta)\neq 0$ then the limit is $0$. How do I handle the case where $\theta=0$ or $\theta = \pi$? And is there a better way to approach this limit?	limits,multivariable-calculus
A.260	límite con dos variables	El límite que necesito para calcular es $\lim_{(x,y)\rightarrow (0,0)}\frac{xy^{2}}{x^{4}+y^{2}}$. Usando las coordenadas polares obtengo: $lim_{r\rightarrow 0}\frac{r\cos(\theta)\sin^{2}(\theta)}{r^{2}\cos^{4}(\theta)+\sin^{2}(\theta)}$. Ahora si $\sin(\theta)\neq 0$ entonces el límite es $0$. ¿Cómo manejo el caso donde $\theta=0$ o $\theta = \pi$? ¿Y hay una mejor manera de acercarse a este límite?	limits,multivariable-calculus
A.261	Show that $\det(T_n)=\sum_{k=0}^{n}\alpha^{n-k}\beta^k$	Let the matrix $T_n\in M(n\times n,\mathbb{F})$, where $\mathbb{F}$ denotes a field, be defined by $T_n=(t_{ij})$ with $$t_{ij}= \begin{cases}        \alpha\beta &  1\leq i\leq n-1,\;j=i+1 \\       \alpha+\beta & 1\leq i\leq n,\; j=i \\       1 & 2\leq i\leq n,\; j=i-1 \\       0 &\textrm{otherwise}    \end{cases} $$ Show that  $$\det(T_n)=\sum_{k=0}^{n}\alpha^{n-k}\beta^k$$  My approach: I tried a proof via induction and while the basis step is trivial, I can't seem to solve the induction step since the matrix is never in upper or lower triangular form but always a block matrix, which makes this seemingly difficult as when calculating the determinant of block matrices, one usually calculates the product of all the "diagonal blocks". I would very much appreciate help, thank you very much.	linear-algebra,matrices,induction,determinant
A.261	Muestra que $\det(T_n)=\sum_{k=0}^{n}\alpha^{n-k}\beta^k$	Sea la matriz $T_n\in M(n\times n,\mathbb{F})$, donde $\mathbb{F}$ denota un campo, definida por $T_n=(t_{ij})$ con $$ t_{ij}= \begin{cases} \alpha\beta & 1\leq i\leq n-1,\;j=i+1 \\ \alpha+\beta & 1\leq i\leq n,\; j=i \\ 1 & 2\leq i\leq n,\; j=i-1 \\ 0 &\textrm{otherwise} \end{casos} $$. Muestre que $$\det(T_n)=\sum_{k=0}^{n}\alpha^{n-k}\beta^k$$ Mi enfoque: He intentado una prueba a través de la inducción y mientras que el paso base es trivial, no puedo resolver el paso de inducción ya que la matriz nunca está en forma triangular superior o inferior sino siempre una matriz de bloque, lo que hace que esto sea aparentemente difícil como cuando se calcula el determinante de las matrices de bloque, uno generalmente calcula el producto de todos los "bloques diagonales". Agradecería mucho la ayuda, muchas gracias.	linear-algebra,matrices,induction,determinant
A.262	Positve part and negative part of a real number	Let $a$ be a real number . The positive part of $a$, denoted by $a^+$ is given by expression $$a^+ = \text{if } a\geq 0 \text{ then $a$ else } 0$$ The negative part of $a$, denoted by $a^-$ is given by expression $$a^- = \text{if } a\geq 0 \text{ then $0$ else } -a$$ Both $a^+$ and $a^-$  are non negative and the following relationship hold $$ a = a^+ - a^-$$ Above is the text from my compiler optimization book and I cannot understand the relationship explained. How can $a$ be a real number and have positive and negative parts?	linear-programming
A.262	Parte positiva y parte negativa de un número real	Sea $a$ un numero real. La parte positiva de $a$, denotada por $a^+$, es dada por la expresión $$a^+ = \text{if } a\geq 0 \text{ then XX1 else } 0$$ La parte negativa de $a$, denotada por $a^-$, es dada por la expresión $$a^- = \text{if } a\geq 0 \text{ then $0$ else } -a$$ Tanto $a^+$ como $a^-$ no son negativos y la siguiente relación tiene $$ a = a^+ - a^-$$ Arriba está el texto de mi libro de optimización del compilador y no puedo entender la relación explicada. ¿Cómo puede $a$ ser un número real y tener partes positivas y negativas?	linear-programming
A.263	Formal relationship between rules of inference and the material conditional	I am not $100\%$ clear as to what constitutes the difference between a rule of inference and the material conditional, at least in classical logic. I am using the truth-functional definition of the material conditional, commonly visualised through its truth table, but I'm not entirely sure what the formal definition of a rule of inference is. The wikipedia article defines it to be a particular kind of logical form, which seems to be a term from philosophical logic that I'm not familiar with, but reading that article didn't really answer my question. It pertains more to the mathematical side of things, and I am specifically interested in the interplay between the concepts on the syntactic and semantic level. As far as I can tell, any rule of inference can be 'captured' by a corresponding material conditional: if we take modus ponens as a well-known example, what is the difference between $$(a\land (a\to b))\to b$$ and $${a\to b,\text{ } a \over b}?$$ On a functional level, both statements seem to be expressing the same thing. What determines the need to use two separate terms and notations, and what, if anything, separates them?	logic,soft-question,formal-systems
A.263	Relación formal entre las reglas de inferencia y la condición material	No estoy claro $100\%$ sobre lo que constituye la diferencia entre una regla de inferencia y la conditional material, al menos en la lógica clásica. Estoy usando la definición verdadero-funcional de la conditional material, comúnmente visualizada a través de su tabla de la verdad, pero no estoy completamente seguro de cuál es la definición formal de una regla de inferencia. El artículo de Wikipedia la define como un tipo particular de forma lógica, que parece ser un término de lógica filosófica que no estoy familiarizado, pero leer ese artículo no respondio realmente mi pregunta. Pertenece más a la respuesta al lado matemático de las cosas, y estoy específicamente interesado en la interacción entre los conceptos a nivel sintáctico y semántico. Hasta donde puedo decir, cualquier regla de inferencia puede ser "capturada" por un condicional material correspondiente: si tomamos el modus ponens como un ejemplo bien conocido, ¿cuál es la diferencia entre $$(a\land (a\to b ))\to b$$ y $${a\to b,\text{ } a \over b}?$$ A nivel funcional, ambas declaraciones parecen expresar lo mismo. ¿Qué determina la necesidad de utilizar dos términos y notaciones separadas, y qué los separa, si es que hay algo?	logic,soft-question,formal-systems
A.264	Modulo Power Arithmetic	While performing some arithmetic operations, i am stuck at one point.I want to know is it possible to write ($a^b)\%p$ as ($a^{(b\%p)})\%p$?	modular-arithmetic
A.264	Aritmética de la potencia de módulo	Mientras realizo algunas operaciones aritméticas, estoy atascado en un punto. Quiero saber si es posible escribir ($a^b)\%p$ como ($a^{(b\%p)})\%p$?	modular-arithmetic
A.265	How to show that $(a/b)^2$ in this situation cannot be an integer	Let $a$,$b$ be relatively prime integers which are both greater than or equal to $2$ . Show that $(a/b)^2$ cannot be an integer. I have tried to use contradiction to prove this result, but I am not sure that my idea is correct or not. Below is my idea (not a complete proof). Suppose $(a/b)^2$ be an integer. We have $a/b$ is an integer. Then, we have $a=bk$ where $k$ is an integer since $b$ divides $a$. Since $a,b$ are relatively prime, therefore $b$ must be $1$. Hence, contradiction arises. I am not sure that my idea is correct or not, could anyone help me to check it? If my idea is wrong, could you give me a idea to do this question?	number-theory,gcd-and-lcm
A.265	¿Cómo demostrar que $(a/b)^2$ en esta situación no puede ser un número entero	Sean $a$,$b$ números enteros relativamente primos que sean mayores o iguales que $2$. Demuestre que $(a/b)^2$ no puede ser un número entero. He intentado utilizar la contradicción para demostrar este resultado, pero no estoy seguro de que mi idea sea correcta o no. A continuación se muestra mi idea (no es una prueba completa). Supongamos que $(a/b)^2$ sea un número entero. Tenemos que $a/b$ es un número entero. Entonces, tenemos $a=bk$ donde $k$ es un número entero ya que $b$ divide a $a$. Dado que $a,b$ son primos relativos, $b$ debe ser $1$. De ahí surge la contradicción. No estoy seguro de que mi idea sea correcta o no, ¿alguien podría ayudarme a comprobarlo? Si mi idea es incorrecta, ¿podrían darme una idea para hacer esta pregunta?	number-theory,gcd-and-lcm
A.266	What happens when we (incorrectly) make improper fractions proper again?	Many folks avoid the "mixed number" notation such as $4\frac{2}{3}$ due to its ambiguity. The example could mean "$4$ and two thirds", i.e. $4+\frac{2}{3}$, but one may also be tempted to multiply, resulting in $\frac{8}{3}$. My questions pertain to what happens when we iterate this process -- alternating between changing a fraction to a mixed number, then "incorrectly" multiplying the mixed fraction. The iteration terminates when you arrive at a proper fraction (numerator $\leq$ denominator) or an integer. I'll "define" this process via sufficiently-complicated example: $$\frac{14}{3} \rightarrow 4 \frac{2}{3} \rightarrow \frac{8}{3} \rightarrow 2 \frac{2}{3} \rightarrow \frac{4}{3} \rightarrow 1\frac{1}{3}\rightarrow \frac{1}{3}.$$  Does this process always terminate?  For which $(p,q)\in\mathbb{N}\times(\mathbb{N}\setminus\{0\})$ does this process, with initial iterate $\frac{p}{q}$, terminate at $\frac{p \mod q}{q}$?	number-theory,elementary-number-theory,recreational-mathematics,fractions
A.266	¿Qué sucede cuando (incorrectamente) hacemos fracciones incorrectas correctas de nuevo?	Muchas personas evitan la notación de "número mixto" como $4\frac{2}{3}$ debido a su ambigüedad. El ejemplo podría significar "$4$ y dos tercios", es decir, $4+\frac{2}{3}$, pero también se puede ser tentado a multiplicar, lo que resulta en $\frac{8}{3}$. Mis preguntas se refieren a lo que sucede cuando iteramos este proceso - alternando entre cambiar una fracción a un número mixto, luego "incorrectamente" multiplicando la fracción mixta. La iteración termina cuando llegues a una fracción adecuada (numerador $\leq$ denominador) o un número entero. Voy a "definir" este proceso a través de un ejemplo suficientemente complicado: $$\frac{14}{3} \rightarrow 4 \frac{2}{3} \rightarrow \frac{8}{3} \rightarrow 2 \frac{2}{3} \rightarrow \frac{4}{3} \rightarrow 1\frac{1}{3}\rightarrow \frac{1}{3}.$$ ¿Si este proceso siempre termina? ¿Para qué $(p,q)\in\mathbb{N}\times(\mathbb{N}\setminus\{0\})$ este proceso, con la iteración inicial $\frac{p}{q}$, termina en $\frac{p \mod q}{q}$?	number-theory,elementary-number-theory,recreational-mathematics,fractions
A.267	Dual of Lagrange Dual	For linear programming, it's well known that the dual of the dual is the primal. I'm wondering if it is the case for Lagrange duality, and I'm having a hard time showing this. Notationally, let the primal problem be: $$\text{minimize } \quad f_0(x)$$ $$\text{subject to } \quad f_i(x) \leq 0, \quad i = 1, \dots, m$$ And the dual be: $$\text{minimize } \quad -g(\lambda) = - \inf_x L(x, \lambda)$$ $$\text{subject to } \quad -\lambda \leq 0$$ Where $L(x,\lambda) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x)$ is the Lagrangian. I suspect it isn't true in general that the dual of dual is the primal. However, intuitively when I hear the term dual I assume that the dual of the dual should be the primal, so this got me confused.	optimization,convex-optimization,linear-programming,duality-theorems
A.267	Dual de Lagrange Dual	Para la programación lineal, es bien sabido que el dual del dual es el primal. Me pregunto si es el caso de la dualidad de Lagrange, y estoy teniendo dificultades para demostrar esto. Notacionalmente, permítan que el problema primal sea: $$\text{minimize } \quad f_0(x)$$ $$\text{subject to } \quad f_i(x) \leq 0, \quad i = 1, \dots, m$$ Y el dual sea: $$\text{minimize } \quad -g(\lambda) = - \inf_x L(x, \lambda)$$ $$\text{subject to } \quad -\lambda \leq 0$$ Donde $L(x,\lambda) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x)$ es el Lagrange. Sospecho que no es cierto en general que el dual del dual es el primal. Sin embargo, intuitivamente cuando escucho el término dual supongo que el dual del dual debe ser el primal, así que esto me confundió.	optimization,convex-optimization,linear-programming,duality-theorems
A.268	If $Z\thicksim\text{Poisson}(\lambda)$, find the expected value of $\frac{1}{1+Z}.$	The Problem: Suppose that $Z\thicksim\text{Poisson}(\lambda)$. Find the expected value of $\dfrac{1}{1+Z}.$ We have that \begin{align*} E\left[\frac{1}{1+Z}\right]&=\sum_{k=0}^\infty\frac{1}{1+k}\frac{e^{-\lambda}\lambda^k}{k!}=e^{-\lambda}\sum_{k=0}^\infty\frac{\lambda^k}{(k+1)!}=\frac{e^{-\lambda}}{\lambda}\left[\sum_{k=0}^\infty\frac{\lambda^k}{k!}-1\right]\\ &=\frac{e^{-\lambda}}{\lambda}[e^\lambda-1]=\frac{1}{\lambda}-\frac{e^{-\lambda}}{\lambda}, \end{align*} where we used the Taylor series for the exponential function.  Do you agree with my approach above? Any feedback is most welcomed. Thank you very much for your time.	probability,solution-verification,expected-value,poisson-distribution
A.268	Si $Z\thicksim\text{Poisson}(\lambda)$, encuentra el valor esperado de $\frac{1}{1+Z}.$	El problema: Supongamos que $Z\thicksim\text{Poisson}(\lambda)$. Encontrar el valor esperado de $\dfrac{1}{1+Z}.$ Tenemos que \begin{align*} E\left[\frac{1}{1+Z}\right]&=\sum_{k=0}^\infty\frac{1}{1+k}\frac{e^{-\lambda}\lambda^k}{k!}=e^{-\lambda}\sum_{k=0}^\infty\frac{\lambda^k}{(k+1)!}=\frac{e^{-\lambda}}{\lambda}\left[\sum_{k=0}^\infty\frac{\lambda^k}{k!}-1\right]\\ &=\frac{e^{-\lambda}}{\lambda}[e^\lambda-1]=\frac{1}{\lambda}-\frac{e^{-\lambda}}{\lambda}, \end{align*} donde usamos la serie Taylor para la funcion exponencial. ¿Estas de acuerdo con mi enfoque anterior? Cualquier comentario es bienvenido. Muchas gracias por tu tiempo.	probability,solution-verification,expected-value,poisson-distribution
A.269	conditional probability in a family	Let a family have two children. It is known that one of the children is a boy. What is the probability that both the children are boys. So for this we build the sample space $S=\{(b,b)(b,g)(g,g)\}$ Let our event E be the case where both children are boys $E=\{ (b,b) \}$ Let the conditional be F $F=\{(b,g),(b,b)\}$ Hence $P(E|F)=\frac{P(E\cap F)}{P(F)}=\frac{1/3}{2/3}=\frac{1}{2}$ But the answer in my book is given as $\frac{1}{3}$ and I can't seem to understand why.	probability,conditional-probability
A.269	probabilidad condicional en una familia	Si una familia tiene dos hijos, se sabe que uno de los niños es un niño, ¿cuál es la probabilidad de que ambos niños sean niños? Así que para esto construimos el espacio de muestra $S=\{(b,b)(b,g)(g,g)\}$ Que nuestro evento E sea el caso donde ambos niños son niños $E=\{ (b,b) \}$ Que la condición sea F $F=\{(b,g),(b,b)\}$ por lo tanto $P(E|F)=\frac{P(E\cap F)}{P(F)}=\frac{1/3}{2/3}=\frac{1}{2}$ Pero la respuesta en mi libro es dada como $\frac{1}{3}$ y no puedo entender por qué.	probability,conditional-probability
A.270	Expected number of heads before it turns up tails five times	A fair coin is flipped repeatedly until it turns up tails five times. What is the expected number of heads before that happens? Based on the link given in the comment, I have found it can be solved using the recursion $E(n)=\frac{1}{2}(E(n)+1)+\frac{1}{2}(E(n-1))$ which is equivalent to $E(n)=E(n-1)+1$. Is it correct?	probability,expected-value
A.270	El número esperado de cabezas antes de que aparezca cola cinco veces	Una moneda justa se vuelca repetidamente hasta que aparece cinco veces. ¿Cuál es el número esperado de cabezas antes de que eso suceda? Según el enlace proporcionado en el comentario, descubrí que se puede resolver usando la recursividad $E(n)=\frac{1}{2}(E(n)+1)+\frac{1}{2}(E(n-1))$ el cual es equivalente a $E(n)=E(n-1)+1$. ¿es esto corecto?	probability,expected-value
A.271	Probability of $\limsup_{n\to \infty} \{X_n X_{n+1}>0\}$ where $\{X_n\}$ are independent Gaussian r.v.'s with mean 0	Let $\{X_n\}$ be a sequence of independent Gaussian random variables with $\mathbb{E}\, X_n = 0$ for all $n \geq 1$. Find the probability of the event $$ \limsup_{n\to \infty} \big\{ X_n X_{n+1}> 0 \big\} $$ My first thought is that it should be 1 since Gaussians are always positive for a finite value. I was thinking of applying Borel-Cantelli and was trying something along the lines of \begin{align*} \mathbb{P} \big( \limsup_{n\to \infty} \big\{ X_n X_{n+1}> 0 \big\}\big) &= \mathbb{P}\big( X_n X_{n+1} > 0 \,\,\, i.o. \big) \\ &\leq \mathbb{P}\big( \big\{ X_n X_{n+1}> 0 \,\,\, i.o \big\} \cap \big\{ X_{n+1} > 0 \,\,\, i.o\big\} \big)\\ &= \mathbb{P}\big( \big\{ X_n X_{n+1}> 0 \,\,\, i.o \big\}\big) \,\,\mathbb{P}\big( \big\{ X_{n+1} > 0 \,\,\, i.o\big\} \big) \,\,\,\, \text{(by independence)} \end{align*} I'm not sure I'm thinking of this problem right, though.	probability-theory,random-variables,borel-cantelli-lemmas
A.271	Probabilidad de $\limsup_{n\to \infty} \{X_n X_{n+1}>0\}$ donde $\{X_n\}$ son r.v.s gaussianas independientes con media 0	Que $\{X_n\}$ sea una secuencia de variables aleatorias gaussianas independientes con $\mathbb{E}\, X_n = 0$ para todas las $n \geq 1$. Encuentra la probabilidad del evento $$ \limsup_{n\to \infty} \big\{ X_n X_{n+1}> 0 \big\} $$ Mi primer pensamiento es que debería ser 1 ya que los gaussianos son siempre positivos para un valor finito. Estaba pensando en aplicar Borel-Cantelli y estaba intentando algo a lo largo de las líneas de \begin{align*} \mathbb{P} \big( \limsup_{n\to \infty} \big\{ X_n X_{n+1}> 0 \big\}\big) &= \mathbb{P}\big( X_n X_{n+1} > 0 \,\,\, i.o. \big) \\ &\leq \mathbb{P}\big( \big\{ X_n X_{n+1}> 0 \,\,\, i.o \big\} \cap \big\{ X_{n+1} > 0 \,\,\, i.o\big\} \big)\\ &= \mathbb{P}\big( \big\{ X_n X_{n+1}> 0 \,\,\, i.o \big\}\big) \,\,\mathbb{P}\big( \big\{ X_{n+1} > 0 \,\,\, i.o\big\} \big) \,\,\,\, \text{(by independence)} \end{align*} No estoy seguro si estoy pensando en este problema de la manera correcta. 	probability-theory,random-variables,borel-cantelli-lemmas
A.272	Why is $\sqrt{ab}$ = $\sqrt{a}\sqrt{b}$ not true when a and b are both negative?	Apparently $\sqrt{ab}$ = $\sqrt{a}\sqrt{b}$ is only true if a and b are both positive or if a is negative and b is positive or if a is positive and b is negative. In other words, a and b can't both be negative. Is it possible to algebraically prove this? Or is it just a result of the way the square root function is defined?  I know of 1 way to prove this radical property, but I'm still not sure why it won't work for negative numbers. Let x = $\sqrt{ab}$. Let y = $\sqrt{a}\sqrt{b}$ Square both sides for both equations. $x^2 = (\sqrt{ab})^2 = ab$ $y^2 = (\sqrt{a}\sqrt{b})^2 = (\sqrt{a}\sqrt{b})(\sqrt{a}\sqrt{b}) = (\sqrt{a})^2(\sqrt{b})^2 = ab$ $\therefore x^2 = y^2$ $x^2-y^2=0$ $(x+y)(x-y)=0$ $\therefore x = y$ or $x = -y$ Or $\therefore y = x$ or $y = -x$  A lot of people will go through this line of reasoning (shown below) in order to justify why a and b can't both be negative.  Considering that mathematicians define $i^2=-1$ or $i = \sqrt{-1}$ $1 = \sqrt{(-1)(-1)} = \sqrt{-1}\sqrt{-1} = (i)(i) = i^2 = -1  $ But this is only a specific instance where this property fails us. This isn't a  rigorous or at least satisfying proof of why $\sqrt{ab}$ = $\sqrt{a}\sqrt{b}$ can only be true if a and b are not both negative. Note: I just started learning about complex and imaginary numbers and I am no means an expert in  mathematical proofs, so if you do know the answer to this question please try (if possible) your best to answer the question without using too much complex or high-order math that I won't be able to understand.	radicals
A.272	¿Por qué $\sqrt{ab}$ = $\sqrt{a}\sqrt{b}$ no es verdad cuando a y b son ambos negativos?	Aparentemente $\sqrt{ab}$ = $\sqrt{a}\sqrt{b}$ es verdad sólo si a y b son ambos positivos o si a es negativo y b es positivo o si a es positivo y b es negativo. En otras palabras, a y b no pueden ser ambos negativos. ¿Es posible probar esto algebraicamente? ¿O es sólo el resultado de la forma en que se define la función de raíz cuadrada? Sé de una manera de probar esta propiedad radical, pero todavía no estoy seguro de por qué no funcionará para números negativos. Vamos a x = $\sqrt{ab}$. Dejamos que y = $\sqrt{a}\sqrt{b}$ Cuadrados de ambos lados de ambas ecuaciones. $x^2 = (\sqrt{ab})^2 = ab$ $y^2 = (\sqrt{a}\sqrt{b})^2 = (\sqrt{a}\sqrt{b})(\sqrt{a}\sqrt{b}) = (\sqrt{a})^2(\sqrt{b})^2 = ab$ $\therefore x^2 = y^2$ $x^2-y^2=0$ $(x+y)(x-y)=0$ $\therefore x = y$ o $x = -y$ O $\sqrt{ab}$0 o $\sqrt{ab}$1 Muchas personas pasarán por esta línea de razonamiento (mostrada abajo) con el fin de justificar a y b no pueden ser negativos. Considerando que los matemáticos definen $\sqrt{ab}$2 o $\sqrt{ab}$313, este es sólo una prueba específica de la cuestión de la que no sea posible. Pero si el experto no sabe la respuesta de la matemática compleja, por qué no es posible que esto sea una prueba específica de la que no sea una propiedad de la realidad, por lo menos que sea que sea posible, si usted no puede aprender una respuesta a la matemática compleja o no es demasiado rígida y no sé por lo que sea posible.	radicals
A.273	Can fractional/decimal radicals/roots exist?	For questions like "What is the 1/2th root of x would the answer be $x^2$? My logic is that since $$ \sqrt[\cfrac{1}{2}]{x}=x^{1/{(\cfrac{1}{2}})} $$ Which simplifies to $x^2$. So as a general rule it could be $$ \sqrt[\cfrac{1}{a}]{x}=x^{1/{(\cfrac{1}{a}})} =x^a $$ And with a different denominator $$\sqrt[\cfrac{b}{a}]{x}=x^{1/{(\cfrac{b}{a}})} =x^{\cfrac{a}{b}}$$ This corresponds to how decimal/fractional exponents denote radicals (their inverse) while fractional radicals are easier shown with exponents. Example : (2/3rd root of 4) $$\sqrt[\cfrac{2}{3}]{4}=4^{1/{(\cfrac{2}{3}})} =4^{\cfrac{3}{2}}= 8$$ Example (22/7th root of π) : $$\sqrt[\cfrac{22}{7}]{π}=π^{1/{(\cfrac{22}{7}})} =π^{\cfrac{7}{22}} \approx 1.439$$ Example (1/2th root of 1/4) : $$\sqrt[\cfrac{1}{2}]{\cfrac{1}{4}}=\cfrac{1}{4}^{1/(\cfrac{2}{1})} =\cfrac{1}{4}^{(\cfrac{2}{1})} =\cfrac{1}{4}^{2} =\cfrac{1}{16} $$	radicals,decimal-expansion,radical-equations
A.273	¿Pueden existir radicales/raíces fraccionarias/decimales?	Para preguntas como "¿Cuál es la raíz 1/2 de x sería la respuesta $x^2$? Mi lógica es que desde $$ \sqrt[\cfrac{1}{2}]{x}=x^{1/{(\cfrac{1}{2}})} $$ que se simplifica a $x^2$. Así que como regla general podría ser $$ \sqrt[\cfrac{1}{a}]{x}=x^{1/{(\cfrac{1}{a}})} =x^a $$ y con un denominador diferente $$\sqrt[\cfrac{b}{a}]{x}=x^{1/{(\cfrac{b}{a}})} =x^{\cfrac{a}{b}}$$ Esto corresponde a cómo exponentes decimales / fraccionarios denotan radicales (su inverso) mientras que los radicales fraccionarios se muestran más fácilmente con exponentes. Ejemplo: (2/3 raíz de 4) $$\sqrt[\cfrac{2}{3}]{4}=4^{1/{(\cfrac{2}{3}})} =4^{\cfrac{3}{2}}= 8$$ Ejemplo: (22/7 raíz de π) : $$\sqrt[\cfrac{22}{7}]{π}=π^{1/{(\cfrac{22}{7}})} =π^{\cfrac{7}{22}} \approx 1.439$$ Ejemplo: (1/2 raíz de 1/4) $$\sqrt[\cfrac{1}{2}]{\cfrac{1}{4}}=\cfrac{1}{4}^{1/(\cfrac{2}{1})} =\cfrac{1}{4}^{(\cfrac{2}{1})} =\cfrac{1}{4}^{2} =\cfrac{1}{16} $$	radicals,decimal-expansion,radical-equations
A.274	Why the Heaviside distribution $H$ doesn't belong to any Sobolev space $H^{s}(\mathbb{R})$	I prooved that the Dirac distribution $\delta_{0}$ is in the Sobolev space $H^{s}\left(\mathbf{R}^{n}\right)=\left\{f \in \mathcal{S}^{\prime}\left(\mathbf{R}^{n}\right)\left|\left(1+|\xi|^{2}\right)^{s / 2} \mathcal{F} f \in L^{2}\left(\mathbf{R}^{n}\right)\right\}\right.$ for every  $s<-n / 2$  but I steel  wrestling to proof that the  Heaviside distribution $H$  $\forall x \in \mathbb{R}, H(x)=\left\{\begin{array}{lll}{0} & {\text { si }} & {x<0} \\ {1} & {\text { si }} & {x \geq 0}\end{array}\right.$ Doesn't belong to any Sobolev space $H^{s}(\mathbb{R})$, could you elaborate on that? Thanks in advance!	real-analysis,functional-analysis,fourier-analysis,sobolev-spaces,distribution-theory
A.274	¿Por qué la distribución de Heaviside $H$ no pertenece a ningún espacio Sobolev $H^{s}(\mathbb{R})$	He probado que la distribución Dirac $\delta_{0}$ está en el espacio Sobolev $H^{s}\left(\mathbf{R}^{n}\right)=\left\{f \in \mathcal{S}^{\prime}\left(\mathbf{R}^{n}\right)\left|\left(1+|\xi|^{2}\right)^{s / 2} \mathcal{F} f \in L^{2}\left(\mathbf{R}^{n}\right)\right\}\right.$ para cada $s<-n / 2$ pero estoy luchando para probar que la distribución Heaviside $H$ $\forall x \in \mathbb{R}, H(x)=\left\{\begin{array}{lll}{0} & {\text { si }} & {x<0} \\ {1} & {\text { si }} & {x \geq 0}\end{array}\right.$ no pertenece a ningún espacio Sobolev $H^{s}(\mathbb{R})$, ¿podría explicar eso? Gracias de antemano	real-analysis,functional-analysis,fourier-analysis,sobolev-spaces,distribution-theory
A.275	Proving an Integral Identity with Increasing Bounds	How can I show that  $$ \lim_{A\rightarrow \infty} \int_0^A \frac{\sin(x)}{x}dx\;=\;\frac{\pi}{2}?$$ I know that can use the fact that, for $x>0$,  $$x^{-1}\;=\;\int_0^\infty e^{-xt}dt$$ but I'm not sure how to begin.	real-analysis,integration,analysis
A.275	Demostrando una identidad integral con límites crecientes	¿Cómo puedo mostrar que $$ \lim_{A\rightarrow \infty} \int_0^A \frac{\sin(x)}{x}dx\;=\;\frac{\pi}{2}?$$ sé que puede usar el hecho de que, para $x>0$, $$x^{-1}\;=\;\int_0^\infty e^{-xt}dt$$ pero no estoy seguro de cómo empezar.	real-analysis,integration,analysis
A.276	Let $k\in\mathbb{N}$ and $a>1$. Show that $\lim_{n\to\infty} \frac{n^k}{a^n}=0$.	I think what I need to do is find the value of $n$ where $n^k. I know that this value occurs whenever $n>k\log_an$, however I don't understand how to interpret this result into a general $N$ to pick as a maximum for the sequence convergence. What am I missing here?	real-analysis
A.276	$k\in\mathbb{N}$ y $a>1$. Muestre que $\lim_{n\to\infty} \frac{n^k}{a^n}=0$.	Creo que lo que necesito hacer es encontrar el valor de $n$ donde $n^k. Se que este valor ocurre siempre que $n>k\log_an$, sin embargo no entiendo como interpretar este resultado en una general $N $ para elegir como máximo para la convergencia de secuencia. ¿Qué me falta aquí?	real-analysis
A.277	Is the AM-GM inequality the only obstruction for getting a specific sum and product?	This might be silly, but here it goes. Let $P,S>0$ be positive real numbers that satisfy $\frac{S}{n} \ge \sqrt[n]{P}$.  Does there exist a sequence of positive real numbers $a_1,\dots,a_n$ such that $S=\sum a_i,P=\prod a_i$?  Clearly, $\frac{S}{n} \ge \sqrt[n]{P}$ is a necessary condition, due to the AM-GM inequality. But is it sufficient? For $n=2$, the answer is positive, as can be seen by analysing the discriminant of the associated quadratic equation. (In fact, the solvability criterion for the quadratic, namely- the non-negativity of the discriminant, is equivalent to the AM-GM inequality for the sum and the product). What about $n>3$?	real-analysis,polynomials,systems-of-equations,a.m.-g.m.-inequality
A.277	¿Es la desigualdad entre AM y GM la única obstrucción para obtener una suma y un producto específico?	Esto podría ser tonto, pero aquí está. Dejemos que $P,S>0$ sean números reales positivos que satisfacen $\frac{S}{n} \ge \sqrt[n]{P}$. ¿Existe una secuencia de números reales positivos $a_1,\dots,a_n$ tal que $S=\sum a_i,P=\prod a_i$? Claramente, $\frac{S}{n} \ge \sqrt[n]{P}$ es una condición necesaria, debido a la desigualdad AM-GM. Pero es suficiente? Para $n=2$, la respuesta es positiva, como se puede ver analizando el discriminante de la ecuación cuadrática asociada. (De hecho, el criterio de solubilidad para el cuadrático, es decir, la no-negatividad del discriminante, es equivalente a la desigualdad AM-GM para la suma y el producto). ¿Qué pasa con $n>3$?	real-analysis,polynomials,systems-of-equations,a.m.-g.m.-inequality
A.278	Is there a differentiable function such that $f(\mathbb Q) \subseteq \mathbb Q$ but $f'(\mathbb Q) \not \subseteq \mathbb Q$?	Is there a differentiable function $f:\mathbb R \rightarrow \mathbb R$ such that $f(\mathbb Q) \subseteq \mathbb Q$, but $f'(\mathbb Q) \not \subseteq \mathbb Q$? A friend of mine asserted this without giving any examples. I seriously doubt it, but I had hard time trying to disprove it since analysis isn't really my thing. I can't even think of any class of differentiable functions with $f(\mathbb Q) \subseteq \mathbb Q$ other than the rational functions.	real-analysis
A.278	¿Existe una función diferenciable tal que $f(\mathbb Q) \subseteq \mathbb Q$ pero $f'(\mathbb Q) \not \subseteq \mathbb Q$?	¿Existe una función diferenciable $f:\mathbb R \rightarrow \mathbb R$ tal como $f(\mathbb Q) \subseteq \mathbb Q$, pero $f'(\mathbb Q) \not \subseteq \mathbb Q$? Un amigo mío afirmó esto sin dar ningún ejemplo. lo dudo seriamente, pero tuve dificultades para tratar de refutarlo ya que el análisis no es realmente mi cosa. Ni siquiera puedo pensar en ninguna clase de funciones diferenciables con $f(\mathbb Q) \subseteq \mathbb Q$ que no sean las funciones racionales.	real-analysis
A.279	If $\lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2,$ show that $\lim_{x\to 0}f(x)=1$.	Question: Suppose $f:(-\delta,\delta)\to (0,\infty)$ has the property that $$\lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2.$$ Show that $\lim_{x\to 0}f(x)=1$.  My approach: Let $h:(-\delta,\delta)\to(-1,\infty)$ be such that $h(x)=f(x)-1, \forall x\in(-\delta,\delta).$ Note that if we can show that $\lim_{x\to 0}h(x)=0$, then we will be done. Now since we have $$\lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2\implies \lim_{x\to 0}\frac{(f(x)-1)^2}{f(x)}=0\implies \lim_{x\to 0}\frac{h^2(x)}{h(x)+1}=0.$$ Next I tried to come up with some bounds in order to use Sandwich theorem to show that $\lim_{x\to 0} h(x)=0,$ but the bounds didn't quite work out. The bounds were the following: $$\begin{cases}h(x)\ge \frac{h^2(x)}{h(x)+1},\text{when }h(x)\ge 0,\\h(x)<\frac{h^2(x)}{h(x)+1},\text{when }h(x)<0.\end{cases}$$ How to proceed after this?	real-analysis,calculus,limits
A.279	Si $\lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2,$ muestra que $\lim_{x\to 0}f(x)=1$.	Pregunta: Supongamos que $f:(-\delta,\delta)\to (0,\infty)$ tiene la propiedad de que $$\lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2.$$ Muestre que $\lim_{x\to 0}f(x)=1$. Mi enfoque: Que $h:(-\delta,\delta)\to(-1,\infty)$ sea tal que $h(x)=f(x)-1, \forall x\in(-\delta,\delta).$ Note que si podemos mostrar que $\lim_{x\to 0}h(x)=0$, entonces habremos terminado. Ahora que tenemos $$\lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2\implies \lim_{x\to 0}\frac{(f(x)-1)^2}{f(x)}=0\implies \lim_{x\to 0}\frac{h^2(x)}{h(x)+1}=0.$$ Luego traté de llegar a algunos límites para usar el teorema de Sandwich para mostrar que $\lim_{x\to 0} h(x)=0,$ pero los límites no funcionaron muy bien. Los límites eran los siguientes: $$\begin{cases}h(x)\ge \frac{h^2(x)}{h(x)+1},\text{when }h(x)\ge 0,\\h(x)<\frac{h^2(x)}{h(x)+1},\text{when }h(x)<0.\end{cases}$$ ¿Cómo proceder después de esto?	real-analysis,calculus,limits
A.280	Why do engineers use derivatives in discontinuous functions? Is it correct?	I am a Software Engineering student and this year I learned about how CPUs work, it turns out that electronic engineers and I also see it a lot in my field, we do use derivatives with discontinuous functions. For instance in order to calculate the optimal amount of ripple adders so as to minimise the execution time of the addition process: $$\text{ExecutionTime}(n, k) = \Delta(4k+\frac{2n}{k}-4)$$ $$\frac{d\,\text{ExecutionTime}(n, k)}{dk}=4\Delta-\frac{2n\Delta}{k^2}=0$$ $$k= \sqrt{\frac{n}{2}}$$ where $n$ is the number of bits in the numbers to add, $k$ is the amount of adders in ripple and $\Delta$ is the "delta gate" (the time that takes to a gate to operate). Clearly you can see that the execution time function is not continuous at all because $k$ is a natural number and so is $n$. This is driving me crazy because on the one hand I understand that I can analyse the function as a continuous one and get results in that way, and indeed I think that's what we do ("I think", that's why I am asking), but my intuition and knowledge about mathematical analysis tells me that this is completely wrong, because the truth is that the function is not continuous and will never be and because of that, the derivative with respect to $k$ or $n$ does not exist because there is no rate of change. If someone could explain me if my first guess is correct or not and why, I'd appreciate it a lot, thanks for reading and helping!	real-analysis,calculus,functions,derivatives,optimization
A.280	¿Por qué los ingenieros usan derivados en funciones discontinuas? ¿Es correcto?	Soy estudiante de Ingeniería de Software y este año aprendí sobre cómo funcionan las CPUs, resulta que los ingenieros electrónicos y también lo veo mucho en mi campo, utilizamos derivados con funciones discontinuas. Por ejemplo, para calcular la cantidad óptima de adicionadores de ondas para minimizar el tiempo de ejecución del proceso de adición: $$\text{ExecutionTime}(n, k) = \Delta(4k+\frac{2n}{k}-4)$$ $$\frac{d\,\text{ExecutionTime}(n, k)}{dk}=4\Delta-\frac{2n\Delta}{k^2}=0$$ $$k= \sqrt{\frac{n}{2}}$$ donde $n$ es el número de bits en los números a sumar, $k$ es la cantidad de adiciones en ondas y $\Delta$ es la "puerta delta" (el tiempo que toma una puerta para operar). Claramente puede ver que la función de tiempo de ejecución no es continua en absoluto porque $k$ es un número natural y también lo es $n$. Esto me está volviendo loco porque por un lado entiendo que puedo analizar la función como continua y obtener resultados de esa manera, y efectivamente creo que eso es lo que hacemos ("pienso", por eso pregunto), pero mi intuición y conocimiento sobre análisis matemático me dice que esto es completamente incorrecto, porque la verdad es que la función no es continua y nunca lo será y por eso la derivada con respecto a $k$ o $n$ no existe. porque no hay tasa de cambio. Si alguien pudiera explicarme si mi primera suposición es correcta o no y por qué, se lo agradecería mucho, ¡gracias por leer y ayudar!	real-analysis,calculus,functions,derivatives,optimization
A.281	Alternate methods to prove that $n^{\frac{1}{n}} \rightarrow 1$ where $n \in \mathbb{N}$	So, I was trying to prove that $n^{\frac{1}{n}} \rightarrow 1$ where $n \in \mathbb{N}$. Here's what I did: Since $n^{1/n}>1$, let us suppose $n=(1+k)^n$ for $n>1$ and some $k>0$. Now, I used the binomial expansion and wrote: $$\begin{align} n&=1+nk+\frac{n(n-1)}{2}k^2+....+k^n \geq1+\frac{n(n-1)}{2}k \\ &\Rightarrow n-1 \geq \frac{n(n-1)}{2}k^2 \\ &\Rightarrow k^2 \leq \frac{2}{n} \end{align}$$ So now, we can always find an $n>0$ such that $\frac{2}{\epsilon^2}. Now, as $|n^{\frac{1}{n}}-1|\geq0$, we have, $n^{\frac{1}{n}}-1=k\leq(\frac{2}{n})^{\frac{1}{2}}<\epsilon$ And in this way, I proved that $n^{\frac{1}{n}}\rightarrow1$. I wanted to know how can I prove this without using Binomial expansion. I tried to do this using the Bernoulli's Inequality,  but couldn't get too far. Any help/hint would be highly appreciable.	real-analysis,sequences-and-series,convergence-divergence
A.281	Metodos alternativos para demostrar que $n^{\frac{1}{n}} \rightarrow 1$ donde $n \in \mathbb{N}$	Así que, estaba tratando de probar que $n^{\frac{1}{n}} \rightarrow 1$ donde $n \in \mathbb{N}$. Esto es lo que hice: Desde $n^{1/n}>1$, supongamos $n=(1+k)^n$ para $n>1$ y algunos $k>0$. Ahora, usé la expansión binomial y escribí: $$\begin{align} n&=1+nk+\frac{n(n-1)}{2}k^2+....+k^n \geq1+\frac{n(n-1)}{2}k \\ &\Rightarrow n-1 \geq \frac{n(n-1)}{2}k^2 \\ &\Rightarrow k^2 \leq \frac{2}{n} \end{align}$$ Así que ahora, siempre podemos encontrar un $n>0$ tal que $\frac{2}{\epsilon^2}. Ahora que $n^{\frac{1}{n}}-1\geq0$n^{\frac{1}{n}} \rightarrow 1$0n^{\frac{1}{n}}-1=k\leq{\frac{2}{n}})^{\frac{1}{2}}<\epsilon$n^{\frac{1}{n}} \rightarrow 1$1n^{\frac{1}{n}}\rightarrow1$. Quería saber cómo puedo probar esto sin usar la expansión binomial. Traté de hacer esto usando la Inequidad de Berno, no pude llegar demasiado lejos. Cualquier ayuda sería muy apreciable, pero sería muy apreciable.	real-analysis,sequences-and-series,convergence-divergence
A.282	How to prove the value o $\zeta(2)$ by Functional Analysis?	I have a question about $\sum_{n=1}^{\infty} 1/n^2$ = $\pi^2/6$ I know it can be proven with standard 1 variable analysis (working on Taylor series of $\arcsin$ or something like that) or basic complex analysis. But someone told me it has also great prove using Functional Analysis. He suggested it is proved on standard first course of FA, but it was not in my case. Can You write here the proof using FA or leave a link in comment? Thanks in advance.	real-analysis,complex-analysis,functional-analysis,sums-of-squares
A.282	¿Cómo probar el valor o $\zeta(2)$ por análisis funcional?	Tengo una pregunta sobre $\sum_{n=1}^{\infty} 1/n^2$ = $\pi^2/6$ sé que se puede probar con el análisis de variables estándar 1 (trabajando en la serie de Taylor de $\arcsin$ o algo así) o análisis complejo básico. Pero alguien me dijo que también tiene una gran prueba usando el análisis funcional. Sugirió que se pruebe en el primer curso estándar de FA, pero no fue en mi caso. ¿Puedes escribir aquí la prueba usando FA o dejar un enlace en el comentario? Gracias por adelantado.	real-analysis,complex-analysis,functional-analysis,sums-of-squares
A.283	Determine whether $(x_n)$ converges or diverges	Given $x_1 := a > 0$ and $x_{n+1} := x_n + \frac{1}{x_n}$ for $n \in \mathbb{N}$, determine whether $(x_n)$ converges or diverges.  Since $x_1 > 0$,  it seems obvious that the sequence is strictly increasing and always positive because we are always adding a positive number to each subsequent element in the sequence.  The trickier part is to show whether $(x_n)$ is bounded or not. If $(x_n)$ is bounded, then $\exists M \in \mathbb{R}$ such that  \begin{align}|x_n| \leq M ~\forall n \in \mathbb{N}\tag{1} \end{align} Alternatively, I think this means that $M$ could be a supremum of the sequence and another way to rewrite $(1)$ is given any $\epsilon > 0$ \begin{align} x_n + \epsilon \leq M \tag{2}\end{align} Choose $\epsilon = \frac{1}{x_n}$. Then  \begin{align} x_{n+1} = x_n + \frac{1}{x_n} \leq M \implies x_n + \epsilon \leq M \tag{3}\end{align} Thus I conclude that the inequality holds $\forall n \in \mathbb{N}$ and that $(x_n)$ is convergent. Thus by the Monotone Sequence Convergence Theorem, $(x_n)$ is convergent.  The part I am not sure about is my reasoning to show that $x_n$ being bounded is correct or not.	real-analysis,sequences-and-series,convergence-divergence
A.283	Determina si $(x_n)$ converge o diverge	Dado que $x_1 := a > 0$ y $x_{n+1} := x_n + \frac{1}{x_n}$ para $n \in \mathbb{N}$, determinar si $(x_n)$ converge o diverge. Desde $x_1 > 0$, parece obvio que la secuencia es estrictamente creciente y siempre positiva porque siempre estamos añadiendo un número positivo a cada elemento subsecuente en la secuencia. La parte más complicada es mostrar si $(x_n)$ está limitado o no. Si $(x_n)$ está limitado, entonces $\exists M \in \mathbb{R}$ tal que \begin{align}x_n \leq M ~\forall n \in \mathbb{N}\tag{1} \end{align} Alternativamente, creo que esto significa que $M$ podría ser una suprema de la secuencia y otra forma de reescribir $(1)$ se da cualquier $\epsilon > 0$ \begin{align} x_n + \epsilon \leq M \tag{2}\end{align} Elige $\epsilon = \frac{1}{x_n}$. Entonces \begin{align} x_{n+1} = x_n + \frac{1}{x_n} \leq M \implica x_n + \epsilon \leq M \tag{3}\end{align} Así concluyo que el la desigualdad se cumple $\forall n \in \mathbb{N}$ y que $(x_n)$ es convergente. Por lo tanto, según el teorema de convergencia de secuencias monótonas, $ (x_n) $ es convergente. La parte de la que no estoy seguro es mi razonamiento para demostrar que el hecho de que $x_n$ esté limitado es correcto o no.	real-analysis,sequences-and-series,convergence-divergence
A.284	Why decimals of rational numbers behave periodically?	I am interesting in the proof of that every rational number cannot have in decimal form infinite number of digits that don't repeat (or the other way around). So, then is enough to prove following statement: For any $n \in \mathbb{N}$ rational number $\frac{1}{n}$ can be represented in decimal form such that it's digits, if there are infinitely many, are repeating. If this is true, then it is true for any $\frac{m}{n} = \frac{1}{n} + \frac{1}{n} + ... + \frac{1}{n}$ ($m$ times).	real-numbers,rational-numbers,decimal-expansion
A.284	¿Por qué las decimales de los números racionales se comportan periódicamente?	Me interesa la prueba de que cada número racional no puede tener en forma decimal un número infinito de dígitos que no se repiten (o viceversa). Así que, entonces es suficiente para probar la siguiente afirmación: Para cualquier número racional $n \in \mathbb{N}$ $\frac{1}{n}$ puede ser representado en forma decimal de tal manera que sus dígitos, si hay infinitamente muchos, se repiten. Si esto es cierto, entonces es cierto para cualquier $\frac{m}{n} = \frac{1}{n} + \frac{1}{n} + ... + \frac{1}{n}$ ($m$ veces).	real-numbers,rational-numbers,decimal-expansion
A.285	How to determine the sum of a series that is neither geometric nor arithmetic but quadratic or cubic?	How to determine the formula of the sum of a series given its $n$th-term formula like: $$U_n= n^2+n$$ or $$U_n= 6n^2 -12n + 5$$	sequences-and-series
A.285	¿Cómo determinar la suma de una serie que no es ni geométrica ni aritmética sino cuadrática o cúbica?	Cómo determinar la fórmula de la suma de una serie dada su fórmula de término $n$ como: $$U_n= n^2+n$$ o $$U_n= 6n^2 -12n + 5$$	sequences-and-series
A.286	Knowing$\sum a_{k}$ converges, how to prove that $\lim _{n \rightarrow+\infty} n a_{n}=0$.	Let $a_{n}$ be decreasing and positive. Then $\sum a_{k}$ converges implies $\lim _{n \rightarrow+\infty} n a_{n}=0$. I think since $na_n$ is positive, the only thing to do is to find an upper bound for the sequence. But I don't know how to split the sequence to form the upper bound.	sequences-and-series,limits
A.286	Saber que $\sum a_{k}$ converge, cómo probar que $\lim _{n \rightarrow+\infty} n a_{n}=0$.	Sea $a_{n}$ decreciente y positivo. Entonces $\sum a_{k}$ converge implica $\lim _{n \rightarrow+\infty} n a_{n}=0$. Creo que dado que $na_n$ es positivo, lo único que hay que hacer es encontrar un límite superior para la secuencia. Pero no sé cómo dividir la secuencia para formar el límite superior.	sequences-and-series,limits
A.287	What is the closed form of $\sum_{i=1}^n\frac{2i}{2^i}$	I looked at $\frac{\sum 2i}{\sum2^i}$ (division), however both expressions are not equal. I am looking for an expression like $\sum_{i=1}^n\frac{2i}{2^i}=5n$ for example.	sequences-and-series
A.287	¿Cuál es la forma cerrada de $\sum_{i=1}^n\frac{2i}{2^i}$	Miré $\frac{\sum 2i}{\sum2^i}$ (división), sin embargo, ambas expresiones no son iguales. Estoy buscando una expresión como $\sum_{i=1}^n\frac{2i}{2^i}=5n$ por ejemplo.	sequences-and-series
A.288	Alternating Harmonic Series Spin-off	We know that the series $\sum (-1)^n/n$ converges, and clearly every other alternating harmonic series with the sign changing every two or more terms such as $$\left(1+\frac{1}{2}+\frac{1}{3}\right)-\left(\frac{1}{4}+\frac{1}{5}+\frac{1}{6}\right)+\left(\frac{1}{7}+\frac{1}{8}+\frac{1}{9}\right)-\cdots$$ must converge. My question here is that does the series below also converge? $$\sum\frac{\textrm{sgn}(\sin(n))}{n}\quad\textrm{or}\quad\sum\frac{\sin(n)}{n|\sin(n)|}$$ Loosely speaking, the sign changes every $\pi$ terms. I'd be surprised if it doesn't converge. Wolfram Mathematica, after a couple of minutes of computing, concluded the series diverges but I can't really trust it. My first approach (assuming the series converges) was that if we bundle up terms with the same sign like the example above every bundle must have three or four terms, and since the first three terms of all bundles make an alternating series I was going to fiddle with the remaining fourth terms but they don't make an alternating series so I guess there's no point in this approach. edit: I don't think we can use Dirichlet's test with $$b_n=\textrm{sgn}(\sin(n)).$$ The alternating cycle here is $\pi$ and I don't believe it would bound the series. For example if the cycle was a number very slightly smaller than $3+1/4$, then $B_n$ (sum of $b_n$) would get larger and larger every four bundles for some time. I believe this should happen for $\pi$ as well since it is irrational. I'm not entirely sure why but $|B_n|\leq3$ for most small $n$ though I guess it's because $\pi-3$ is slightly smaller than $1/7$? Anyway $B_{312\ 692}=4$, $B_{625\ 381}=5$, $B_{938\ 070}=6$, $B_{166\ 645\ 135}=-7$, and $B_{824\ 054\ 044}=8$. $|B_n|$ does not hit $9$ up to $n=1\ 000\ 000\ 000$ with $B_{1\ 000\ 000\ 000}=-2$.	sequences-and-series
A.288	Alternación de Spin-off de la Serie Armónica	Sabemos que la serie $\sum (-1)^n/n$ converge, y claramente todas las otras series armónicas alternativas con el signo cambiando cada dos o más términos como $$\left(1+\frac{1}{2}+\frac{1}{3}\right)-\left(\frac{1}{4}+\frac{1}{5}+\frac{1}{6}\right)+\left(\frac{1}{7}+\frac{1}{8}+\frac{1}{9}\right)-\cdots$$ deben converger. Mi pregunta aquí es si la serie de abajo también converge? $$\sum\frac{\textrm{sgn}(\sin(n))}{n}\quad\textrm{or}\quad\sum\frac{\sin(n)}{n|\sin(n)|}$$ De manera relajada, la señal cambia cada $\pi$ términos. Me sorprendería si no converge. Wolfram Mathematica, después de un par de minutos de cálculo, concluyó que la serie diverge pero no puedo confiar en él. Mi primer enfoque (suponiendo que las series convergen) fue que si juntamos términos con el mismo signo como el ejemplo de arriba de cada paquete debe tener tres o cuatro términos, y ya que los tres primeros términos de todos los paquetes hacen una serie alternada iba a jugar con los cuatro términos restantes pero no suman una serie alternada así que supongo que el punto en este enfoque. editor: No creo que podamos usar la prueba de Dirichlet con $$b_n=\textrm{sgn}(\sin(n)).$$ El ciclo alterno aquí es $\pi$ y no creo que se limite las series. Por ejemplo, si el ciclo fuera un número ligeramente menor que $3+1/4$, entonces $B_n$ (suma de $b_n$) aumentaría cada vez más cada cuatro paquetes durante algún tiempo. Creo que esto también debería suceder con $\pi$, ya que es irracional. No estoy del todo seguro de por qué, pero $|B_n|\leq3$ para la mayoría de los $n$ pequeños, aunque supongo que es porque $\pi-3$ es un poco más pequeño que $1/7$. De todos modos $B_{312\ 692}=4$, $B_{625\ 381}=5$, $B_{938\ 070}=6$, $B_{166\ 645\ 135}=-7$ y $ B_{824\ 054\ 044}=8$. $|B_n|$ no llega a $9$ hasta $n=1\ 000\ 000\ 000$ con $B_{1\ 000\ 000\ 000}=-2$.	sequences-and-series
A.289	Find $x^n+y^n+z^n$ general solution	If we know $$x+y+z=1$$$$x^2+y^2+z^2=2$$$$x^3+y^3+z^3=3$$ Is it possible to calculate the general solution for $a_n=x^n+y^n+z^n$? I know $a_5=6$ but the way to get it is more an algorithm than an actual solution.	sequences-and-series
A.289	Encuentra la solución general $x^n+y^n+z^n$	Si sabemos $$x+y+z=1$$$$x^2+y^2+z^2=2$$$$x^3+y^3+z^3=3$$ ¿Es posible calcular la solución general para $a_n=x^n+y^n+z^n$? Sé $a_5=6$ pero la forma de obtenerlo es más un algoritmo que una solución real.	sequences-and-series
A.290	Best method for proving that $7\times11^{2n+1}-3^{4n-1}$ is divisible by $10$	I am asked to prove by induction that $7\times11^{2n+1}-3^{4n-1}$ is divisible by $10$. I wonder whether there is a more direct method, for example factorizing by $10$. If an expression is divisible by $10$, does this mean that I can factorize it by $10$? Thanks in advance	sequences-and-series,elementary-number-theory,divisibility
A.290	Mejor método para demostrar que $7\times11^{2n+1}-3^{4n-1}$ es divisible por $10$	Me piden que demuestre por inducción que $7\times11^{2n+1}-3^{4n-1}$ es divisible por $10$. Me pregunto si hay un método más directo, por ejemplo factorizando por $10$. Si una expresión es divisible por $10$, ¿esto significa que puedo factorizarla por $10$? Gracias por adelantado	sequences-and-series,elementary-number-theory,divisibility
A.291	Summation of $n$th partial products of the square of even numbers diverges, but for odd numbers they converge in this series I'm looking at. Why?	So I have the two following series: $$\sum_{n=1}^\infty \frac{\prod_{k=1}^n(2k)^2}{(2n+2)!}$$ $$\sum_{n=0}^\infty \frac{\prod_{k=0}^n(2k+1)^2}{(2n+3)!}$$ I figured out the $n$th partial products: $$\prod_{k=1}^n(2k)^2=4^n(n!)^2$$ $$\prod_{k=0}^n (2k+1)^2=\frac{((2n+1)!)^2}{4^n(n!)^2}$$ So putting these back into my series they become the following: $$\sum_{n=1}^\infty \frac{\prod_{k=1}^n(2k)^2}{(2n+2)!}=\sum_{n=1}^\infty\frac{4^n(n!)^2}{(2n+2)!}$$ Now this diverges as expected by the limit test test. However when I look at my other series: $$\sum_{n=0}^\infty \frac{\prod_{k=0}^n(2k+1)^2}{(2n+3)!}=\sum_{n=0}^\infty\frac{( (2n+1)!)^2}{4^n(n!)^2(2n+3)!}$$ By the limit test maybe diverges or maybe doesn't, and the ratio test is inconclusive. Since I wasn't sure what to use for the a comparison test I threw this into wolfram alpha and it told me it converges which is baffling to me since both series are very similar if we write them out: $$\sum_{n=1}^\infty \frac{\prod_{k=1}^n(2k)^2}{(2n+2)!}=\frac{2^2}{4!}+\frac{2^24^2}{6!}+\frac{2^24^26^2}{8!}\cdot\cdot\cdot\cdot$$ $$\sum_{n=0}^\infty \frac{\prod_{k=0}^n(2k+1)^2}{(2n+3)!}=\frac{1^2}{3!}+\frac{1^23^2}{5!}+\frac{1^23^25^2}{7!}+\cdot\cdot\cdot$$ They both have the nth parial product of the even/odd integers squared in the numerator, and are over a factorial that is two greater than $n$, so I'm not sure why one is diverging and the other is converging. Is wolframalpha wrong, as it can be at times? Or is there someething here that I am missing?	sequences-and-series,factorial,products
A.291	La suma de los productos parciales del cuadrado de números pares difiere, pero para números impares convergen en esta serie que estoy viendo. ¿Por qué?	Así que tengo las dos siguientes series: $$\sum_{n=1}^\infty \frac{\prod_{k=1}^n(2k)^2}{(2n+2)!}$$ $$\sum_{n=0}^\infty \frac{\prod_{k=0}^n(2k+1)^2}{(2n+3)!}$$ Yo descubrí el $n$o producto parcial: $$\prod_{k=1}^n(2k)^2=4^n(n!)^2$$ $$\prod_{k=0}^n (2k+1)^2=\frac{((2n+1)!)^2}{4^n(n!)^2}$$ Así que poniendo estos de nuevo en mi serie se convierten en lo siguiente: $$\sum_{n=1}^\infty \frac{\prod_{k=1}^n(2k)^2}{(2n+2)!}=\sum_{n=1}^\infty\frac{4^n(n!)^2}{(2n+2)!}$$ Ahora esto diverge como se esperaba por la prueba de límite. Sin embargo, cuando miro a mi otra serie: $$\sum_{n=0}^\infty \frac{\prod_{k=0}^n(2k+1)^2}{(2n+3)!}=\sum_{n=0}^\infty\frac{( (2n+1)!)^2}{4^n(n!)^2(2n+3)!}$$ Por la prueba de límite tal vez diverge o tal vez no, y la prueba de relación es inconclusiva. Ya que no estaba seguro de qué usar para la prueba de comparación lancé esto en wolfram alfa y me dijo que converge que me desconcertó ya que ambas series son muy similares si las escribimos: $$\sum_{n=1}^\infty \frac{\prod_{k=1}^n(2k)^2}{(2n+2)!}=\frac{2^2}{4!}+\frac{2^24^2}{6!}+\frac{2^24^26^2}{8!}\cdot\cdot\cdot\cdot$$ $$\sum_{n=0}^\infty \frac{\prod_{k=0}^n(2k+1)^2}{(2n+3)!}=\frac{1^2}{3!}+\frac{1^23^2}{5!}+\frac{1^23^25^2}{7!}+\cdot\cdot\cdot$$ ambos tienen el n pro-producto parial de los enteros pares / raros cuadrado en el numerador, y están sobre un factor que es dos más grande que $n$, así que no estoy seguro de por qué uno está divergiendo y el otro esta convergiendo. ¿Está Wolframalpha equivocado, como puede estarlo a veces? ¿O hay algo aquí que me falta?	sequences-and-series,factorial,products
A.292	Find the limit of the sequence $ \frac{x^n}{n^k}$ as $n \to \infty$ for all values of$ x > $0 and $k = 1, 2,\cdots$	I have tried using the ratio lemma to tackle this question and also the fact $(n+1)^k \geq 1 + nk$ and I haven't reached an answer. How should I go about solving this problem?	sequences-and-series,limits,analysis,exponentiation,ratio
A.292	Encuentra el límite de la secuencia $ \frac{x^n}{n^k}$ como $n \to \infty$ para todos los valores de $ x > $0 y $k = 1, 2,\cdots$	He intentado utilizar el lema de la relación para abordar esta pregunta y también el hecho $(n+1)^k \geq 1 + nk$ y no he llegado a una respuesta. ¿Cómo debo resolver este problema?	sequences-and-series,limits,analysis,exponentiation,ratio
A.293	Summation $\sum_{j=2}^{n-1}j^2$ Properties	I'm dealing with something like $\sum_{j=2}^{n-1}j^2$. I know I can do this $\sum_{j=1}^{n-1}j^2 - \sum_{j=1}^{1}j^2$. Would that be equal to $\frac{j(j+1)(2j+1)}{6} - j^2$ or I'm missing some properties with $n-1$? If so, which ones?	sequences-and-series,summation
A.293	Sumatoria $\sum_{j=2}^{n-1}j^2$ Propiedades	Estoy tratando con algo como $\sum_{j=2}^{n-1}j^2$. sé que puedo hacer esto $\sum_{j=1}^{n-1}j^2 - \sum_{j=1}^{1}j^2$. ¿Eso es igual a $\frac{j(j+1)(2j+1)}{6} - j^2$ o me faltan algunas propiedades con $n-1$? ¿Si es asi, cuales?	sequences-and-series,summation
A.294	Why is there no hyper-hypercohomology?	I am looking for a reference to answer the question in the title. Let me try to clarify a little what I mean: If a single sheaf $\mathscr F$ has a resolution $\mathscr G^\bullet$ by not necessarily injective objects, then the usual cohomology of $\mathscr F$ is isomorphic to the hypercohomology of $\mathscr G^\bullet$:  $$ H^i(X, \mathscr F) \cong \mathbb H^i(X,\mathscr G^\bullet). $$ Now, if one was starting with a complex of sheaves $\mathscr F^\bullet$ and a "resolution" thereof, i.e. a complex of complexes $(\mathscr G^\bullet)^\bullet$, then one should touch on a concept that could be called hyper-hypercohomology.  Yet, I never heard of its existence and I'm pretty sure it does not give you anything new, as soon as you work in the derived category. I just find myself unable to pin down why exactly this is the case.  Any ideas anyone?	sheaf-cohomology,derived-categories
A.294	¿Por qué no hay hiper-hipercohomología?	Busco una referencia para responder a la pregunta en el título. Déjame tratar de aclarar un poco lo que quiero decir: si una sola casilla $\mathscr F$ tiene una resolución $\mathscr G^\bullet$ por objetos no necesariamente inyectivos, entonces la cohomología habitual de $\mathscr F$ es isomórfica a la hipercohomología de $\mathscr G^\bullet$: $$ H^i(X, \mathscr F) \cong \mathbb H^i(X,\mathscr G^\bullet). $$ Ahora bien, si uno comenzó con un complejo de casillas $\mathscr F^\bullet$ y una "resolución" de la misma, es decir, un complejo de complejos $(\mathscr G^\bullet)^\bullet$, entonces uno debería tocar un concepto que podría llamarse hiper-hipercohomología. Sin embargo, nunca he oído hablar de su existencia y estoy bastante seguro de que no le da nada nuevo, ya que usted trabaja en la categoría derivada. Simplemente me encuentro incapaz de precisar por qué exactamente este es el caso. ¿Alguien tiene alguna idea?	sheaf-cohomology,derived-categories
A.295	Help calculating series	I need help with understanding how to solve this task, because I'm a bit lost at the moment. Use the powerseries  $$f(x)=\frac{1}{1-x}$$ to decide the sum of the series  $\sum_{n=1}^{\infty} n(n+1)x^n$    and $\sum_{n=1}^{\infty} \frac{n(n+1)}{3^n}$ I don't understand how to manipulate the sums to use the power series of the function.	summation,power-series
A.295	Ayuda para calcular series	Necesito ayuda para entender cómo resolver esta tarea, porque estoy un poco perdido en este momento. Usa la serie de potencias $$f(x)=\frac{1}{1-x}$$ para decidir la suma de la serie $\sum_{n=1}^{\infty} n(n+1)x^ n$ y $\sum_{n=1}^{\infty} \frac{n(n+1)}{3^n}$ No entiendo cómo manipular las sumas para usar la serie de potencias de la función.	summation,power-series
A.296	A little confused about the Taylor series of $e^x$	We know that $$ e^x=\sum_{n=0}^{\infty}\frac{x^n}{n!},x\in \mathbb R, $$ which can be written out $$ e^x=\frac {x^0}{0!}+\frac {x^1}{1!}+\frac {x^2}{2!}+\cdots, $$ but $0^0$ isn't well defined.	taylor-expansion,exponential-function
A.296	Un poco confundido con la serie de Taylor de $e^x$	Sabemos que $$ e^x=\sum_{n=0}^{\infty}\frac{x^n}{n!},x\in \mathbb R, $$ que puede ser escrito fuera $$ e^x=\frac {x^0}{0!}+\frac {x^1}{1!}+\frac {x^2}{2!}+\cdots, $$ pero $0^0$ no está bien definido.	taylor-expansion,exponential-function
A.297	difficult question	I am confused about a homework problem I have, and don't really know where to begin. I need to prove this. Any idea of where I can start. The statement is that Find all integers n that satisfies $\phi(n)=320$ where $\phi$ is the Euler’s Phi function.	totient-function
A.297	pregunta difícil	Estoy confundido sobre un problema de tarea que tengo, y no sé realmente por dónde empezar. Necesito probar esto. Cualquier idea de dónde puedo comenzar. La declaración es que encontrar todos los enteros n que satisface $\phi(n)=320$ donde $\phi$ es la función Euler  Phi.	totient-function
A.298	Variable transformation of a Dirac delta function	I am struggling to understand the variable transformation of a Dirac delta function. More specifically, a transformation of the following type, $$\delta(a\chi(z)-b) \rightarrow \delta(z-c)$$ Here, $a, b$ and $c$ are constants. The specific relationship between $\chi(z)$ and $z$ is, $$\chi(z)=\int{\frac{1}{H(z)}dz}$$ where $H(z)$ is a non-zero, positive and smoothly varying function of $z$. In the context of my Physics problem, for the sake of the interested audience, $H(z)$ is the Hubble parameter of the Universe while $\chi(z)$ is the comoving distance and $z$ is the redshift of any time in the past. So, let me add what I have done so far:  I start by defining the Dirac delta function in the form a unit step function $\Theta (a\chi(z)-b)$as, $$\frac{d}{d(a\chi(z))}\Theta(a\chi(z)-b)=\delta(a\chi(z)-b)$$ Then converting this in the form of z using the chain rule as, $$\frac{d}{dz}\Theta(a\chi(z)-b)\frac{dz}{d(a\chi(z))}$$ Using the relation with $H(z)$, we can write the equation as, $$\frac{d}{dz}\Theta(a\chi(z)-b)\frac{H(z)}{a}$$ Also, $\chi(z)$ can also be replaced by the integral as well, so that left side containing the unit step function can be written as, $$\frac{H(z)}{a}\frac{d}{dz}\Theta(\ a\int_0^z{\frac{1}{H(z')}dz'}-b)$$. After this, I am not sure what else to try. Hopefully, this addition helps. Also, please point out an error if you see one.	transformation,dirac-delta,change-of-variable
A.298	Transformación de variable de una función delta Dirac	Estoy luchando por comprender la transformación variable de una función delta de Dirac. Más específicamente, una transformación del siguiente tipo, $$\delta(a\chi(z)-b) \rightarrow \delta(z-c)$$ Aquí, $a, b$ y $c$ son constantes. La relación específica entre $\chi(z)$ y $z$ es, $$\chi(z)=\int{\frac{1}{H(z)}dz}$$ donde $H(z)$ es una función distinta de cero, positiva y que varía suavemente de $z$. En el contexto de mi problema de Física, por el bien de la audiencia interesada, $H(z)$ es el parámetro de Hubble del Universo mientras que $\chi(z)$ es la distancia comoving y $z$ es el corrimiento al rojo de cualquier tiempo en el pasado. Entonces, permítanme agregar lo que he hecho hasta ahora: comienzo definiendo la función delta de Dirac en la forma de una función de paso unitario $\Theta (a\chi(z)-b)$as, $$\frac{d} {d(a\chi(z))}\Theta(a\chi(z)-b)=\delta(a\chi(z)-b)$$ Luego, convierte esto en la forma de z usando la regla de la cadena como, $$\frac{d}{dz}\Theta(a\chi(z)-b)\frac{dz}{d(a\chi(z))}$$ Usando la relación con $H(z )$, podemos escribir la ecuación como, $$\frac{d}{dz}\Theta(a\chi(z)-b)\frac{H(z)}{a}$$ Además, $\chi (z)$ también se puede reemplazar por la integral, de modo que el lado izquierdo que contiene la función de paso unitario se puede escribir como $$\frac{H(z)}{a}\frac{d}{dz}\ Theta(\ a\int_0^z{\frac{1}{H(z')}dz'}-b)$$. Después de esto, no estoy seguro de qué más probar. Con suerte, esta adición ayuda. Además, indique un error si ve uno.	transformation,dirac-delta,change-of-variable
A.299	Prove $\cos \frac{\pi}5-\cos \frac{2 \pi}5=\frac12$ but without finding $\cos \frac{ \pi}5$	I can find the value of $\cos \left(\frac{ \pi}{5}\right)$, but is there a way to prove the equality without finding it? I tried looking for both algebraic and geometric methods, but couldn't find anything	trigonometry,proof-writing,alternative-proof
A.299	Prueba $\cos \frac{\pi}5-\cos \frac{2 \pi}5=\frac12$ pero sin encontrar $\cos \frac{ \pi}5$	Puedo encontrar el valor de $\cos \left(\frac{ \pi}{5}\right)$, pero ¿hay una manera de probar la igualdad sin encontrarlo? Traté de buscar métodos algebraicos y geométricos, pero no pude encontrar nada	trigonometry,proof-writing,alternative-proof
A.300	Uniformly continuous or not?	So I supposed to find out if $$f(x)=\frac{1}{1+\ln^2 x}$$ is uniformly continuous on $I=(0,\infty)$ So I have been thinking a lot. Could I say that $f$ is continuous on $[0,1]$ and therefore uniformly continuous here? Or is this not valid, because $\ln$ is not defined at $x=0$? And then say that the derivate is bounded at $[1,\infty]$?	uniform-continuity
A.300	¿Uniforme continuo o no?	Así que se supone que debo averiguar si $$f(x)=\frac{1}{1+\ln^2 x}$$ es uniformemente continuo en $I=(0,\infty)$ Así que he estado pensando mucho. ¿Podría decir que $f$ es continuo en $[0,1]$ y por lo tanto uniformemente continuo aquí? ¿O esto no es válido, porque $\ln$ no se define en $x=0$? ¿Y luego decir que la derivada está acotada en $[1,\infty]$?	uniform-continuity
A.301	Inequality between norm 1,norm 2 and norm $\infty$ of Matrices	Suppose $A$ is a $m\times n$ matrix. Then Prove that, $\begin{equation*} \|A\|_2\leq \sqrt{\|A\|_1 \|A\|_{\infty}} \end{equation*}$ I have proved the following relations: $\begin{align*} \frac{1}{\sqrt{n}}\|A\|_{\infty}\leq \|A\|_2\leq\sqrt{m}\|A\|_{\infty}\\ \frac{1}{\sqrt{m}}\|A\|_{1}\leq \|A\|_2\leq\sqrt{n}\|A\|_{1} \end{align*}$ Also I feel that somehow Holder's inequality for the special case when $p=1$ and $q=\infty$ might be useful.But I couldn't prove that. Edit: I would like to have a prove that do not use the information that $\|A\|_2=\sqrt{\rho(A^TA)}$ Usage of inequalities like Cauchy Schwartz or Holder is fine.	linear-algebra,matrices,inequality,norm,holder-inequality
A.301	Desigualdad entre la norma 1, la norma 2 y la norma $\infty$ de las matrices	Supongamos que $A$ es una matriz $m\times n$. Entonces prueba que, $\begin{equation*} \|A\|_2\leq \sqrt{\|A\|_1 \|A\|_{\infty}} \end{equation*}$ he probado las siguientes relaciones: $\begin{align*} \frac{1}{\sqrt{n}}\|A\|_{\infty}\leq \|A\|_2\leq\sqrt{m}\|A\|_{\infty}\\ \frac{1}{\sqrt{m}}\|A\|_{1}\leq \|A\|_2\leq\sqrt{n}\|A\|_{1} \end{align*}$ También siento que de alguna manera la desigualdad de Holder para el caso especial cuando $p=1$ y $q=\infty$ podría ser útil.Pero no pude probar eso. Edit: Me gustaría tener una prueba de que no se utiliza la información de que $\|A\|_2=\sqrt{\rho(A^TA)}$ El uso de desigualdades como Cauchy Schwartz o Holder está bien.	linear-algebra,matrices,inequality,norm,holder-inequality
A.302	$n$-th root of a complex number	I am confused about the following problem. With $w=se^{i{\phi}}$, where $s\ge 0$ and $\phi \in \mathbb{R}$, solve the equation $z^n=w$ in $\mathbb{C}$ where $n$ is a natural number. How many solutions are there? Now my approach is simply taking the $n$-th root which gives $$z=\sqrt[n]{s}e^{\frac{i\varphi}{n}}$$ However, it seems that this problem is asking us to show the existance of the $n$-th root. Can I assume that the $n$-th root of a complex number already exists? Moreoover, would I be correct to say that there is only one solution which is given above?	real-analysis,calculus,complex-analysis,analysis
A.302	$n$ raíz de un número complejo	Estoy confundido sobre el siguiente problema. con $w=se^{i{\phi}}$, donde $s\ge 0$ y $\phi \in \mathbb{R}$, resolver la ecuación $z^n=w$ en $\mathbb{C}$ donde $n$ es un número natural. ¿Cuántas soluciones hay? Ahora mi enfoque es simplemente tomar la raíz $n$ que da $$z=\sqrt[n]{s}e^{\frac{i\varphi}{n}}$$ Sin embargo, parece que este problema nos está pidiendo que demuestren la existencia de la raíz $n$. ¿Puedo asumir que la raíz $n$ de un número complejo ya existe? Además, sería correcto decir que sólo hay una solución que se da arriba?	real-analysis,calculus,complex-analysis,analysis
A.303	Number of non-commutative Lie algebras of dimension 2	Theorem- Up to isomorphism, the only noncommutative Lie algebra of dimension 2 is that with basis $x , y$ and bracket determined by $[x,y] = x$. I understand that all vector spaces of dimension 2 over the field $K$ are isomorphic to each other. So the number of lie algebras of dimension 2 in a field $K$ is determined by the number of possible bilinear operations [ ]$:\ V \ X \ V  \rightarrow V$ satisfying the conditions $a)$ $[x,x]=0$ for all $x\in V$ $b)$ $[x,[y,z]]+[y,[z,x]]+[z,[x,y]]=0$ for all $x,y,z \in V$ The bilinear operations on the other hand is determined by the elements to which the pair of base elements are mapped to in the bilinear operation. And since in a lie algebra $[x,x]=[y,y]=0$ and $[x,y]=-[x,y]$ we ony need to determine $[x,y]$. Now how do we prove that $[x,y]=x$ and $[y,x]=-x$ always and why can't it be [y,x]=y or any other vector ?	lie-algebras,noncommutative-algebra
A.303	Número de álgebra de Lie no commutativa de dimensión 2	Teorema- Hasta el isomorfismo, el único álgebra de Lie no commutativa de dimensión 2 es que con base $x , y$ y corchilla determinada por $[x,y] = x$. Entiendo que todos los espacios vectoriales de dimensión 2 sobre el campo $K$ son isomorfos entre sí. Así que el número de álgebra se encuentra en la dimensión 2 en un campo $K$ se determina por el número de posibles operaciones bilineares [ ]$:\ V \ X \ V  \rightarrow V$ satisfaciendo las condiciones $a)$ $[x,x]=0$ para todos los $x\in V$ $b)$ $[x,[y,z]]+[y,[z,x]]+[z,[x,y]]=0$ para todos los $x , y$0 Las operaciones bilineares por el otro lado se determinan por los elementos a los que se cartografian el par de elementos base en la operación bilinear. Y dado que en una álgebra $x , y$1 y $x , y$2 necesitamos determinar $x , y$3. Ahora ¿cómo probamos que  $[x,y]=x$ y $[y,x]=-x$  siempre y por qué no puede ser [yx, y] cualquier otro vector?	lie-algebras,noncommutative-algebra
A.304	Elementary proof that a non-orientable manifold of real dimension $2$ does not admit a quasi-complex structure.	Is there an easy proof that a non-orientable real surface $X$ does not admit a quasi-complex structure? The proof I know is to observe that any quasi-complex structure on a real surface $X$ necessarily satisfies the integrability condition $$[T_X^{0,1}, T_X^{0,1}] \subset T_X^{0,1}$$ of the Newlander-Nirenberg theorem, because $T_X^{0,1}$ is a $1$-dimensional complex vector bundle, and the bracket $[-,-]$ is alternating, i.e. it vanishes on $T_X^{0,1}$. So by the Newlander-Nirenberg theorem, $X$ admits a complex structure, and complex manifolds have to be orientable. However, the Newlander-Nirenberg theorem is a deep theorem and feels a bit overkill. Also I don't really see why there cannot be a quasi-complex structure. Is there a more elementary proof to convince myself?	algebraic-geometry,reference-request,complex-geometry
A.304	Prueba elemental de que un variedad no orientable de dimensión real $2$ no admite una estructura cuasi compleja.	¿Existe una prueba fácil de que una superficie real no orientable $X$ no admite una estructura cuasicompleja? La prueba que conozco es observar que cualquier estructura cuasicompleja en una superficie real $X$ satisface necesariamente la condición de integrabilidad $$[T_X^{0,1}, T_X^{0,1}] \subset T_X^{0,1}$$ del teorema de Newlander-Nirenberg, porque $T_X^{0,1}$ es un conjunto vectorial complejo de $1$ dimensiones, y el bracket $[-,-]$ está alternando, es decir, desaparece en $T_X^{0,1}$. Entonces, según el teorema de Newlander-Nirenberg, $X$ admite una estructura compleja, y las variedades complejas tienen que ser orientables. Sin embargo, el teorema de Newlander-Nirenberg es un teorema profundo y parece un poco exagerado. Además, no veo por qué no puede haber una estructura cuasi compleja. ¿Existe alguna prueba más elemental para convencerme?	algebraic-geometry,reference-request,complex-geometry
A.305	What will be the value of floor function of $\lim\limits_{N\to\infty}\left\lfloor\sum\limits_{r=1}^N\frac{1}{2^r}\right\rfloor$	What would be the value of floor function of $\lim\limits_{N\to\infty}\left\lfloor\sum\limits_{r=1}^N\frac{1}{2^r}\right\rfloor$ would it be $1$ or would it be $0$ ? The formula I use for this is that of infinite summation series that is $\frac{a}{1-r}$ but I have no clue how to find out what the floor value of the above expression would be. P.s I am a high school student so please explain in simple terms, and yes I do know basic calculus. EDIT: I'm sorry it was given $\lim_{N \to \infty}$ in the problem	summation
A.305	Cuál será el valor de la función piso de $\lim\limits_{N\to\infty}\left\lfloor\sum\limits_{r=1}^N\frac{1}{2^r}\right\rfloor$	¿Cuál sería el valor de la función de piso de $\lim\limits_{N\to\infty}\left\lfloor\sum\limits_{r=1}^N\frac{1}{2^r}\right\rfloor$ sería $1$ o sería $0$? La fórmula que uso para esto es la de la serie de sumación infinita que es $\frac{a}{1-r}$ pero no tengo idea de cómo averiguar el valor de piso de la expresión anterior sería. P.s Soy estudiante de secundaria así que por favor expliquen en términos simples, y sí, sé cálculo básico. EDIT: Lamento que se haya dado $\lim_{N \to \infty}$ en el problema	summation
A.306	On the irrationality of Euler Mascheroni constant	I saw one of the expansions of Euler Mascheroni constant in terms of Meissel Mertens constant as a consequence of Mertens theorem. $$ B = \gamma + \sum_p \left\{ \log\left( 1 - \frac 1p\right) + \frac 1p\right\}$$ This is that expansion. Now I don't understand why is it difficult to prove the irrationality of Euler Mascheroni constant. Since we have infinitely many prime numbers, the sum over all those primes in the above equation, if converges must be a irrational, then why is it not considered as a proof of irrationality?	irrational-numbers,euler-mascheroni-constant
A.306	Sobre la irracionalidad de la constante de Euler Mascheroni	Vi una de las expansiones de la constante de Euler Mascheroni en términos de constante de Meissel Mertens como consecuencia del teorema de Mertens. $$ B = \gamma + \sum_p \left\{ \log\left( 1 - \frac 1p\right) + \frac 1p\right\}$$ Esta es esa expansión. Ahora no entiendo por qué es difícil probar la irracionalidad de la constante de Euler Mascheroni. Dado que tenemos infinitos números primos, la suma sobre todos esos números primos en la ecuación anterior, si convergencias deben ser una irracionalidad, entonces ¿por qué no se considera como una prueba de irracionalidad?	irrational-numbers,euler-mascheroni-constant
A.307	Carmichael function and the largest multiplicative order modulo n	By definition, the Carmichael function maps$a $positive integer $n$ to the smallest positive integer $t$ such that $a^t\equiv1\pmod n$ for all integers $a$ with $\gcd(a,n)=1$. It is denoted as $\lambda(n)$. The Wikipedia page on Carmichael function states that $\lambda(n)=\max\{\operatorname{ord}_n(a):\gcd(a,n)=1\}$. My question is: why is this true? In other words, why is it the case that there always exists an integer $x$ coprime to $n$ with $\operatorname{ord}_n(x)=\lambda(n)$?	elementary-number-theory,carmichael-function
A.307	Función Carmichael y el mayor orden multiplicativo modulo n	Por definición, la función Carmichael mapea el número entero positivo $n$ al número entero positivo más pequeño $t$ de tal manera que $a^t\equiv1\pmod n$ para todos los números enteros $a$ con $\gcd(a,n)=1$. Se denota como $\lambda(n)$. La página de Wikipedia sobre la función Carmichael afirma que $\lambda(n)=\max\{\operatorname{ord}_n(a):\gcd(a,n)=1\}$. Mi pregunta es: ¿por qué es esto cierto? En otras palabras, ¿por qué siempre existe un número entero $x$ coprimo a $n$ con $\operatorname{ord}_n(x)=\lambda(n)$?	elementary-number-theory,carmichael-function
A.308	Riemann's definition of the zeta function	I am having trouble understanding Riemann's definition of the zeta function, and I will need to give a brief summary here before I can get to my question. In his 1859 paper, Riemann derived the integral representation $$\zeta(s)=\sum_{n=1}^\infty\frac{1}{n^s}=\frac{1}{\Gamma(s)}\int_0^\infty \frac{x^{s-1}}{e^x-1}dx$$ that is valid for $\mbox{Re}(s)\gt 1$, and then modified the integral in order to define a function that is defined for all complex values of $s$, except $s=1$, where it has a simple pole. The extension is given by $$\zeta(s)=\frac{\Gamma(1-s)}{2\pi i}\int_C \frac{(-z)^s}{e^z-1}\frac{dz}{z}$$ where $C$ is a "Hankel contour", that is, a path that travels from $+\infty $ at a small distance $\epsilon$ above the positive $x$-axis, circles around the origin once in counterclockwise direction with a small radius $\delta$, and returns to $+\infty$ traveling at distance $\epsilon$ below the positive real axis. Taking the limit as $\epsilon\rightarrow 0$ and $\delta \rightarrow 0$ one can see that the integral $$\int_C \frac{(-z)^s}{e^z-1}\frac{dz}{z}$$ becomes $$(e^{i\pi s}-e^{-i\pi s})\int_0^\infty\frac{x^{s-1}}{e^x-1}dx$$ and then the rest follows easily from known identities satisfied by the Gamma function.  While the original real integral over $[0,\infty)$ is clearly divergent if $\mbox{Re}(s)\leq 1$, the contour integral over $C$ is defined for all complex $s$, because the path stays away from the singularity at $s=0$ and from the branch cut along the positive $x$-axis. My problem is understanding why the integral over $C$ does not depend on $\epsilon$ and $\delta$, so that we can keep them at a safe positive distance from the singularities for the definition, but we can take the limit for the purpose of evaluating the integral. I know that by Cauchy's theorem we can modify a path of integration (without changing the value of the integral) starting and ending at the same point as long as we do not cross any singularity, but this path starts and ends at infinity, so I am not sure how to rigorously proceed using Cauchy's theorem. Even if I start the path at $R+i\epsilon$ and end it at $R-i\epsilon$ for some large $R$, the path starting and ending points change as $\epsilon$ changes.	contour-integration,riemann-zeta
A.308	La definición de la función zeta de Riemann	Tengo problemas para comprender la definición de Riemann de la función zeta y tendré que hacer un breve resumen aquí antes de poder responder a mi pregunta. En su artículo de 1859, Riemann derivó la representación integral $$\zeta(s)=\sum_{n=1}^\infty\frac{1}{n^s}=\frac{1}{\Gamma(s) }\int_0^\infty \frac{x^{s-1}}{e^x-1}dx$$ que es válido para $\mbox{Re}(s)\gt 1$, y luego modificó la integral para definir una función que esté definida para todos los valores complejos de $s$, excepto $s=1$, donde tiene un polo simple. La extensión viene dada por $$\zeta(s)=\frac{\Gamma(1-s)}{2\pi i}\int_C \frac{(-z)^s}{e^z-1}\ frac{dz}{z}$$ donde $C$ es un "contorno de Hankel", es decir, un camino que viaja desde $+\infty $ a una pequeña distancia $\epsilon$ por encima del eje positivo $x$, gira alrededor del origen una vez en sentido antihorario con un radio pequeño $\delta$, y regresa a $+\infty$ viajando a una distancia $\epsilon$ por debajo del eje real positivo. Tomando el límite como $\epsilon\rightarrow 0$ y $\delta \rightarrow 0$ se puede ver que la integral $$\int_C \frac{(-z)^s}{e^z-1}\frac{dz }{z}$$ se convierte en $$(e^{i\pi s}-e^{-i\pi s})\int_0^\infty\frac{x^{s-1}}{e^x- 1}dx$$ y luego el resto se deduce fácilmente de identidades conocidas satisfechas por la función Gamma. Mientras que la integral real original sobre $[0,\infty)$ es claramente divergente si $\mbox{Re}(s)\leq 1$, la integral de contorno sobre $C$ se define para todos los $s$ complejos, porque El camino se mantiene alejado de la singularidad en $s=0$ y de la rama cortada a lo largo del eje positivo $x$. Mi problema es entender por qué la integral sobre $C$ no depende de $\epsilon$ y $\delta$, de modo que podamos mantenerlos a una distancia positiva segura de las singularidades para la definición, pero podemos tomar el límite para el propósito de evaluar la integral. Sé que por el teorema de Cauchy podemos modificar un camino de integración (sin cambiar el valor de la integral) comenzando y terminando en el mismo punto siempre que no crucemos ninguna singularidad, pero este camino comienza y termina en el infinito, así que No estoy seguro de cómo proceder rigurosamente utilizando el teorema de Cauchy. Incluso si comienzo el camino en $R+i\epsilon$ y lo termino en $R-i\epsilon$ para un $R$ grande, los puntos inicial y final del camino cambian a medida que cambia $\epsilon$.	contour-integration,riemann-zeta
A.309	Number of solutions of equation over a finite field	I have a question regarding the number of solutions of a equation over a finite field $\mathbb{F}_p$. First of all, consider the equation $x^3=a$ over $\mathbb{F}_p$, where $p$ is a prime such that $p\equiv 2 (\text{mod }3)$. The book that I'm currently reading says that this equation has exactly one solution in $\mathbb{F}_p$ for every $a\in \mathbb{F}_p$, because $\gcd(3,p-1)=1$, but the book does not prove this. Unfortunately, this doesn't convince me enough. Is there is a convincing elementary straightforward proof justifying why is this true?	number-theory,elementary-number-theory,finite-fields
A.309	Número de soluciones de la ecuación sobre un campo finito	Tengo una pregunta sobre el número de soluciones de una ecuación sobre un campo finito $\mathbb{F}_p$. En primer lugar, considere la ecuación $x^3=a$ sobre $\mathbb{F}_p$, donde $p$ es un primo tal que $p\equiv 2 (\text{mod }3)$. El libro que estoy leyendo actualmente dice que esta ecuación tiene exactamente una solución en $\mathbb{F}_p$ para cada $a\in \mathbb{F}_p$, porque $\gcd(3,p-1)=1$, pero el libro no prueba esto. Desafortunadamente, esto no me convence lo suficiente. ¿Hay una prueba convincente elemental directa que justifique por qué esto es cierto?	number-theory,elementary-number-theory,finite-fields
A.310	Finding positive integer solutions to $\frac{4}{x}+\frac{10}{y}=1$	Find the positive integer solutions for: $\frac{4}{x} + \frac{10}{y} = 1$  I had calculated the solutions manually but it was a very tedious process. Is there any better way to do this?	elementary-number-theory,solution-verification
A.310	Encontrar soluciones de números enteros positivos a $\frac{4}{x}+\frac{10}{y}=1$	Encuentra las soluciones de números enteros positivos para: $\frac{4}{x} + \frac{10}{y} = 1$ había calculado las soluciones manualmente pero fue un proceso muy tedioso. ¿Hay alguna mejor manera de hacer esto?	elementary-number-theory,solution-verification
A.311	Given a function $f(x)=\frac{9^{x}}{9^x+3}$, what is $f(\frac{1}{27})+ f(\frac{2}{27}) + f(\frac{3}{27})+ ...+ f(\frac{26}{27})$?	While I was going through past Olympiad math papers, I found this question without any explanation. Here is the question:  Given a function $f(x)=\frac{9^{x}}{9^x+3}$, what is $f(\frac{1}{27})+ f(\frac{2}{27}) + f(\frac{3}{27})+ ...+ f(\frac{26}{27})$?  The answer was 13. I took a really bad approach and converted $\frac{9^{x}}{9^x+3}$ to $1+\frac{9^{x}}{3}$, which I then noticed was wrong. I also accidentally multiplied $9^{\frac{1}{27}}$ with $9^{\frac{2}{27}}$, $9^{\frac{2}{27}}$ with $9^{\frac{3}{27}}$, and so on, before realizing that the functions were added and not multiplied. I suspect that there is something to the power of $\frac{n}{27}$, because 9 is a multiple of 27. However, I am not completely sure. Is there a law that tells me how I can solve this question? Since this is a Math Olympiad question, there is probably a maximum time limit of five minutes to do this question. This means that I probably won’t have time for tedious mathematical calculations with a calculator and online tools, or something like that. Please give me a quick, fast solution that is probably suitable for an 8th grader, at most a solution at a 10th grader level.	functions,contest-math,power-series,fractions
A.311	Dada una función $f(x)=\frac{9^{x}}{9^x+3}$, ¿qué es $f(\frac{1}{27})+ f(\frac{2}{27}) + f(\frac{3}{27})+ ...+ f(\frac{26}{27})$?	Mientras revisaba los documentos anteriores de matemáticas de la Olimpiada, encontré esta pregunta sin ninguna explicación. Aquí está la pregunta: Dado una función $f(x)=\frac{9^{x}}{9^x+3}$, ¿qué es $f(\frac{1}{27})+ f(\frac{2}{27}) + f(\frac{3}{27})+ ...+ f(\frac{26}{27})$? La respuesta fue 13. Tomé un enfoque muy malo y convertí $\frac{9^{x}}{9^x+3}$ a $1+\frac{9^{x}}{3}$, lo que luego noté que estaba mal. También accidentalmente multiplicé $9^{\frac{1}{27}}$ con $9^{\frac{2}{27}}$, $9^{\frac{2}{27}}$ con $9^{\frac{3}{27}}$, etc., antes de darme cuenta de que las funciones se agregaron y no se multiplicaron. Sospecho que hay algo con la potencia de $\frac{n}{27}$, porque 9 es un múltiplo de 27. Sin embargo, no estoy completamente seguro. ¿Hay una ley que me diga cómo puedo resolver esta pregunta? Dado que se trata de una pregunta de la Olimpiada de Matemáticas, probablemente haya un límite de tiempo máximo de cinco minutos para responderla. Esto significa que probablemente no tendré tiempo para tediosos cálculos matemáticos con una calculadora y herramientas en línea, o algo así. Por favor, deme una solución rápida que probablemente sea adecuada para un estudiante de octavo grado, como máximo una solución para un nivel de décimo grado.	functions,contest-math,power-series,fractions
A.312	Proving $\left\lfloor \frac{\left\lfloor a/b \right\rfloor}{c} \right\rfloor=\left\lfloor\frac{a}{bc}\right\rfloor$ for positive integer $a$, $b$, $c$	How can we prove the following? $$\left\lfloor \frac{\left\lfloor \dfrac{a}{b} \right\rfloor}{c} \right\rfloor = \left\lfloor \frac{a}{bc} \right\rfloor$$ for $a,b,c \in \mathbb{Z}^+$  I don’t know if I’m doing something wrong, but I can’t prove it even though I’m pretty sure it’s true. Obviously, because the concept of algebra isn’t aware of the fact that we are restricting the variables to positive integers, and given my assumption that the equality doesn’t necessarily hold for non-integers, an element of non-algebraic problem solving is needed, i.e. making a change to the expression given our knowledge of that condition, which then allows for algebraic maneuvers that show that the equality holds. I think that’s what I’m missing. Thanks.	elementary-number-theory,proof-writing,ceiling-and-floor-functions
A.312	Prueba de $\left\lfloor \frac{\left\lfloor a/b \right\rfloor}{c} \right\rfloor=\left\lfloor\frac{a}{bc}\right\rfloor$ para el número entero positivo $a$, $b$, $c$	¿Cómo podemos probar lo siguiente? $$\left\lfloor \frac{\left\lfloor \dfrac{a}{b} \right\rfloor}{c} \right\rfloor = \left\lfloor \frac{a}{bc} \right\rfloor$$ para $a,b,c \in \mathbb{Z}^+$ No sé si estoy haciendo algo mal, pero no puedo probarlo aunque estoy bastante seguro de que es cierto. Obviamente, porque el concepto de álgebra no es consciente del hecho de que estamos restringindo las variables a números enteros positivos, y dado mi suposición de que la igualdad no se aplica necesariamente a los no enteros, se necesita un elemento de resolución de problemas no algebraicos, es decir, hacer un cambio en la expresión dado nuestra conocimiento de esa condición, lo que luego permite maniobras algebraicas que muestren que la igualdad se mantiene. Creo que eso es lo que me falta. Gracias.	elementary-number-theory,proof-writing,ceiling-and-floor-functions
A.313	Let $a,b\in G$, a finite abelian group and $|a|=r, |b|=s$ with $\gcd(r,s)=1$. Prove that $|ab|=rs$.	Let $a,b\in G$, a finite abelian group and $|a|=r, |b|=s$ with $\gcd(r,s)=1$. Prove that $|ab|=rs$.  My attempt: Let $|ab|=n$. Since $G$ is ableian, $(ab)^n=a^nb^n=1$. Thus $r\mid n$ and $s\mid n$. Together with $\gcd(r,s)=1$, it follows that $rs\mid n$. This is where I'm stuck; need to show that $rs=n$. Any hints on how to proceed? Edit: I've come up with a solution that is a somewhat different approach to what has been provided in the hints. Here it goes: Since $G$ is abelian, $n\mid{\rm lcm}(r,s)$. But since $\gcd(r,s)=1$, ${\rm lcm}(r,s)=rs$ by an elementary result in number theory. Thus $n\mid rs$. Together with $rs\mid n$, we have that $n=rs$, which is what we want to prove.	group-theory,finite-groups,abelian-groups
A.313	Dejar que $a,b\in G$, un grupo abeliano finito y $|a|=r, |b|=s$ con $\gcd(r,s)=1$.	Sea $a,b\in G$, un grupo abeliano finito y $|a|=r, |b|=s$ con $\gcd(r,s)=1$. Demuestre que $|ab|=rs$. Mi intento: Deje $|ab|=n$. Dado que $G$ es capaz, $(ab)^n=a^nb^n=1$. Por lo tanto $r\mid n$ y $s\mid n$. Junto con $\gcd(r,s)=1$, se deduce que $rs\mid n$. Aquí es donde estoy estancado; Necesito demostrar que $rs=n$. ¿Alguna pista sobre cómo proceder? Edit: Se me ocurrió una solución que es un enfoque algo diferente a lo que se proporciona en las sugerencias. Aquí va: dado que $G$ es abeliano, $n\mid{\rm lcm}(r,s)$. Pero dado que $\gcd(r,s)=1$, ${\rm lcm}(r,s)=rs$ por un resultado elemental en teoría de números. Por lo tanto $n\mid rs$. Junto con $rs\mid n$, tenemos ese $n=rs$, que es lo que queremos demostrar.	group-theory,finite-groups,abelian-groups
A.314	Closed span of a sequence in Hilbert spaces.	Suppose that you have an orthonormal basis $\{e_n\}$ in a Hilbert space such that $\sum \|e_n-x_n\| < 1$. Is this condition enough to prove that the closed span of $\{x_n\}$ is $H$? My efforts to prove this have not led anywhere promising. I have tried showing that the only vector perpendicular to all of the $x_n$ would be $0$. Not sure which way I can proceed. Does anyone have an idea how to approach this? Thank you.	functional-analysis,hilbert-spaces
A.314	Espacio cerrado de una secuencia en espacios Hilbert.	Supongamos que tienes una base ortónormal $\{e_n\}$ en un espacio de Hilbert tal que $\sum \|e_n-x_n\| < 1$. ¿Es esta condición suficiente para probar que el espacio cerrado de $\{x_n\}$ es $H$? ¿Mis esfuerzos para probar esto no han llevado a nada prometedor. He intentado mostrar que el único vector perpendicular a todos los $x_n$ sería $0$. No estoy seguro de qué manera puedo proceder. ¿Alguien tiene una idea de cómo abordar esto? Gracias	functional-analysis,hilbert-spaces
A.315	Backwards Induction (Exercise 2.2.6) Analysis 1 by Terence Tao	I am new to the study of analysis and I decided to start with Terence's book in my endeavor. I want to show my "proof" of backwards induction since I have some difficulty in understanding this. I want to now if my proof is correct or have some error, because if have,$a $can't infer that. Any feedback is appreciated.   Let $n$ be a natural number, and let $P(m)$ be a property pertaining to the natural numbers such that whenever $P(m\text{++})$ is true, then $P(m)$ is true. Suppose that $P(n)$ is also true. Prove that $P(m)$ is true for all natural numbers $m ≤ n$; this is known as the principle of backwards induction. (Hint: apply induction to the variable $n$.)  First i want to show $P(m)$ is true $\forall$ $0\geq m$. H1: $\forall m$ $P(m\text{++})\implies P(m)$ H2: $P(0)$ C: $P(m)$ is true $\forall$ $0\geq m$. $0\geq m$ means $0=m+a$ for some natural number $a$, then $m=a=0$ for corollary 2.2.9. But $P(0)$ is true for H2, then the case $n=0$ is proved. Suppose now that works for $n$ and prove $n\text{++}$. then: H1: $\forall m$ $P(m\text{++})\implies P(m)$ H2: $P(n)\implies P(m)$ $\forall$ $n\geq m$ H3: $P(n\text{++})$. In H1, for $m=n$ we have $P(n\text{++})\implies P(n)$ and for H2 we now $P(n)\implies P(m)$ (specifically for $n=m$), then $P(n\text{++})\implies P(m)$ for $n\text{++}>m$. We need to prove that works for $n\text{++}=m$ but for that $P(n\text{++})$ is true for H3. We conclude that $P(n\text{++})\implies P(m)$ $\forall$ $n\text{++}\geq m$.	real-analysis,solution-verification,induction
A.315	Inducción hacia atrás (Ejercicio 2.2.6) Análisis 1 de Terence Tao	Soy nuevo en el estudio del análisis y decidí comenzar con el libro de Terence en mi esfuerzo. Quiero mostrar mi "prueba" de inducción hacia atrás ya que tengo algunas dificultades para entender esto. Quiero saber ahora si mi prueba es correcta o tiene algún error, porque si es así,$a $no puedo inferir eso. Se agradece cualquier comentario. Sea $n$ un número natural y sea $P(m)$ una propiedad perteneciente a los números naturales tal que siempre que $P(m\text{++})$ sea verdadero, entonces $P(m)$ es verdad. Supongamos que $P(n)$ también es cierto. Demuestre que $P(m)$ es cierto para todos los números naturales $m ≤ n$; esto se conoce como principio de inducción hacia atrás. (Sugerencia: aplique la inducción a la variable $n$.) Primero quiero mostrar que $P(m)$ es verdadero $\forall$ $0\geq m$. H1: $\forall m$ $P(m\text{++})\implica P(m)$ H2: $P(0)$ C: $P(m)$ es verdadero $\forall$ $0\geq m$. $0\geq m$ significa $0=m+a$ para algún número natural $a$, luego $m=a=0$ para el corolario 2.2.9. Pero $P(0)$ es cierto para H2, entonces se demuestra el caso $n=0$. Supongamos ahora que eso funciona para $n$ y demuestra $n\text{++}$. entonces: H1: $\forall m$ $P(m\text{++})\implica P(m)$ H2: $P(n)\implica P(m)$ $\forall$ $n\geq m $ H3: $P(n\text{++})$. En H1, para $m=n$ tenemos $P(n\text{++})\implica P(n)$ y para H2 ahora $P(n)\implica P(m)$ (específicamente para $ n=m$), entonces $P(n\text{++})\implica P(m)$ para $n\text{++}>m$. Necesitamos demostrar que eso funciona para $n\text{++}=m$ pero para eso $P(n\text{++})$ es cierto para H3. Concluimos que $P(n\text{++})\implica P(m)$ $\forall$ $n\text{++}\geq m$.	real-analysis,solution-verification,induction
A.316	Multiplicative of group of odd numbers modulo power of two	I want to understand the group structure of the group of units in the ring $\mathbb{Z}/2^m \mathbb{Z}$ for positive integers $m$. I expect this has been answered before here on MSE, but so far the automatic suggestions didn't turn it up. Here is what I understand: it is an abelian group, hence a product of cyclic ones. Also the order of this group is $2^{m-1}$ so that the orders of the factors in the product are all powers of two by Lagrange's theorem. Now these conditions are restrictive enough to compute the first few cases by hand. We have $(\mathbb{Z}/2 \mathbb{Z})^* = C_1$, $(\mathbb{Z}/4 \mathbb{Z})^* = C_2$, $(\mathbb{Z}/8 \mathbb{Z})^* = C_2 \times C_2$, $(\mathbb{Z}/16 \mathbb{Z})^* = C_2 \times C_4$, $(\mathbb{Z}/32 \mathbb{Z})^* = C_2 \times C_8$. I'm starting to believe that for $m > 1$ we have that $(\mathbb{Z}/2^m \mathbb{Z})^* = C_2 \times C_{2^{m-2}}$ but is there a simple conceptual explanation for that (if true)?	elementary-number-theory
A.316	Multiplicador del grupo de números impares modulo potencia de dos	Quiero entender la estructura de grupo del grupo de unidades en el anillo $\mathbb{Z}/2^m \mathbb{Z}$ para los números enteros positivos $m$. Espero que esto haya sido contestado antes aquí en MSE, pero hasta ahora las sugerencias automáticas no lo han hecho. Aquí es lo que entiendo: es un grupo abeliano, por lo tanto un producto de los cíclicos. También el orden de este grupo es $2^{m-1}$ de modo que los órdenes de los factores en el producto son todos poderes de dos por el teorema de Lagrange. Ahora estas condiciones son lo suficientemente restrictivas como para calcular los primeros casos a mano. Tenemos $(\mathbb{Z}/2 \mathbb{Z})^* = C_1$, $(\mathbb{Z}/4 \mathbb{Z})^* = C_2$, $(\mathbb{Z}/8 \mathbb{Z})^* = C_2 \times C_2$, $(\mathbb{Z}/16 \mathbb{Z})^* = C_2 \times C_4$, $(\mathbb{Z}/32 \mathbb{Z})^* = C_2 \times C_8$. Estoy empezando a creer que para $m > 1$ tenemos ese $\mathbb{Z}/2^m \mathbb{Z}$0 pero ¿hay una explicación conceptual simple para eso (si es verdad)?	elementary-number-theory
A.317	$\int \frac{1}{\left(x^2+1\right)^n}dx$	Let be $n\in \mathbb{Z_+}$. Compute the following integral: $$\int \frac{1}{\left(x^2+1\right)^n}dx$$ I obtained that for $$n=1$$ the value of the integral is $$\tan^{-1}x+C$$ and for $$n=2$$ $$x\left(\frac{1}{2\left(x^2+1\right)}+\frac{\tan \:^{-1}}{2x}\right)+C$$ How to do the rest of the cases?	integration
A.317	$\int \frac{1}{\left(x^2+1\right)^n}dx$	Deja que $n\in \mathbb{Z_+}$. la siguiente integral: $$\int \frac{1}{\left(x^2+1\right)^n}dx$$ obtuve que para $$n=1$$ el valor de la integral es $$\tan^{-1}x+C$$ y para $$n=2$$ $$x\left(\frac{1}{2\left(x^2+1\right)}+\frac{\tan \:^{-1}}{2x}\right)+C$$ ¿Cómo hacer el resto de los casos?	integration
A.318	How do you prove $e^{x} \geq \left(1+\frac{x}{n}\right)^{n}$ for $n \geq 1$	How do you prove $e^{x} \geq \left(1+\frac{x}{n}\right)^{n}$ for $n \geq 1$? I can prove this for natural numbers only via induction, but how do you prove this for any real $n \geq 1$? We start with the base case $n=1$. We have $e^x \geq 1+x$ by a variety of methods. For the induction step, assume $e^{x} \geq \left(1+\frac{x}{n}\right)^{n}$. Notice that taking the derivative of $(1+\frac{x}{n+1})^{n+1}$ gives us $(1+\frac{x}{n+1})^{n}$ and thus $(1+\frac{x}{n+1})^{n} < \left(1+\frac{x}{n}\right)^{n} \leq e^x = \frac{d}{dx} e^x$. I'm not sure how to extend this to the non-integer case. Any help would be appreciated.	calculus
A.318	¿Cómo se prueba $e^{x} \geq \left(1+\frac{x}{n}\right)^{n}$ para $n \geq 1$	¿Cómo se prueba $e^{x} \geq \left(1+\frac{x}{n}\right)^{n}$ para $n \geq 1$? Puedo probar esto para números naturales sólo a través de la inducción, pero cómo se prueba esto para cualquier $n \geq 1$ real? Comenzamos con el caso base $n=1$. Tenemos $e^x \geq 1+x$ por una variedad de métodos. Para el paso de inducción, suponga $e^{x} \geq \left(1+\frac{x}{n}\right)^{n}$. Observe que tomar la derivada de $(1+\frac{x}{n+1})^{n+1}$ nos da $(1+\frac{x}{n+1})^{n}$ y por lo tanto $(1+\frac{x}{n+1})^{n} < \left(1+\frac{x}{n}\right)^{n} \leq e^x = \frac {d}{dx}e^x$. No estoy seguro de cómo extender esto al caso no entero. Cualquier ayuda sería apreciada.	calculus
A.319	A wrong argument for $\mathbb{R}$ being countable	We assume $A$ is the set of all countable subsets of the set of real numbers. We know $A$ is a partially ordered set $(A, \subseteq)$. Suppose $$A_1 \subseteq A_2 \subseteq \ldots \subseteq A_n \subseteq A_{n+1} \subseteq \ldots$$ is a chain in $A$. We can prove $B=\bigcup_{n \in \Bbb{N}} A_n$ is a countable set. For each natural number $m$, we have $A_m \subseteq B$. So $B$ is an upper bound for $A$. This shows each chain in $A$ has an upper bound according to Zorn's lemma. $A$ has a maximal element $X$, and we know $X$ is a countable set. Now we prove $X = \Bbb{R}$. If $X \neq \Bbb{R}$, then there is an $x \in \Bbb{R}$ such that $x \notin X$. Let $Y=X \cup \{x\}$. It's obvious that $Y$ is a countable subset of the real numbers and $X \subsetneq Y$. This contradicts $X$ being a maximal element. Thus, $X = \Bbb{R}$ and $\Bbb{R}$ is a countable set. What is wrong with this argument?	elementary-set-theory,cardinals
A.319	Un argumento equivocado para que $\mathbb{R}$ sea contable	Suponemos que $A$ es el conjunto de todos los subconjuntos contables del conjunto de números reales. Sabemos que $A$ es un conjunto parcialmente ordenado $(A, \subseteq)$. Supongamos que $$A_1 \subseteq A_2 \subseteq \ldots \subseteq A_n \subseteq A_{n+1} \subseteq \ldots$$ es una cadena en $A$. Podemos probar que $B=\bigcup_{n \in \Bbb{N}} A_n$ es un conjunto contable. Para cada número natural $m$, tenemos $A_m \subseteq B$. Así que $B$ es un límite superior para $A$. Esto muestra que cada cadena en $A$ tiene un límite superior de acuerdo con el lema de Zorn. $A$ tiene un elemento máximo $X$, y sabemos que $X$ es un conjunto contable. Ahora demostramos $X = \Bbb{R}$. Si $X \neq \Bbb{R}$, entonces hay un $x \in \Bbb{R}$ tal que $x \notin X$. Sea $Y=X \cup \{x\}$. Es obvio que $Y$ es un subconjunto contable de los números reales y $X \subsetneq Y$. Esto contradice que $X$ sea un elemento maximal. Por lo tanto, $X = \Bbb{R}$ y $\Bbb{R}$ es un conjunto contable. ¿Qué hay de malo en este argumento?	elementary-set-theory,cardinals
A.320	An inverse trigonometric integral	So my integral is $$\int_{0}^{1}\frac{\sin^{-1}(x)}{x}$$ To avoid confusion let me re-write the integral as $$\mathcal I = \int_0^1 \frac{\arcsin(x)}{x}$$ I started off with a trig-substitution that is let $x = \sin(t)$ and $t = \arcsin(x)$ which means that $dx = \cos(t) dt$ So our integrand becomes $$\mathcal I = \int_0^{\frac{\pi}{2}} \frac{t}{\sin(t)} \cos(t) dt\tag{Bounds have changed}$$ $$= \int_0^{\frac{\pi}{2}} t\space\cot(t) dt$$ Then using Integration by Parts,$\space$$u = t$ $\implies du = dt$ and $dv = \cot(t)$ $\implies v = \ln(\sin(t))$ So our integrand thus becomes, $= t\space\ln(\sin(t))$ from $0$ to $\frac{\pi}{2}$ $$-\int_0^{\frac{\pi}{2}} \ln(sin(t))dt\tag{t*ln(sin(t)) = 0}$$ From here, I don't know how to proceed further. Any help/hint is appreciated :) Thanks in advance	integration,definite-integrals,trigonometric-integrals
A.320	Una integral trigonómica inversa	Entonces mi integral es $$\int_{0}^{1}\frac{\sin^{-1}(x)}{x}$$ Para evitar confusiones, permítanme reescribir la integral como $$\mathcal I = \int_0^1 \frac{\arcsin(x)}{x}$$ Comencé con una sustitución trigonométrica que es $x = \sin(t)$ y $t = \arcsin(x)$ que significa que $dx = \cos(t) dt$ Entonces nuestro integrando se convierte en $$\mathcal I = \int_0^{\frac{\pi}{2}} \frac{t}{\sin(t)} \cos (t) dt\tag{Los límites han cambiado}$$ $$= \int_0^{\frac{\pi}{2}} t\space\cot(t) dt$$ Luego, usando Integración por partes,$\space $$u = t$ $\implica du = dt$ y $dv = \cot(t)$ $\implica v = \ln(\sin(t))$ Entonces nuestro integrando se convierte en, $= t\space\ ln(\sin(t))$ de $0$ a $\frac{\pi}{2}$ $$-\int_0^{\frac{\pi}{2}} \ln(sin(t))dt \tag{t*ln(sin(t)) = 0}$$ A partir de aquí, no sé cómo continuar. Se agradece cualquier ayuda/pista :) Gracias de antemano	integration,definite-integrals,trigonometric-integrals
A.321	Solve Equation $ax+by=d$ where $d \neq \gcd(a,b)$ using Bézout	I want to solve this equation: $3x+4y=14$ I present you what I have so far: $\gcd(3,4)=1$ which is not $14$. I notice that: $3(6) + 4(-1) =14$ So using Bézout : $3(6-4k) + 4(-1+3k) = 14 (1)$, where $k$ integer. So we have $k>1/3$ and $k<3/2$. So $k = 1$. By replacing $k$ in equation $(1)$ we get: $a=2, b=2$, which indeed solves the equation. However, I dont get my previous solution $(a,b)=(4,-1)$ which is also correct. Am I applying Bézout wrong? Or am I not supposed to find the solution that I used to find the new ones, if they exist? Do I have $2$ solutions and that's it or am I missing something? Thank you.	elementary-number-theory,solution-verification
A.321	Resolver la ecuación $ax+by=d$ donde $d \neq \gcd(a,b)$ usando Bézout	Quiero resolver esta ecuación: $3x+4y=14$ Les presento lo que tengo hasta ahora: $\gcd(3,4)=1$ que no es $14$. Noto que: $3(6) + 4(-1) =14$ Entonces, usando Bézout: $3(6-4k) + 4(-1+3k) = 14 (1)$, donde $k$ entero. Entonces tenemos $k>1/3$ y $k<3/2$. Entonces $k = 1$. Al reemplazar $k$ en la ecuación $(1)$ obtenemos: $a=2, b=2$, lo que de hecho resuelve la ecuación. Sin embargo, no entiendo mi solución anterior $(a,b)=(4,-1)$ que también es correcta. ¿Estoy aplicando mal Bézout? ¿O se supone que no debo encontrar la solución que usé para encontrar las nuevas, si existen? ¿Tengo soluciones de $2$ y listo o me falta algo? Gracias.	elementary-number-theory,solution-verification
A.322	How do I calculate the sum of sum of triangular numbers?	As we know, triangular numbers are a sequence defined by $\frac{n(n+1)}{2}$. And it's first few terms are $1,3,6,10,15...$. Now I want to calculate the sum of the sum of triangular numbers. Let's define $$a_n=\frac{n(n+1)}{2}$$ $$b_n=\sum_{x=1}^na_x$$ $$c_n=\sum_{x=1}^nb_x$$ And I want an explicit formula for $c_n$. After some research, I found the explicit formula for $b_n=\frac{n(n+1)(n+2)}{6}$. Seeing the patterns from $a_n$ and $b_n$, I figured the explicit formula for $c_n$ would be $\frac{n(n+1)(n+2)(n+3)}{24}$ or $\frac{n(n+1)(n+2)(n+3)}{12}$. Then I tried to plug in those two potential equations, If $n=1$, $c_n=1$, $\frac{n(n+1)(n+2)(n+3)}{24}=1$, $\frac{n(n+1)(n+2)(n+3)}{12}=2$. Thus we can know for sure that the second equation is wrong. If $n=2$, $c_n=1+4=5$, $\frac{n(n+1)(n+2)(n+3)}{24}=5$. Seems correct so far. If $n=3$, $c_n=1+4+10=15$, $\frac{n(n+1)(n+2)(n+3)}{24}=\frac{360}{24}=15$. Overall, from the terms that I tried, the formula above seems to have worked. However, I cannot prove, or explain, why that is. Can someone prove (or disprove) my result above?	sequences-and-series,triangular-numbers
A.322	¿Cómo calculo la suma de la suma de números triangulares?	Como sabemos, los números triangulares son una secuencia definida por $\frac{n(n+1)}{2}$. Y sus primeros términos son $1,3,6,10,15...$. Ahora quiero calcular la suma de la suma de números triangulares. Definamos $$a_n=\frac{n(n+1)}{2}$$ $$b_n=\sum_{x=1}^na_x$$ $$c_n=\sum_{x=1}^nb_x$ $ Y quiero una fórmula explícita para $c_n$. Después de investigar un poco, encontré la fórmula explícita para $b_n=\frac{n(n+1)(n+2)}{6}$. Al ver los patrones de $a_n$ y $b_n$, pensé que la fórmula explícita para $c_n$ sería $\frac{n(n+1)(n+2)(n+3)}{24}$ o $ \frac{n(n+1)(n+2)(n+3)}{12}$. Luego intenté conectar esas dos ecuaciones potenciales, Si $n=1$, $c_n=1$, $\frac{n(n+1)(n+2)(n+3)}{24}=1 $, $\frac{n(n+1)(n+2)(n+3)}{12}=2$. Por tanto, podemos estar seguros de que la segunda ecuación es incorrecta. Si $n=2$, $c_n=1+4=5$, $\frac{n(n+1)(n+2)(n+3)}{24}=5$. Parece correcto hasta ahora. Si $n=3$, $c_n=1+4+10=15$, $\frac{n(n+1)(n+2)(n+3)}{24}=\frac{360}{ 24}=15$. En general, según los términos que probé, la fórmula anterior parece haber funcionado. Sin embargo, no puedo probar ni explicar por qué es así. ¿Alguien puede probar (o refutar) mi resultado anterior?	sequences-and-series,triangular-numbers
A.323	How to use the Euclidean Algorithm to find the inverse of $\overline{x^2 -x}$ in $(\mathbb{R} [x]/(x^4 + 1))^*$?	I've tried a lot with the Euclidean Algorithm, but I still can't figure it out. Do you know how I can use the Euclidean Algorithm to find the inverse of $\overline{x^2 -x}$ in $(\mathbb{R} [x]/(x^4 + 1))^*$? Thanks in advance!	abstract-algebra,ring-theory,inverse,euclidean-algorithm
A.323	¿Cómo usar el algoritmo euclidiano para encontrar la inversa de $\overline{x^2 -x}$ en $(\mathbb{R} [x]/(x^4 + 1))^*$?	He intentado mucho con el algoritmo euclidiano, pero todavía no puedo averiguarlo. ¿Sabes cómo puedo usar el algoritmo euclidiano para encontrar la inversa de $\overline{x^2 -x}$ en $(\mathbb{R} [x]/(x^4 + 1))^*$? Gracias por adelantado!	abstract-algebra,ring-theory,inverse,euclidean-algorithm
A.324	The Double Basel Problem	I have been playing with the series which I had been calling the 'Double Basel problem' for the past couple of hours $$ \sum_{n=1}^{\infty} \sum_{m=1}^\infty \frac{1}{{n^2 +m^2}}. $$ After wrestling with this for awhile, I managed to generalize a result demonstrated here. This identity is: $$ \sum_{m=1}^{\infty}\frac{1}{x^2+m^2} = \frac{1}{2x}\left[ \pi \coth{\pi x} - \frac{1}{x}\right]. $$ Hence the original series becomes: $$ \sum_{n=1}^{\infty} \frac{1}{2n}\left[\pi \coth{\pi n} - \frac{1}{n} \right]. $$ I have no idea where to go next with this problem.  I seriously doubt that this series is convergent; however, I have been unable to prove it.  Can you prove that this series is divergent? If it converges what is its value?  Thanks so much!	sequences-and-series,fourier-series,harmonic-numbers
A.324	El problema doble de Basilea	He estado jugando con la serie que había estado llamando el "problema de doble Basilea" durante las últimas horas $$ \sum_{n=1}^{\infty} \sum_{m=1}^\infty \frac{1}{{n^2 +m^2}}. $$ Después de luchar con esto durante un tiempo, logré generalizar un resultado demostrado aquí. Esta identidad es: $$ \sum_{m=1}^{\infty}\frac{1}{x^2+m^2} = \frac{1}{2x}\left[ \pi \coth{\pi x} - \frac{1}{x}\right]. $$ Así que la serie original se convierte en: $$ \sum_{n=1}^{\infty} \frac{1}{2n}\left[\pi \coth{\pi n} - \frac{1}{n} \right]. $$ No tengo idea de dónde ir a seguir con este problema. Dudo seriamente de que esta serie sea convergente; sin embargo, no he sido capaz de probarlo. ¿Puedes probar que esta serie es divergente? si converge, cuál es su valor? ¡Muchas gracias!	sequences-and-series,fourier-series,harmonic-numbers
A.325	Find consecutive composite numbers	How to find 100 consecutive composite numbers? After many attempts I arrived at the conclusion that to find $m$ consecutive composite numbers we can use this $n!+2, n! +3, ..., n! + n$ where $n! + 2$ is divisible by $2$, $n! + 3$ is divisible by $3$ and so on... and where $m$ = $n-1$ Thus $n!+2, n! +3, ..., n! + n$ tells that there are $(n-1)$ consecutive numbers. However, there seems to be some gaps or incompetence. For example: $4!+2, 4! +3, 4! +4$ $→$ $26, 27, 28$. Although it's right there are for sure smaller numbers such as $8, 9, 10$ and $14, 15 ,16.$ Is there another method for solving such a problem mathematically? Is it a correct method or have I misunderstood it?	number-theory,prime-numbers
A.325	Encuentra números compuestos consecutivos	¿Cómo encontrar 100 números compuestos consecutivos? Después de muchos intentos llegué a la conclusión de que para encontrar $m$ números compuestos consecutivos podemos usar este $n!+2, n! +3, ..., n! + n$ donde $n! + 2$ es divisible por $2$, $n! + 3$ es divisible por $3$ y así sucesivamente... y donde $m$ = $n-1$ Así que $n!+2, n! +3, ..., n! + n$ dice que hay $(n-1)$ números consecutivos. Sin embargo, parece que hay algunos huecos o incompetencia. Por ejemplo: $4!+2, 4! +3, 4! +4$ $m$0 $m$1. Aunque es cierto que hay números más pequeños como $8, 9, 10$ and $14, 15 ,16.$ ¿Existe otro método para resolver tal problema matemáticamente? ¿Es un método correcto o lo he entendido mal?	number-theory,prime-numbers
A.326	Every projective module is a direct summand of free module.	I was reading "Serial Rings" by Gennadi Puninski. There it is written that , "Since every module is a homomorphic image of a free module, every projective module is a direct summand of free module".(ie. if $P$ is a  projective module, there exists a free module F such that, $ F=P \oplus T$ for some module $T$.) But I can't understand how "Every module is a homomorphic image of a free module" implies that "Every projective module is a direct summand of free module". (I have found a proof for "Every projective module is a direct summand of free module" but the first part of the above mentioned sentence wasn't used there.)	modules,direct-sum,projective-module,free-modules
A.326	Cada módulo proyectivo es una suma directa de módulos libres.	Estaba leyendo "Serial Rings" de Gennadi Puninski. Allí se escribe que , "Dado que cada módulo es una imagen homomorfa de un módulo libre, cada módulo proyectivo es una suma directa de módulo libre". (es decir, si $P$ es un módulo proyectivo, existe un módulo libre F tal que $ F=P \oplus T$ para algún módulo $T$.) Pero no puedo entender cómo "Cada módulo es un módulo homomórfico imagen de un módulo libre" implica que "Cada módulo proyectivo es una suma directa de un módulo libre". (Encontré una prueba para "Cada módulo proyectivo es una suma directa de un módulo libre", pero la primera parte de la oración mencionada anteriormente no se usó allí).	modules,direct-sum,projective-module,free-modules
A.327	Determinant not equal to volume error (closed)	The determinant of a $3\times 3$ matrix $\begin{vmatrix} 1 & 1 &1 \\  x & y & z \\ x^2 & y^2 &z^2 \\ \end{vmatrix} $ is the volume of a parallelopiped with its three sides as the vectors whose tails rest on origin and heads at the coordinates $(1,x,x^2),(1,y,y^2)$ and $(1,z,z^2)$ $^{[1]} $. The determinant of this matrix can be simplified to $(x-y)(y-z)(z-x)$.  Proof: Subtracting column$1 $from column 2, and putting that in column  2, $\begin{equation*} \begin{vmatrix} 1 & 1 &1 \\  x & y & z \\ x^2 & y^2 &z^2 \\ \end{vmatrix}  =  \begin{vmatrix} 1 & 0 &1 \\  x & y-x & z \\ x^2 & y^2-x^2 &z^2 \\ \end{vmatrix}  \end{equation*}$ $ = z^2(y-x)-z(y^2-x^2)+x(y^2-x^2)-x^2(y-x) $ rearranging the terms, $ =z^2(y-x)-x^2(y-x)+x(y^2-x^2)-z(y^2-x^2) $ taking out the common terms $(y-x)$ and $(y^2-x^2)$, $ =(y-x)(z^2-x^2)+(y^2-x^2)(x-z) $ expanding the terms $(z^2-x^2)$ and $(y^2-x^2)$ $ =(y-x)(z-x)(z+x)+(y-x)(y+x)(x-z) $ $ =(y-x)(z-x)(z+x)-(y-x)(z-x)(y+x) $ taking out the common term $(y-x)(z-x)$ $ =(y-x)(z-x) [z+x-y-x] $ $ =(y-x)(z-x)(z-y) $ $ =(x-y)(y-z)(z-x) $  As the $x$ coordinate of the heads of these three vectors is $1$, the head of these vectors lies in a plane perpendicular to the $x$-axis and a distance of $1$ unit away from the origin. (If we connect these three points, we get a triangle.) This plane will cut the parallelopiped into two equal triangular pyramids whose base lies in the plane. The perpendicular distance from the base of the pyramid to the tip is $1$ unit. The volume of the required parallelogram is the sum of the volume of the two triangular pyramids. $\text{volume of a pyramid}=\frac{1}{3}bh$. The height is $1$ units. The area of a triangle is, by Shoelace formula, $$A = \frac{1}{2} \begin{vmatrix} 1 & 1 &1 \\  x_1 & x_2 & x_3 \\ y_1 & y_2 & y_3 \\ \end{vmatrix} $$ where the vertices of the triangle are $(x_1,y_1),(x_2.y_2),(x_3,y_3)$ $^{[2]}$ The vertices of the required traingle has the coordinates $(x,x^2),(y,y^2)$ and $(z,z^2)$. So the area of the triangle, $$A=\frac{1}{2}\begin{vmatrix} 1 & 1 &1 \\  x & y & z \\ x^2 & y^2 &z^2 \\ \end{vmatrix}$$ which, as shown above, can be simplified to $\frac{1}{2} (x-y)(y-z)(z-x)$ So, the volume is $$\frac{1}{3}bh=\frac{1}{3}\times\frac{1}{2}(x-y)(y-z)(z-x)\times 1$$ $$= \frac{1}{6}(x-y)(y-z)(z-x)  $$ But, shouldn't the volume be equal to the determinant which is $(x-y)(y-z)(z-x)$ ?  References [1]Youtube video by 3Blue1brown: https://youtu.be/Ip3X9LOh2dk?t=345 [2]Wikipedia article:https://en.wikipedia.org/wiki/Shoelace_formula	geometry,determinant
A.327	Determinante no igual al error de volumen (cerrado)	El determinante de una matriz $3\times 3$ $\begin{vmatrix} 1 & 1 &1 \\ x & y & z \\ x^2 & y^2 &z^2 \\ \end{vmatrix} $ es el volumen de un paralelepípedo con sus tres lados como los vectores cuyas colas descansan en el origen y las cabezas en las coordenadas $(1,x,x^2),(1,y,y^2)$ y $(1,z,z^ 2)$ $^{[1]} $. El determinante de esta matriz se puede simplificar a $(x-y)(y-z)(z-x)$. Prueba: Restar la columna $1 $ de la columna 2 y ponerla en la columna 2, $\begin{equation*} \begin{vmatrix} 1 & 1 &1 \\ x & y & z \\ x^2 & y^2 &z ^2 \\ \end{vmatrix} = \begin{vmatrix} 1 & 0 &1 \\ x & y-x & z \\ x^2 & y^2-x^2 &z^2 \\ \end{vmatrix} \ end{ecuación*}$ $ = z^2(y-x)-z(y^2-x^2)+x(y^2-x^2)-x^2(y-x) $ reordenando los términos, $ = z^2(y-x)-x^2(y-x)+x(y^2-x^2)-z(y^2-x^2) $ sacando los términos comunes $(y-x)$ y $(y ^2-x^2)$, $ =(y-x)(z^2-x^2)+(y^2-x^2)(x-z) $ expandiendo los términos $(z^2-x^2) $ y $(y^2-x^2)$ $ =(yx)(zx)(z+x)+(yx)(y+x)(xz) $ $ =(yx)(zx)(z+ x)-(y-x)(z-x)(y+x) $ sacando el término común $(y-x)(z-x)$ $ =(y-x)(z-x) [z+x-y-x] $ $ =(y-x)(z-x) (z-y) $ $ =(x-y)(y-z)(z-x) $ Como la coordenada $x$ de las cabezas de estos tres vectores es $1$, la cabeza de estos vectores se encuentra en un plano perpendicular al eje $x$ y una distancia de $1$ unidad del origen. (Si conectamos estos tres puntos, obtenemos un triángulo). Este plano cortará el paralelepípedo en dos pirámides triangulares iguales cuya base se encuentra en el plano. La distancia perpendicular desde la base de la pirámide hasta la punta es $1$ unidad. El volumen del paralelogramo requerido es la suma del volumen de las dos pirámides triangulares. $\text{volumen de una pirámide}=\frac{1}{3}bh$. La altura es de $1$ unidades. El área de un triángulo es, según la fórmula del cordón, $$A = \frac{1}{2} \begin{vmatrix} 1 & 1 &1 \\ x_1 & x_2 & x_3 \\ y_1 & y_2 & y_3 \\ \end {vmatriz} $$ donde los vértices del triángulo son $(x_1,y_1),(x_2.y_2),(x_3,y_3)$ $^{[2]}$ Los vértices del trengle requerido tienen las coordenadas $(x,x^2 ),(y,y^2)$ y $(z,z^2)$. Entonces el área del triángulo, $$A=\frac{1}{2}\begin{vmatrix} 1 & 1 &1 \\ x & y & z \\ x^2 & y^2 &z^2 \\ \ end{vmatrix}$$ que, como se muestra arriba, se puede simplificar a $\frac{1}{2} (x-y)(y-z)(z-x)$ Entonces, el volumen es $$\frac{1}{3} bh=\frac{1}{3}\times\frac{1}{2}(x-y)(yz)(z-x)\times 1$$ $$= \frac{1}{6}(x-y)(yz )(z-x) $$ Pero, ¿no debería el volumen ser igual al determinante que es $(x-y)(yz)(z-x)$? Referencias [1] Vídeo de Youtube de 3Blue1brown: https://youtu.be/Ip3X9LOh2dk?t=345 [2] Artículo de Wikipedia:https://en.wikipedia.org/wiki/Shoelace_formula	geometry,determinant
A.328	Proving $\sum_{k=1}^{n}\cos\frac{2\pi k}{n}=0$	I want to prove that the below equation can be held.  $$\sum_{ k=1 }^{ n  } \cos\left(\frac{  2 \pi k }{  n  } \right) =0, \qquad n>1 $$  Firstly I tried to check the equation with small values of $n$ $$  \text{As } n=2 $$ $$  \cos\left(\frac{  2 \pi \cdot 1   }{  2  } \right) + \cos\left(\frac{  2 \pi \cdot 2   }{  2   } \right)  $$ $$ = \cos\left(\pi\right)  + \cos\left(2 \pi\right)    $$ $$ = -1+ 1 =0  ~~ \leftarrow~~ \text{Obvious}  $$ But $$  \text{As}~~ n=3  $$ $$  \cos\left(\frac{  2 \pi \cdot  1   }{  3   } \right) +\cos\left(\frac{  2 \pi  \cdot  2   }{  3   } \right) + \cos\left(\frac{  2 \pi  \cdot  3   }{  3   } \right)  $$ $$ = \cos\left(\frac{  2 \pi  }{  3  } \right) + \cos\left(\frac{  4 \pi  }{  3  } \right) + \cos\left( 2\pi \right)  $$ $$ = \cos\left(\frac{  2 \pi  }{  3  } \right) + \cos\left(\frac{  4 \pi  }{  3  } \right) + 1  =?$$ What formula(s) or property(s) can be used to prove the equation?	sequences-and-series,trigonometry,trigonometric-series
A.328	Prueba de que $\sum_{k=1}^{n}\cos\frac{2\pi k}{n}=0$	Quiero probar que la ecuación de abajo puede ser sostenida. $$\sum_{ k=1 }^{ n  } \cos\left(\frac{  2 \pi k }{  n  } \right) =0, \qquad n>1 $$ Primero intenté comprobar la ecuación con valores pequeños de $n$ $$  \text{As } n=2 $$ $$  \cos\left(\frac{  2 \pi \cdot 1   }{  2  } \right) + \cos\left(\frac{  2 \pi \cdot 2   }{  2   } \right)  $$ $$ = \cos\left(\pi\right)  + \cos\left(2 \pi\right)    $$ $$ = -1+ 1 =0  ~~ \leftarrow~~ \text{Obvious}  $$ Pero $$  \text{As}~~ n=3  $$ $$  \cos\left(\frac{  2 \pi \cdot  1   }{  3   } \right) +\cos\left(\frac{  2 \pi  \cdot  2   }{  3   } \right) + \cos\left(\frac{  2 \pi  \cdot  3   }{  3   } \right)  $$ $$ = \cos\left(\frac{  2 \pi  }{  3  } \right) + \cos\left(\frac{  4 \pi  }{  3  } \right) + \cos\left( 2\pi \right)  $$ $$ = \cos\left(\frac{  2 \pi  }{  3  } \right) + \cos\left(\frac{  4 \pi  }{  3  } \right) + 1  =?$$ ¿Qué fórmula o propiedad puede ser utilizada para probar la ecuación?	sequences-and-series,trigonometry,trigonometric-series
A.329	How do I show that if $A$ is compact and $U \supseteq A$ is open, then there is an open $V$ with $A \subseteq V \subseteq \overline{V} \subseteq U$?	This question is from Wayne Patty's Topology Section 5.2.  Consider $A$ be a compact subset of a regular space and let $U$ be an open set such that $A\subseteq U$. Prove that there is an open set $V$ such that $A \subseteq  V \subseteq \overline{V} \subseteq U$.  Let $p \in A$ which implies $p \in U$. Then a result is given in   the book (Theorem 5.11): A $T_1$-space $(X, \mathcal T)$ is regular if and only if for each member $p$ of $X$ and each neighbourhood $U$ of $p$, there is a neighbourhood $V$ of $p$ such that $\overline{V}\subseteq U$. So, I got $ V \subseteq \overline{V} \subseteq U$. But I am unable to prove that $A\subseteq V \subseteq \overline{V}$. I thought that I should let $V\subseteq \overline{V} \subseteq A$ but I am not able to find a contradiction. Can you please help with that?	general-topology,separation-axioms
A.329	¿Cómo puedo mostrar que si $A$ es compacto y $U \supseteq A$ es abierto, entonces hay un $V$ abierto con $A \subseteq V \subseteq \overline{V} \subseteq U$?	Esta pregunta es de la Topología de Wayne Patty Sección 5.2. Considerar $A$ como un subconjunto compacto de un espacio regular y dejar $U$ como un conjunto abierto tal que $A\subseteq U$. Pruebe que hay un conjunto abierto $V$ tal que $A \subseteq  V \subseteq \overline{V} \subseteq U$. Que $p \in A$ que implica $p \in U$. Entonces se da un resultado en el libro (Teorema 5.11): Un $T_1$-espacio $(X, \mathcal T)$ es regular si y sólo si para cada miembro $p$ de $X$ y cada vecindario $U$ de $p$, hay un vecindario $V$ de $p$ tal que $\overline{V}\subseteq U$. Así que, tengo$ V \subseteq \overline{V} \subseteq U$. Pero no puedo probar que $A\subseteq V \subseteq \overline{V}$. Pensé que debería dejar $V\subseteq \overline{V} \subseteq A$ pero no puedo encontrar una contradicción. ¿Puedes ayudarme con eso?	general-topology,separation-axioms
A.330	Shilov's Linear Algebra - Chapter 1, Problem 9	Calculate the $n$-th order determinant: $$\Delta= \begin{vmatrix} x&a&a&\ldots&a\\ a&x&a&\ldots&a\\ a&a&x&\ldots&a\\ \cdot&\cdot&\cdot&\ldots&\cdot\\ a&a&a&\ldots&x \end{vmatrix}$$ The answer is $\Delta=[x+a(n-1)](x-a)^{n-1}$. If we add all the other columns to the first column, we get the first multiplicative factor of the answer, and are left with the following determinant: $$\begin{vmatrix} 1&a&a&\ldots&a\\ 1&x&a&\ldots&a\\ 1&a&x&\ldots&a\\ \cdot&\cdot&\cdot&\ldots&\cdot\\ 1&a&a&\ldots&x \end{vmatrix}$$ How can we calculate this determinant to obtain the answer?	linear-algebra,determinant
A.330	El álgebra lineal de Shilov - Capítulo 1, Problema 9	Calcule el determinante de orden $n$: $$\Delta= \begin{vmatrix} x&a&a&\ldots&a\\ a&x&a&\ldots&a\\ a&a&x&\ldots&a\\ \cdot&\cdot&\cdot&\ldots&\cdot\\ a&a&a&\ldots&x \end{vmatrix}$$ La respuesta es $\Delta=[x+a(n-1)](x-a)^{n-1}$. Si sumamos todas las otras columnas a la primera columna, obtenemos el primer factor multiplicativo de la respuesta, y quedamos con el siguiente determinante: $$\begin{vmatrix} 1&a&a&\ldots&a\\ 1&x&a&\ldots&a\\ 1&a&x&\ldots&a\\ \cdot&\cdot&\cdot&\ldots&\cdot\\ 1&a&a&\ldots&x \end{vmatrix}$$ ¿Cómo podemos calcular este determinante para obtener la respuesta?	linear-algebra,determinant
A.331	Finding roots of $4^x+6^x=9^x$ by hand	The function $f(x)=4^x+6^x-9^x$ is such that $f(0)=1>0, f(1)=1>0, f(2)=-29$ and next $g(x)=(4/9)^x+(6/9)^x-1 \implies f'(x)<0$ for all real values of $x$. So $g(x)$ being monotonic the equation $$4^x+6^x=9^x$$ has exactly one real solution. The question is whether this real root can be found analytically by hand.	real-analysis
A.331	Encontrar las raíces de $4^x+6^x=9^x$ a mano	La función $f(x)=4^x+6^x-9^x$ es tal que $f(0)=1>0, f(1)=1>0, f(2)=-29$ y el siguiente $g(x)=(4/9)^x+(6/9)^x-1 \implies f'(x)<0$ para todos los valores reales de $x$. Así que $g(x)$ siendo monótono la ecuación $$4^x+6^x=9^x$$ tiene exactamente una solución real. La pregunta es si esta raíz real se puede encontrar analíticamente a mano.	real-analysis
A.332	Every number can be expressed as a product of (least prime factor)*(largest integer dividing n less than n) [EDITED]	Let $L:{\Bbb N} \to {\Bbb N}$ such that $L(n) = {\text{least prime factor p of n}}$. Let $g:{\Bbb N} \to {\Bbb N}$ such that $g(n) = {\text{biggest positive integer d such that d|n and 1}} \leqslant {\text{d < n}}$. Show that:$$g(n) = \frac{n}{{L(n)}},\forall n \in {\Bbb N}{\text{ such that }}n \geqslant 2$$ My proof: Since $n \geqslant 2$, $n$ is either composite or prime. If $n$ is prime(i.e $n = p$ where $p$ is prime), then $$g(n) = 1 = \frac{p}{p} = \frac{n}{{L(n)}}$$ This is because $L(n)$ is defined as least prime factor of n. If$n $is composite, by the Fundamental Theorem of Arithmetic: "Any positive integer bigger than 1 can be expressed as a product of primes." Let $p$ be the smallest prime of the product $$n = {p_1}^{{\alpha _1}}p_2^{{\alpha _2}}...{p_m}^{{\alpha _m}},{\text{ }}i < j,{\text{ }}{p_i} < {p_j}{\text{ and }}i,j \in {{\Bbb Z}^ + }{\text{ and }}{\alpha _i} \in {{\Bbb Z}^{ \geqslant 0}}$$ That is, pick $p = p_1$. Notice that $p$ is necessarily $L(n)$ because since $p = p_1$ implies that $p|n$ and is the smallest prime that divides n. Therefore, $$p_1 = p = L(n)$$ Then, if $x = p = L(n)$, then $1 < x < n$ and $x|n$ (by definition). This implies that $$\exists y \in {{\Bbb Z}^ + }:y = \frac{n}{x} = \frac{n}{{L(n)}} = {p_1}^{{\alpha _1} - 1}{p_2}^{{\alpha _2}}...{p_m}^{{\alpha _m}}$$ But $y|n$ such that $y < n$. Implies that $y$ is the greatest integer less than$n $such that $y|n$. Now let $d = g(n)$. Since $$\frac{n}{d} = \frac{n}{{g(n)}} = m$$ then $m$ is prime and $m = p$. BWOC, If $n=md$ and $m = ab$ where $1 < a,b < m \Rightarrow n = abd \Rightarrow ad|n$. Since $1. But we found another factor of $n$ ($ad$) that divides $n$! This contradicts the definition of $d=g(n)$. Also, since $d = g(n)$ is the biggest factor of n, implies that $m = \frac{n}{d} = \frac{n}{g(n)}$ is the smallest prime factor of n. So $m = L(n) = p$. Hence, we have that $$x = m = p = L(n) = \frac{n}{g(n)} \Rightarrow g(n) = \frac{n}{p} =\frac{n}{L(n)} = y$$ or just $$g(n) = \frac{n}{L(n)}$$ Q.E.D.	elementary-number-theory
A.332	Cada número puede expresarse como el producto de (factor primo mínimo) *(el mayor número entero dividido por n menos de n) [EDITED]	Sea $L:{\Bbb N} \to {\Bbb N}$ tal que $L(n) = {\text{factor mínimo primo p de n}}$. Sea $g:{\Bbb N} \to {\Bbb N}$ tal que $g(n) = {\text{entero positivo mayor d tal que d|n y 1}} \leqslant {\text{d < n}}$. Demuestre que:$$g(n) = \frac{n}{{L(n)}},\forall n \in {\Bbb N}{\text{ tal que }}n \geqslant 2$$ Mi prueba : Dado que $n \geqslant 2$, $n$ es compuesto o primo. Si $n$ es primo (es decir, $n = p$ donde $p$ es primo), entonces $$g(n) = 1 = \frac{p}{p} = \frac{n}{{L(n )}}$$ Esto se debe a que $L(n)$ se define como el factor primo mínimo de n. Si $n $es compuesto, según el Teorema Fundamental de la Aritmética: "Cualquier número entero positivo mayor que 1 puede expresarse como producto de números primos". Sea $p$ el primo más pequeño del producto $$n = {p_1}^{{\alpha _1}}p_2^{{\alpha _2}}....{p_m}^{{\alpha _m}}, {\text{ }}i < j,{\text{ }}{p_i} < {p_j}{\text{ y }}i,j \in {{\Bbb Z}^ + }{\text{ y } }{\alpha _i} \in {{\Bbb Z}^{ \geqslant 0}}$$ Es decir, elija $p = p_1$. Observe que $p$ es necesariamente $L(n)$ porque dado que $p = p_1$ implica que $p|n$ y es el primo más pequeño que divide a n. Por lo tanto, $$p_1 = p = L(n)$$ Entonces, si $x = p = L(n)$, entonces $1 < x < n$ y $x|n$ (por definición). Esto implica que $$\exists y \in {{\Bbb Z}^ + }:y = \frac{n}{x} = \frac{n}{{L(n)}} = {p_1}^{ {\alpha _1} - 1}{p_2}^{{\alpha _2}}....{p_m}^{{\alpha _m}}$$ Pero $y|n$ tal que $y < n$. Implica que $y$ es el mayor entero menor que$n $tal que $y|n$. Ahora sea $d = g(n)$. Dado que $$\frac{n}{d} = \frac{n}{{g(n)}} = m$$ entonces $m$ es primo y $m = p$. BWOC, si $n=md$ y $m = ab$ donde $1 < a,b < m \Rightarrow n = abd \Rightarrow ad|n$. Desde $1. ¡Pero encontramos otro factor de $n$ ($ad$) que divide a $n$! Esto contradice la definición de $d=g(n)$. Además, dado que $d = g(n)$ es el factor más grande de n, implica que $m = \frac{n}{d} = \frac{n}{g(n)}$ es el factor primo más pequeño de norte. Entonces $m = L(n) = p$. Por lo tanto, tenemos que $$x = m = p = L(n) = \frac{n}{g(n)} \Rightarrow g(n) = \frac{n}{p} =\frac{n} {L(n)} = y$$ o simplemente $$g(n) = \frac{n}{L(n)}$$ Q.E.D.	elementary-number-theory
A.333	Bivariate normal density	$( X, Y)$ have a bivariate normal density centered at the origin with $E(X^2)$ = $E(Y^2) = 1$, and $E(XY) = p$ . In polar coordinates $(X, Y)$ becomes $(R,\Phi)$ where $R^2 = X^2 + Y^2$. Prove that $\Phi$ has a density given by $$\frac{\sqrt{1-p^2}}{2\pi(1-2p\sin(\varphi)\cos(\varphi))}$$ And is uniformly distributed iff $p = 0$. (To this point everything is clear) what i do not understand is how to conclude that $P\{XY > 0\} =  \frac{1}{2} +\pi^{-1} \arcsin (p)$ and $P\{XY < 0\}= \pi^{-1} \arccos (p)$.	probability,probability-theory,probability-distributions
A.333	Densidad normal bivariable	$( X, Y)$ tiene una densidad normal bivariada centrada en el origen con $E(X^2)$ = $E(Y^2) = 1$ y $E(XY) = p$ . En las coordenadas polares $(X, Y)$ se convierte en $(R,\Phi)$ donde $R^2 = X^2 + Y^2$. Pruebe que $\Phi$ tiene una densidad dada por $$\frac{\sqrt{1-p^2}}{2\pi(1-2p\sin(\varphi)\cos(\varphi))}$$ y está distribuida uniformemente si $p = 0$. (Hasta este punto todo está claro) lo que no entiendo es cómo concluir que $P\{XY > 0\} =  \frac{1}{2} +\pi^{-1} \arcsin (p)$ and $P\{XY < 0\}= \pi^{-1} \arccos (p)$.	probability,probability-theory,probability-distributions
A.334	logarithm proof for $a^{log_a(b)}=b$	I have tried proving for $a^{log_a(b)}=b$ , but I feel is incorrect, so how can I prove this? I have proved it as follows: $log_aa^{log_a(b)}=log_ab$ $log_a(b)log_aa= log_ab$ $log_a(b)= log_ab$	logarithms
A.334	prueba de logaritmo para $a^{log_a(b)}=b$	He intentado probar para $a^{log_a(b)}=b$ , pero siento que es incorrecto, así que ¿cómo puedo probar esto? Lo he demostrado de la siguiente manera: $log_aa^{log_a(b)}=log_ab$ $log_a(b)log_aa= log_ab$ $log_a(b)= log_ab$	logarithms
A.335	Matrix exponential converges to a matrix	Let $A$ be a square matrix. To show: Matrix exponential converges to some matrix $X$. $$ \lim_{N \rightarrow \infty} \sum_{k=0}^{N}\frac{A^k}{k!} =X  $$ In some proofs that I have seen it is stated that because (for a sub-multiplicative norm) $$ 0 \le  \sum_{k=0}^{\infty} \left\Vert  \frac{A^k }{k!} \right\Vert  \le  \sum_{k=0}^{N} \frac{\Vert A \Vert ^k }{k!} then the series $\sum_{k=0}^{N}\frac{A^k}{k!}$ has to be convergent. That however isn't clear to me. To me more intuitive way to show convergence would be to show that $$ \lim_{N \rightarrow \infty} \left\Vert \sum_{k=0}^{N} \frac{A^k}{k!} -X \right\Vert  =0$$ and use some intuitive matrix norm for which it is clear that all elements of $\frac{A^k}{k!} -X$ converge to zero. Any hints?	matrix-exponential
A.335	La matriz exponencial converge a una matriz	Dejamos que $A$ sea una matriz cuadrada. Para mostrar: la matriz exponencial converge a alguna matriz $X$. $$ \lim_{N \rightarrow \infty} \sum_{k=0}^{N}\frac{A^k}{k!} =X  $$ En algunas pruebas que he visto se afirma que porque (para una norma sub-multiplicativa) $$ 0 \le  \sum_{k=0}^{\infty} \left\Vert  \frac{A^k }{k!} \right\Vert  \le  \sum_{k=0}^{N} \frac{\Vert A \Vert ^k }{k!} entonces las series $\sum_{k=0}^N}\frac{A^k}{k!}$ tiene que ser convergente. Eso sin embargo no me queda claro. Para mí, una forma más intuitiva de mostrar la convergencia sería mostrar que $$ \lim_{N \rightarrow \infty} \left\Vert \sum_{k=0}^N} \frac{A^k}{k!} -X \right\Vert =0$$ y utilizar alguna norma matricial intuitiva para la cual esté claro que todos los elementos de $\frac{A^k}{k!} -X$ convergen a cero. ¿Alguna pista?	matrix-exponential
A.336	For each $n \in \mathbb{N}$, specify matrices $A, B \in \mathbb{R}^{n \times n}$ for which $A B \neq B A$ is true.	I have got the following task: (1) For each $n \in \mathbb{N}$, specify matrices $A, B \in \mathbb{R}^{n \times n}$ for which $A B \neq B A$ is true. (2) Determine $$ M:=\left\{A \in \mathbb{R}^{2 \times 2}: A B=B A \quad \forall B \in \mathbb{R}^{2 \times 2}\right\} $$ thus the matrices $A \in \mathbb{R}^{2 \times 2}$, which commute with all matrices $B \in \mathbb{R}^{2 \times 2}$. Could someone please help me with this or give me an approach? That would be very helpful. Thanks.	linear-algebra,matrices
A.336	Para cada $n \in \mathbb{N}$, especifique las matrices $A, B \in \mathbb{R}^{n \times n}$ para las cuales $A B \neq B A$ es verdadero.	Tengo la siguiente tarea: (1) Para cada $n \in \mathbb{N}$, especifique matrices $A, B \in \mathbb{R}^{n \times n}$ para las cuales $A B \neq B A$ es verdadera. (2) Determine $$ M:=\left\{A \in \mathbb{R}^{2 \times 2}: A B=B A \quad \forall B \in \mathbb{R}^{2 \times 2}\right\} $$ por lo tanto las matrices $A \in \mathbb{R}^{2 \times 2}$, que viajan con todas las matrices $B \in \mathbb{R}^{2 \times 2}$. ¿Puede alguien por favor ayudarme con esto o darme un enfoque? Eso sería muy útil. Gracias.	linear-algebra,matrices
A.337	Suppose that all the tangent lines of a regular plane curve pass through some fixed point. Prove that the curve is part of a straight line.	Question. Suppose that all the tangent lines of a regular plane curve pass through some fixed point. Prove that the curve is part of a straight line. Prove the same result if all the normal lines are parallel. I am working on differential geometry from the book by Pressley and I have a doubt in the solution of the above question whose (brief) solution is given by: Solution: We can assume that the curve $\gamma$ is unit-speed and that the tangent lines all pass through the origin (by applying a translation to $\gamma$). Then, there is a scalar $\lambda(t)$ such that $\gamma'(t) = \lambda(t)\gamma(t)$ for all $t$. Then, $\gamma '' = \lambda'\gamma   + \lambda \gamma' = (\lambda' + \lambda^2)\gamma$. Can anyone please explain me how does this line follow : " Then, there is a scalar $\lambda(t)$ such that $\gamma'(t) = \lambda(t)\gamma(t)$ for all $t$." Thanks in advance.	differential-geometry,curves,tangent-line
A.337	Supongamos que todas las líneas tangentes de una curva plana regular pasan a través de algún punto fijo.	Pregunta. Supongamos que todas las líneas tangentes de una curva plana regular pasan a través de algún punto fijo. Pruebe que la curva es parte de una línea recta. Pruebe el mismo resultado si todas las líneas normales son paralelas. Estoy trabajando en geometría diferencial del libro de Pressley y tengo dudas en la solución de la pregunta anterior cuya (breve) solución es dada por: Solución: Podemos suponer que la curva $\gamma$ es unidad de velocidad y que las líneas tangentes pasan a través del origen (aplicando una traducción a $\gamma$). Entonces, hay un escalar $\lambda(t)$ tal que $\gamma'(t) = \lambda(t)\gamma(t)$ para todo $t$. Entonces, $\gamma '' = \lambda'\gamma + \lambda \gamma' = (\lambda' + \lambda^2)\gamma$. ¿Alguien puede explicarme cómo sigue esta línea: "Entonces, hay un escalar $\lambda(t)$ tal que $\gamma'(t) = \lambda(t)\gamma(t)$ para todo $t $." Gracias de antemano.	differential-geometry,curves,tangent-line
A.338	Find all integer solutions of equation $y = \frac{a+bx}{b-x}$	How to find all integer solutions for the equation $y = \frac{a+bx}{b-x}$, where a and b are known integer values? P.S. x and y must be integer at the same time	elementary-number-theory,diophantine-equations
A.338	Encuentra todas las soluciones de números enteros de la ecuación $y = \frac{a+bx}{b-x}$	¿Cómo encontrar todas las soluciones de números enteros para la ecuación $y = \frac{a+bx}{b-x}$, donde a y b son valores de números enteros conocidos? PD x y y deben ser números enteros al mismo tiempo	elementary-number-theory,diophantine-equations
A.339	Extension of Euclid's lemma	This is$a $somewhat obvious fact that is intuitively obvious to me, but I haven't been able to construct a proof of it. Euclid's lemma states for for $p$ a prime and $ab$ a product of integers (let's take everything to be positive for simplicity), if $p \mid ab$, then $p \mid a$ or $p \mid b$. This is clear, and I know how to prove it. Let's extend it somewhat. Suppose that $a$ and $b$ are two relatively prime integers, and we have $a \mid bc$ for some other integer $c$. Then $a \not \mid b$, so it must divide $c$. This fact is obvious to me, but I can't figure out how to prove it. Does anyone have any hints or advice? Do I need the assumption of positivity? (For my purposes at the moment, I only need them to be positive, but there is value in having the most general result possible). EDIT: Updated attempt: We have that $a,b$ are relatively prime, so there exist $r,s \in \mathbb{Z}$ such that $ar + bs = 1$ by Bézout's lemma. Multiply through by $c$ to get $arc + bsc = c$. Then $a \mid arc$ and $a \mid bsc$, so $a \mid c$. How is that?	elementary-number-theory
A.339	Extensión del lema de Euclides	Este es un hecho algo obvio que es intuitivamente obvio para mí, pero no he podido construir una prueba de ello. El lema de Euclides establece que para $p$ es un primo y $ab$ es un producto de números enteros (supongamos que todo es positivo para simplificar), si $p \mid ab$, entonces $p \mid a$ o $p \mid b $. Esto está claro y sé cómo demostrarlo. Ampliémoslo un poco. Supongamos que $a$ y $b$ son dos números enteros relativamente primos, y tenemos $a \mid bc$ para algún otro número entero $c$. Entonces $a \no \mid b$, por lo que debe dividir $c$. Este hecho es obvio para mí, pero no sé cómo probarlo. ¿Alguien tiene alguna pista o consejo? ¿Necesito asumir la positividad? (Para mis propósitos en este momento, solo necesito que sean positivos, pero es valioso tener el resultado más general posible). EDIT: Intento actualizado: Tenemos que $a,b$ son primos relativos, por lo que existen $r,s \in \mathbb{Z}$ tales que $ar + bs = 1$ según el lema de Bézout. Multiplique por $c$ para obtener $arc + bsc = c$. Entonces $a \mid arc$ y $a \mid bsc$, entonces $a \mid c$. ¿Como es eso?	elementary-number-theory
A.340	I have the following problem: Let $|x_{n+1} - x_n| < 1/3^n$. Show that $(x_n)$ is a Cauchy sequence.	We have that $(x_n)$ is a sequence of real numbers. And the relation on the title: $$ |x_{n+1} - x_n| < \frac{1}{3^n}. $$ We must prove that this is a Cauchy sequence. I know that an Cauchy sequence follows the definition: given $\epsilon>0$, exists $n_0 > 0$, such that $m,n > n_o \Rightarrow |x_m - x_n|< \epsilon$ But I don't know how to use both informations to prove the exercise. If someone please may help me, I'd be very thankful.	real-analysis,sequences-and-series
A.340	Tengo el siguiente problema: $|x_{n+1} - x_n| < 1/3^n$. Muestre que $(x_n)$ es una secuencia Cauchy.	Tenemos que $(x_n)$ es una secuencia de números reales. Y la relación en el título: $$ |x_{n+1} - x_n| < \frac{1}{3^n}. $$ Debemos probar que esta es una secuencia de Cauchy. Sé que una secuencia de Cauchy sigue la definición: dado $\epsilon>0$, existe $n_0 > 0$, así que $m,n > n_o \Rightarrow |x_m - x_n|< \epsilon$ Pero no sé cómo usar ambas informaciones para probar el ejercicio. Si alguien me puede ayudar, estaría muy agradecido.	real-analysis,sequences-and-series
A.341	Do all arithmetic sequences with coprime coefficients contain a prime?	Given $a, b \in Z^+$, where $\gcd(a, b) = 1$, we can define an arithmetic sequence $c_i = a + i \cdot b$. The sequence is thus $\{a, a+b, a+2b, \cdots\}$. Do all such sequences contain a prime? Do they contain an infinite number of primes? Example: $a=2, b=3$. Then, $c_1 = a+b = 5$, which is prime. Meanwhile, $a=4, b=2$ does not have primes, but $\gcd(a, b) = 2 \neq 1$, so this isn't a counterexample.	elementary-number-theory,arithmetic-progressions,coprime
A.341	¿Todas las secuencias aritméticas con coeficientes de coprimos contienen un primo?	Dado $a, b \in Z^+$, donde $\gcd(a, b) = 1$, podemos definir una secuencia aritmética $c_i = a + i \cdot b$. La secuencia es así $\{a, a+b, a+2b, \cdots\}$. ¿Todas estas secuencias contienen un número primo? ¿Contienen un número infinito de números primos? Ejemplo: $a=2, b=3$. Entonces, $c_1 = a+b = 5$, que es primo. Mientras tanto, $a=4, b=2$ no tiene números primos, pero $\gcd(a, b) = 2 \neq 1$, por lo que este no es un contraejemplo.	elementary-number-theory,arithmetic-progressions,coprime
A.342	Four students are giving presentations	In four sections of a course, running (independently) in parallel, there are four students giving presentations that are each Exponential in length, with expected value of$10 $minutes each. How much time do we expect to be needed until all four of the presentations are completed? I'm a little thrown off by this question since it's in the chapter of order statistics in my book. But I believe that this is just gamma distribution. If each student has expected value of $10$ minutes each. Shouldn't the time needed till all four of the presentations are completed be $40$ minutes? $(10 \cdot 4 = 40)$ Or is it the following. Calculate the density of the fourth order statistics $$f(x_4) =\frac{2}{5}e^{\frac{-x}{10}}\left(1-e^{\frac{-x}{10}}\right)^3.$$ Then $$E(X_4) = \int_0^\infty\frac{2x}{5}e^\frac{-x}{10}\left(1-e^\frac{-x}{10}\right)^3 \,dx= 125/6.$$ So is the answer $40$ minutes or $125/6$ minutes? Any help is greatly appreciated.	solution-verification,exponential-distribution,order-statistics,gamma-distribution
A.342	Cuatro estudiantes están dando presentaciones	En cuatro secciones de un curso, que se ejecutan (independientemente) en paralelo, hay cuatro estudiantes dando presentaciones que son exponenciales de longitud, con un valor esperado de $10 $ minutos cada uno. ¿Cuánto tiempo esperamos que sea necesario hasta que se completen las cuatro presentaciones? Estoy un poco desanimado por esta pregunta ya que está en el capítulo de estadísticas de orden en mi libro. Pero creo que esto es sólo distribución gamma. Si cada estudiante ha tenido un valor esperado de $10$ minutos cada uno. ¿No debería el tiempo necesario para completar las cuatro presentaciones de $40$ minutos? $(10 \cdot 4 = 40)$ O es el siguiente. Calcular la densidad de la cuarta orden estadística $$f(x_4) =\frac{2}{5}e^{\frac{-x}{10}}\left(1-e^{\frac{-x}{10}}\right)^3.$$ Entonces $$E(X_4) = \int_0^\infty\frac{2x}{5}e^\frac{-x}{10}\left(1-e^\frac{-x}{10}\right)^3 \,dx= 125/6.$$ Así que la respuesta es $40$ minutos o $125/6$ minutos? Cualquier ayuda es muy apreciada.	solution-verification,exponential-distribution,order-statistics,gamma-distribution
A.343	When two dice are identical, why are ordered pairs considered for determining probability of getting sum x?	Problem statement : +++++++++++++++ Given two identical unbiased dice, determine the probability of getting sum as 7. Event  = Sum of dots on the top face of both dice is 7. $E = {(1,6),\ (2,5),\ (3,4),\ (4,3),\ (3,4),\ (5,2),\ (6,1)}$ $|Sample Space|$ = $36$. Hence, $P(E) = 1/6$ I have a doubt here. As the two dice are given identical, why do we have to consider ordered pairs? Shouldn't it be unordered consisting of only 3 possible pairs $\{(1,6),\ (2,5),\ (3,4)\} $? Hence, $|S| = 21$ and $P(E) = 3/21$.	probability
A.343	Cuando dos dados son idénticos, ¿por qué se consideran pares ordenados para determinar la probabilidad de obtener la suma de x?	Planteamiento del problema: +++++++++++++++ Dados dos dados idénticos e imparciales, determine la probabilidad de obtener una suma de 7. Evento = La suma de los puntos en la cara superior de ambos dados es 7. $E = {(1,6),\ (2,5),\ (3,4),\ (4,3),\ (3,4),\ (5,2),\ (6,1)} $ $|Espacio de muestra|$ = $36$. Por lo tanto, $P(E) = 1/6$ Tengo una duda aquí. Como los dos dados son idénticos, ¿por qué tenemos que considerar pares ordenados? ¿No debería estar desordenado y constar de solo 3 pares posibles $\{(1,6),\ (2,5),\ (3,4)\} $? Por lo tanto, $|S| = 21$ y $P(E) = 3/21$.	probability
A.344	Any collection of subsets of a set is a subbasis for a topology	Theorem Any collection of subsets $\mathcal{A}$ of a nonempty set $X$ forms the subbasis for a unique topology $\tau$ on $X$. This theorem is absolutely amazing to me. I really enjoy the idea of it as a powerful tool, but I have come up with a counterexample that I just can't get over. So the theorem states that any collection of subsets of a nonempty $X$ form a subbasis for a unique topology on $X$. The emphasis there is any. So, consider the following counterexample: Let $X= \{a,b,c,d,e\}$ and let $\mathcal{A}=\{\{a\}\}$. Clearly, this is a collection of subsets of $X$. Assume that, by our theorem, then $\mathcal{A}=\{\{a\}\}$ is a subbasis for some topology on $X$. Okay, so since $\mathcal{A}$ is a subbasis of some topology on $X$, let's try taking intersections of members of $\mathcal{A}$. Well, $\{a\}\cap\{a\}=\{a\}$. Then our basis for our topology is $\mathcal{B} = \{\{a\}\}$ This is problematic because this means that our basis $\mathcal{B}$ is just $\{a\}$, but note that $\displaystyle\bigcup \mathcal{B} = \{a\}$ and $\{a\} \neq X.$ Therefore, $X \not \in \tau.$ How do we get $X$ in $\tau$? Is my counterexample logically consistent?	general-topology
A.344	Cualquier colección de subconjuntos de un conjunto es una subbase para una topología	Teorema Cualquier colección de subconjuntos $\mathcal{A}$ de un conjunto no vacío $X$ forma la subbase para una topología única $\tau$ en $X$. Este teorema es absolutamente sorprendente para mí. Realmente disfruto la idea de que sea una herramienta poderosa, pero se me ocurrió un contraejemplo que simplemente no puedo superar. Entonces, el teorema establece que cualquier colección de subconjuntos de un $X$ no vacío forma una subbase para una topología única en $X$. El énfasis hay cualquiera. Entonces, considere el siguiente contraejemplo: Sea $X= \{a,b,c,d,e\}$ y sea $\mathcal{A}=\{\{a\}\}$. Claramente, esta es una colección de subconjuntos de $X$. Supongamos que, según nuestro teorema, entonces $\mathcal{A}=\{\{a\}\}$ es una subbase para alguna topología en $X$. Bien, dado que $\mathcal{A}$ es una subbase de alguna topología en $X$, intentemos tomar intersecciones de miembros de $\mathcal{A}$. Bueno, $\{a\}\cap\{a\}=\{a\}$. Entonces nuestra base para nuestra topología es $\mathcal{B} = \{\{a\}\}$. Esto es problemático porque significa que nuestra base $\mathcal{B}$ es simplemente $\{a\}$, pero tenga en cuenta que $\displaystyle\bigcup \mathcal{B} = \{a\}$ y $\{a\} \neq X.$ Por lo tanto, $X \not \in \tau.$ ¿Cómo obtenemos $X? $ en $\tau$? ¿Es mi contraejemplo lógicamente consistente?	general-topology
A.345	Elementary geometry question: How to calculate distance between two skew lines?	I am helping someone with highschool maths but I got stacked in a elementary geometry problem. I am given the equation of two straigh lines in the space $r\equiv \begin{cases} x=1 \\ y=1 \\z=\lambda -2 \end{cases}$ and $s\equiv\begin{cases} x=\mu \\ y=\mu -1 \\ z=-1\end{cases}$ and asked for some calculations. First I am asked the relative position of them so I get they are skew lines. After that I am asked for the distance between the two lines. In order to get the distance I have to calculate the line that is perpendicular to both of them in the "skewing" point, check the points where it touches the other two lines (sorry, not sure about the word in English) and calculate the module of this vector. Im having trouble calculating the perpendicular line. I know I can get the director vector using vectorial product, but  I'm not sure how to find a point so that I can build the line.	geometry
A.345	Pregunta de geometría elemental: ¿Cómo calcular la distancia entre dos líneas sesgadas?	Estoy ayudando a alguien con matemáticas de secundaria, pero me quedé atascado en un problema de geometría de primaria. Me dan la ecuación de dos rectas en el espacio $r\equiv \begin{cases} x=1 \\ y=1 \\z=\lambda -2 \end{cases}$ y $s\equiv\begin {cases} x=\mu \\ y=\mu -1 \\ z=-1\end{cases}$ y solicitó algunos cálculos. Primero me preguntan la posición relativa de ellos, por lo que entiendo que son líneas sesgadas. Después de eso me preguntan la distancia entre las dos líneas. Para obtener la distancia tengo que calcular la línea que es perpendicular a ambas en el punto "sesgado", verificar los puntos donde toca las otras dos líneas (lo siento, no estoy seguro de la palabra en inglés) y calcular la módulo de este vector. Tengo problemas para calcular la línea perpendicular. Sé que puedo obtener el vector director usando el producto vectorial, pero no estoy seguro de cómo encontrar un punto para poder construir la línea.	geometry
A.346	Greatest lower bound in Q	I have a set $$ \{ r \in \mathbb Q \mid r^2 >2, r>0 \}$$ I was wondering why it does not have the greatest lower bound. Isn't $0 \in \mathbb Q$ a greatest lower bound for this set in rational numbers?	supremum-and-infimum
A.346	El límite inferior más grande en Q	Tengo un conjunto $$ \{ r \in \mathbb Q \mid r^2 >2, r>0 \}$$ me preguntaba por qué no tiene el límite inferior más grande. ¿No es $0 \in \mathbb Q$ un límite inferior más grande para este conjunto en números racionales?	supremum-and-infimum
A.347	GCD (a,b) =1 prove GCD ( (a+b), (a-b) ) = 1 or 2	if GCD of $(a, b) = 1$, prove that GCD $(a+b, a-b) = 1$ or $2 .$ The proof goes like: Let GCD $( a+b, a-b ) = d$ and let there exist integers m and n such that $ a+b =md$  and $ a-b = nd.$ By adding and subtracting these two equations we get: $2a = (m+n)d$ and $2b = (m-n)d$ , because $a, b$ are coprime then $2$ GCD $(a,b)$ = GCD $(2a, 2b),$ and so on. My question is, why do we have to add and subtract above equations? I need to understand the concept of this prove in some more details. Thanks!	divisibility,gcd-and-lcm
A.347	GCD (a,b) =1 prueba GCD (a+b), (a-b) = 1 o 2	Si GCD de $(a, b) = 1$, prueba que GCD $(a+b, a-b) = 1$ o $2 .$ La prueba es: Que GCD $( a+b, a-b ) = d$ y que existan enteros m y n tales que $ a+b =md$ y $ a-b = nd.$ Al sumar y restar estas dos ecuaciones obtenemos: $2a = (m+n)d$ y $2b = (m-n)d$ , porque $a, b$ son coprimos entonces $2$ GCD $(a,b)$ = GCD $(2a, 2b),$ y así sucesivamente. Mi pregunta es, ¿por qué tenemos que sumar y restar por encima de ecuaciones? Necesito entender el concepto de esto probar en algunos detalles más. Gracias!	divisibility,gcd-and-lcm
A.348	Question about determinant of a block matrix	I was studying block matrices and suddenly this question came to my mind. Let $A, B \in \Bbb R^{n \times n}$. From this Wikipedia page, $$\det \begin{pmatrix} A & B\\ B & A\end{pmatrix} = \det(A-B)\det(A+B)$$ even if $A$ and $B$ do not commute. Does a similar condition hold for the following block matrix? $$\begin{pmatrix} A & -B\\  B & A \end{pmatrix}$$	matrices,determinant,block-matrices
A.348	Pregunta sobre determinante de una matriz de bloque	Estudiaba matrices de bloques y de repente me vino a la mente esta pregunta. Deje $A, B \in \Bbb R^{n \times n}$. De esta página de Wikipedia, $$\det \begin{pmatrix} A & B\\ B & A\end{pmatrix} = \det(A-B)\det(A+B)$$ incluso si $A$ y $B$ no se desplazan. ¿Es una condición similar para la siguiente matriz de bloques? $$\begin{pmatrix} A & -B\\  B & A \end{pmatrix}$$	matrices,determinant,block-matrices
A.349	Inverse to Stirling's Approximation	The equation for Stirling's Approximation is the following: $$x! = \sqrt{2\pi x} * (\frac{x}{e})^x$$ Writing as a function for y gives us the following: $$y = \sqrt{2\pi x} * (\frac{x}{e})^x$$ Is there a way to solve this equation for x, effectively finding an inverse to this function?	factorial,inverse-function
A.349	La inversa de la aproximación de Stirling	La ecuación para la aproximación de Stirling es la siguiente: $$x! = \sqrt{2\pi x} * (\frac{x}{e})^x$$ Escribir como una función para y nos da lo siguiente: $$y = \sqrt{2\pi x} * (\frac{x}{e})^x$$ ¿Hay una manera de resolver esta ecuación para x, encontrando efectivamente una inversa a esta función?	factorial,inverse-function
A.350	Induction proof for natural numbers in a division operation	I want to proove that 2 and 3 divide $x^3 - x, x \in \mathbb{N}$ and I'm stuck at the inductive step, here's where I'm at: For all $x \in \mathbb{N}$, let $P(x)$ be the  proposition: 2 and 3 divide $x^3 - x$ Basic step: the first term in $\mathbb{N}$ is $0$, then: $\frac{0^3 - 0}{2} = 0$ et $\frac{0^3 - 0}{3} = 0$, thus $P(0)$ is true. Inductive step: For the inductive hypothesis, we assume that $P(k)$ is true for an arbitrary nonnegative integer k bigger than 0. That is, we assume that 2 and 3 divide $k^3 - k$ To carry out the inductive step using this assumption, we must show that when we assume that $P(k)$ is true, then $P(k + 1)$ is also true. That is, we must show that 2 and 3 divide $(k+1)^3 - (k+1)$ Is the next step here is that we need to prove that $\frac{(k+1)^3-(k+1)}{2}$ and $\frac{(k+1)^3-(k+1)}{3}$ are integers? thus 2 and 3 divide $(k+1)^3 - (k+1)$?	induction,divisibility,natural-numbers
A.350	Prueba de inducción de números naturales en una operación de división	Quiero probar que 2 y 3 dividen $x^3 - x, x \in \mathbb{N}$ y estoy estancado en el paso inductivo, aquí es donde estoy: para todos los $x \in \mathbb{N}$, que $P(x)$ sea la proposición: 2 y 3 dividen $x^3 - x$ Paso básico: el primer término en $\mathbb{N}$ es $0$, entonces: $\frac{0^3 - 0}{2} = 0$ y $\frac{0^3 - 0}{3} = 0$, por lo que $P(0)$ es verdadero. Paso inductivo: Para la hipótesis inductiva, suponemos que $P(k)$ es verdadero para un numero entero no negativo arbitrario k mayor que 0. Es decir, suponemos que 2 y 3 dividen $k^3 - k$ Para llevar a cabo el paso inductivo usando esta suposición, debemos mostrar que cuando suponemos que$P(k)$ es verdadero, entonces $P(k + 1)$ también es verdadero. Es decir, debemos mostrar que 2 y 3 dividen $(k+1)^3 - (k+1)$ ¿El siguiente paso aquí es que necesitamos probar que $\frac{(k+1)^3-(k+1)}{2}$ y $\frac{(k+1)^3-(k+1)}{3}$ son enteros? entonces 2 y 3 dividen $(k+1)^3 - (k+1)$?	induction,divisibility,natural-numbers
A.351	Application of Fatou's Lemma but something simpler is better?	The question Let $(X,\mathcal{A},\mu)$ be a measure space. Let $A_n$ be a sequence of sets in $\mathcal{A}$. Define $A := \{ x \in X $ such that  for all but finitely many $n \in \mathbb{N}$ it holds that $x ∈ A_n$ $ \}$ Show that $\lim_{n\to\infty}\inf \mu (A_n) \geq \mu(A)$ My attempt $ \mu (A_n) = \int_X \chi_{A_n} d\mu$ and so by Fatou's lemma: $\lim_{n\to\infty}\inf \mu (A_n) = \lim_{n\to\infty}\inf \int_X \chi_{ A_n} \geq \int_X \lim_{n\to\infty}\inf \chi_{A_n} d\mu$ Now all I need to show is that $\lim_{n\to\infty}\inf \chi_{A_n}(x)  = \chi_A(x)$ a.e Consider $x\in A$ then eventually $x\in A_n \forall n $ eventually and so $\chi_{A_n}(x) = 1 \forall n $ and so $\lim_{n\to\infty}\inf \chi_{A_n}(x)  = \lim_{n\to\infty} 1 = 1= \chi_A(x)$ Now consider $x\not\in A$ then $\forall N \in \mathbb{N} \exists n >N$ such that $x \not\in A_n$ and so $\inf_{m \geq n} \chi_{A_n}(x) = 0 \forall n$ and hence $\lim_{n\to\infty}\inf \chi_{A_n}(x)  = \lim_{n\to\infty} 0 = 0= \chi_A(x)$ Ando so the required follows. However this feels awfully complicated and I was wondering if anyone has any tips for something simpler	measure-theory
A.351	¿La aplicación del Lemma de Fatou, pero algo más simple es mejor?	La pregunta Sea $(X,\mathcal{A},\mu)$ un espacio de medida. Sea $A_n$ una secuencia de conjuntos en $\mathcal{A}$. Defina $A := \{ x \in X $ tal que para todos menos un número finito de $n \in \mathbb{N}$ se cumple que $x ∈ A_n$ $ \}$ Demuestre que $\lim_{n\to \infty}\inf \mu (A_n) \geq \mu(A)$ Mi intento $ \mu (A_n) = \int_X \chi_{A_n} d\mu$ y así por el lema de Fatou: $\lim_{n\ to\infty}\inf \mu (A_n) = \lim_{n\to\infty}\inf \int_X \chi_{ A_n} \geq \int_X \lim_{n\to\infty}\inf \chi_{A_n} d\mu$ Ahora todo lo que necesito mostrar es que $\lim_{n\to\infty}\inf \chi_{A_n}(x) = \chi_A(x)$ a.e Considere $x\in A$ y eventualmente $ x\in A_n \forall n $ eventualmente y entonces $\chi_{A_n}(x) = 1 \forall n $ y entonces $\lim_{n\to\infty}\inf \chi_{A_n}(x) = \ lim_{n\to\infty} 1 = 1= \chi_A(x)$ Ahora considere $x\not\in A$ entonces $\forall N \in \mathbb{N} \exists n >N$ tal que $x \not\in A_n$ y entonces $\inf_{m \geq n} \chi_{A_n}(x) = 0 \forall n$ y por lo tanto $\lim_{n\to\infty}\inf \chi_{A_n} (x) = \lim_{n\to\infty} 0 = 0= \chi_A(x)$ Y así se sigue lo requerido. Sin embargo, esto parece tremendamente complicado y me preguntaba si alguien tiene algún consejo para algo más simple.	measure-theory
A.352	find a positive continuous function with a finite area : $\int_0^\infty f(x) dx$ , but the $f(x)\rightarrow$ doesn't exist.	find a positive continuous function with a finite area : $\int_0^\infty f(x) dx$ , but the limit of $f(x)$ as $x$ goes to infinity doesn't exist. I tried finding such a function but I failed .	real-analysis,calculus,limits
A.352	encontrar una función continua positiva con un área finita: $\int_0^\infty f(x) dx$, pero el $f(x)\rightarrow$ no existe.	encontrar una función continua positiva con un área finita: $\int_0^\infty f(x) dx$ , pero el límite de $f(x)$ como $x$ va al infinito no existe. traté de encontrar tal función pero no pude .	real-analysis,calculus,limits
A.353	Can someone explain why if two random variables, X and Y, are uncorrelated, it does not necessarily mean they are independent?	I understand that two independent random variables are by definition uncorrelated as their covariance is equivalent to 0: $Cov(x,y) = E(xy)- E(x)E(y)$ $E(x)*E(y) = E(xy)$, when x and y are two random independent variables. Therefore, $Cov(x,y) = 0$. However, I am having trouble understanding if two random variables, X and Y, are uncorrelated, it does not necessarily mean they are independent. Could someone also give me a real world example of when two random variables are neither independent nor casually connected? I believe it will help me understand this concept better.	probability,probability-theory,independence,covariance,causality
A.353	¿Puede alguien explicar por qué si dos variables aleatorias, X y Y, no están correlacionadas, no significa necesariamente que sean independientes?	Entiendo que dos variables aleatorias independientes no están correlacionadas por definición ya que su covarianza es equivalente a 0: $Cov(x,y) = E(xy)- E(x)E(y)$ $E(x)*E(y) = E(xy)$, cuando x y y son dos variables independientes aleatorias. Por lo tanto, $Cov(x,y) = 0$. Sin embargo, tengo problemas para entender si dos variables aleatorias, X y Y, no están correlacionadas, no significa necesariamente que sean independientes. ¿Podría alguien también darme un ejemplo real de cuando dos variables aleatorias no son ni independientes ni casualmente conectadas? Creo que me ayudará a entender mejor este concepto.	probability,probability-theory,independence,covariance,causality
A.354	Prove that if $p_1\mid a$ and $p_2\mid a$ then $p_1p_2\mid a$	So I am supposed to be proving that if $p_1$ and $p_2$ are distinct primes and $p_1\mid a$ and $p_2\mid a$ then $p_1p_2\mid a$, and I need to use Euclid's Lemma except as far as I understand Euclid's lemma is the converse of this statement and I have tried for the last few hours to work with Euclid's and GCDs to figure this one out and I just don't know where to start since I can't wrap my head around this one. Can anyone help me out?	elementary-number-theory
A.354	Demostrar que si $p_1\mid a$ y $p_2\mid a$ entonces $p_1p_2\mid a$	Así que se supone que estoy probando que si $p_1$ y $p_2$ son primos distintos y $p_1\mid a$ y $p_2\mid a$ entonces $p_1p_2\mid a$, y necesito usar el lema de Euclides excepto en la medida en que entiendo que el lema de Euclides es el opuesto de esta afirmación y he intentado durante las últimas horas trabajar con Euclides y GCDs para averiguar esto y yo simplemente no sé por dónde empezar ya que no puedo envolver mi cabeza alrededor de este. ¿Puede alguien ayudarme?	elementary-number-theory
A.355	$f(f(x)^2+f(y))=xf(x)+y$	Find all functions $f:\mathbb{R}\rightarrow\mathbb{R}$ such that $$f(f(x)^2+f(y))=xf(x)+y$$ for all $x,y\in{\mathbb{R}}$.  Here is my approach to the problem:  We see that $f(x)=x$ is an obvious solution (Just trying easy linear equations). I think this would be the only solution to the problem.  Am I right? And how to prove that there is no other solution? (Note: I am a beginner at functional equations)	linear-algebra,proof-writing,contest-math,functional-equations
A.355	$f(f(x)^2+f(y))=xf(x)+y$	Encuentra todas las funciones $f:\mathbb{R}\rightarrow\mathbb{R}$ tales como $$f(f(x)^2+f(y))=xf(x)+y$$ para todas $x,y\in{\mathbb{R}}$. Aquí está mi enfoque del problema: vemos que $f(x)=x$ es una solución obvia (solo intentando ecuaciones lineales fáciles). Creo que esta sería la única solución al problema. ¿Tengo razón? ¿Y cómo demostrar que no hay otra solución? (Nota: soy un principiante en ecuaciones funcionales)	linear-algebra,proof-writing,contest-math,functional-equations
A.356	How can I evaluate $ \sum_{n=0}^{\infty}{\frac{x^{kn}}{(kn)!}} $ where $k$ is a natural number?	I suddenly interested in the differential equation $$ f^{(k)}(x)=f(x) $$ So I tried to calculate for some $n$. When $ k=1 $, we know the solution $$ f(x)=A_0e^x=\sum_{n=0}^{\infty}{\frac{A_0x^n}{n!}} $$ Also, for $ k=2 $, $$ f(x)=Ae^x-Be^{-x}=\sum_{n=0}^{\infty}{(\frac{A_0x^{2n}}{(2n)!}+\frac{A_1x^{2n+1}}{(2n+1)!})} $$ where $ A_0=A+B $ and $ A_1=A-B $. Inductively, I could guess that the solution of the differential equation would be in the form $$ f(x)=\sum_{n=0}^{\infty}{\sum_{i=0}^{k-1}{\frac{A_ix^{kn+i}}{(kn+i)!}}} $$ But I could neither prove that it is the only solution nor get the explicit formula. How should I evaluate $ \sum_{n=0}^{\infty}{\frac{x^{kn}}{(kn)!}} $, cause if we know the answer for it, we can evaluate the original expression by differentiating it. Thanks to WolframAlpha, I know the answer for $ k=3 $, $$ \sum_{n=0}^{\infty}{\frac{x^{3n}}{(3n)!}}=\frac{1}{3}(2e^{-\frac {x}{2}}\cos{(\frac {\sqrt{3}}{2}x)}+e^{x}) $$ I think the answer might related to $ \sin $ and $ \cos $ of $ \frac {2\pi}{k} $.	sequences-and-series,ordinary-differential-equations,power-series,functional-equations
A.356	¿Cómo puedo evaluar $ \sum_{n=0}^{\infty}{\frac{x^{kn}}{(kn)!}} $ donde $k$ es un número natural?	De repente me interesó la ecuación diferencial $$ f^{(k)}(x)=f(x) $$ Así que traté de calcular para algún $n$. Cuando $ k=1 $, conocemos la solución $$ f(x)=A_0e^x=\sum_{n=0}^{\infty}{\frac{A_0x^n}{n!}} $$ También, para $ k=2 $, $$ f(x)=Ae^x-Be^{-x}=\sum_{n=0}^{\infty}{(\frac{A_0x^{2n}}{(2n)!}+\frac{A_1x^{2n+1}}{(2n+1)!})} $$ donde $ A_0=A+B $ y $ A_1=A-B $. inductivamente, podría adivinar que la solución de la ecuación diferencial sería en la forma $$ f(x)=\sum_{n=0}^{\infty}{\sum_{i=0}^{k-1}{\frac{A_ix^{kn+i}}{(kn+i)!}}} $$ Pero no podía demostrar que es la única solución ni obtener la fórmula explícita. ¿Cómo debería evaluar $$ f^{(k)}(x)=f(x) $$0, porque si sabemos la respuesta para ella, podemos evaluar la expresión original al diferenciarla. Gracias a WolframAlpha, sé la respuesta para $ k=3 $, $$ \sum_{n=0}^{\infty}{\frac{x^{3n}}{(3n)!}}=\frac{1}{3}(2e^{ -\frac {x}{2}}\cos{(\frac {\sqrt{3}}{2}x)}+e^{x}) $$ Creo que la respuesta podría estar relacionada con $ \sin $ y $ \cos $ de $ \frac {2\pi}{k} $.	sequences-and-series,ordinary-differential-equations,power-series,functional-equations
A.357	How many functions can be used to describe to a finite series?	I was learning more about series today and would like to know if there are existing proofs I could look at about this problem. Basically, if you are given an infinite series representing a function f : $\Bbb N \Rightarrow \Bbb R$ but only shown the first n numbers, how many functions f, written in terms of n, could you write to represent that series. I'm not including piecewise functions, because I assume that would always be infinite. Take the series $(2, 4, ...)$ with 2 numbers given. $f(n)=2n$ , $f(n)=n^2-n+2$ , and $f(n)=2^n$ would all be functions that could fit this series, although they differ after the first two numbers. I believe there are more polynomials that fit this description but I'm not sure how many. My question is, essentially, are there an infinite number of functions for which $f(1) = 2$ and $f(2) = 4$, and if this is the case, does this also apply to any finite number of outputs? (e.g. the first n digits of pi written as $(3, 1, 4, 1, 5, 9...)$) If not, could you find out how many possible functions there are?	sequences-and-series,number-theory
A.357	¿Cuántas funciones se pueden usar para describir una serie finita?	Hoy estaba aprendiendo más sobre series y me gustaría saber si hay pruebas existentes que podría mirar sobre este problema. Básicamente, si se te da una serie infinita que representa una función f: $\Bbb N \Rightarrow \Bbb R$ pero solo se muestran los primeros n números, ¿cuántas funciones f, escritas en términos de n, podrías escribir para representar esa serie. No estoy incluyendo funciones piezas, porque supongo que siempre sería infinito. Tomemos la serie $(2, 4, ...)$ con 2 números dados. $f(n)=2n$ , $f(n)=n^2-n+2$ , y $f(n)=2^n$ serían todas funciones que podrían encajar en esta serie, aunque difieren después de los dos primeros números. Creo que hay más polinomios que encajan en esta descripción pero no estoy seguro de cuántos. Mi pregunta es, esencialmente, ¿hay un número infinito de funciones para las cuales $f(1) = 2$ y $f(2) = 4$, y si este es el caso, ¿podría esta función también aplicar a cualquier número finito de pi? (p.g. de las funciones escritas como la primera, si no hay muchos números como $(3, 1, 4, 1, 5, 9...)$) Si no, ¿podrías averiguar cuántas funciones posibles hay?	sequences-and-series,number-theory
A.358	Confusion about the formula of the area of a surface of revolution	Before I read the formula of the area of revolution which is $\int 2\pi y \,ds$, where $ds = \sqrt{1 + \frac{dy}{dx}^2}$, I thought of deriving it myself. I tried to apply the same logic used for calculating the volume of revolution (e.g., $\int \pi y^2 dx $). My idea is to use many tiny hollow cylinders (inspired from the shell method), each has a surface area of $(2\pi y) (dx)$:  $2\pi y$ is the circumference of the cylinder, and $dx$ is the height of the cylinder  Their product is the surface area of the hollow (e.g., empty from the inside) cylinder. With this logic, the area is $\int 2\pi y dx$. Where is my mistake? Also it's confusing why for the volume it was enough to partition the object using cylinders and for areas not.	calculus,area
A.358	Confusión sobre la fórmula del área de una superficie de revolución	Antes de leer la fórmula del área de revolución que es $\int 2\pi y \,ds$, donde $ds = \sqrt{1 + \frac{dy}{dx}^2}$, pensé en derivarla yo mismo. Traté de aplicar la misma lógica utilizada para calcular el volumen de revolución (por ejemplo, $\int \pi y^2 dx $). Mi idea es usar muchos pequeños cilindros huecos (inspirados en el método de la concha), cada uno tiene un área de superficie de $(2\pi y) (dx)$: $2\pi y$ es la circunferencia del cilindro, y $dx$ es la altura del cilindro. Su producto es el área de superficie del cilindro hueco (por ejemplo, vacío desde el interior). Con esta lógica, el área es $\int 2\pi y dx$. ¿Dónde está mi error? También es confuso por qué para el volumen fue suficiente dividir el objeto usando cilindros y para las áreas no.	calculus,area
A.359	Apparent inconsistencies in integration	In a problem, the substitution $$\tan\theta=\frac{x}{2}$$ was made. In the end, the answer was in terms of sines, and to convert back, $sin\theta$ was defined as $$\sin\theta=\frac{x}{\sqrt{4+x^2}}$$ This is a typical example of some stuff about integration I'm struggling to understand; (1) Why are the absolute values of square roots never taken? This is something I keep seeing in every situation involving an integral. (Here, if $\theta$ is in the third quadrant, sines would be negative and tans would be positive. So this definitely doesn't work for the third quadrant.) (2) Expanding upon the stuff in the parantheses up there, a possible explanation is that while doing trig substitutions, the angle is always a principal angle of the inverse trigonometric operation on whatever you're making the substitution. Is there such a rule?	integration,trigonometry,roots,substitution,trigonometric-integrals
A.359	Aparentes inconsistencias en la integración	En un problema, se hizo la sustitución $$\tan\theta=\frac{x}{2}$$. Al final, la respuesta fue en términos de los senos, y para convertir de nuevo, $sin\theta$ se definió como $$\sin\theta=\frac{x}{\sqrt{4+x^2}}$$ Este es un ejemplo típico de algunas cosas sobre la integración que estoy luchando por entender; (1) ¿Por qué los valores absolutos de raíces cuadradas nunca se toman? Esto es algo que sigo viendo en todas las situaciones que involucran una integral. (Aquí, si $\theta$ está en el tercer cuadrante, los senos serían negativos y las tangentes serian positivos. Así que definitivamente esto no funciona para el tercer cuadrante.) (2) Expandiendo las cosas en las paranteses allí arriba, una posible explicación es que mientras que los desencadenos, el ángulo siempre es un ángulo principal de la operación trigonométrica inversa en cualquier cosa que usted está haciendo la sustitución. ¿Existe tal regla?	integration,trigonometry,roots,substitution,trigonometric-integrals
A.360	Fourier transform of function $1/ \vert x \vert$	What is the Fourier transform of function $$f(x) = \frac{1}{\vert x \vert}?$$ This is not a homework. I would also appreciate help for calculating it myself.	fourier-transform
A.360	Transformación de Fourier de la función $1/ \vert x \vert$	¿Qué es la transformación de Fourier de la función $$f(x) = \frac{1}{\vert x \vert}?$$ Esto no es una tarea. También agradecería ayuda para calcularlo yo mismo.	fourier-transform
A.361	Is a Riemann-integrable function always differentiable?	Let $f:[a,b]\to\mathbb{R}$ be Riemann-integrable and $F(x)=\int_a^xf(t)dt$. Is this function $F$ always differentiable? Because the antiderivative is defined as $F'=f$ right, so you would think that it always holds.	real-analysis
A.361	¿Es una función integrable de Riemann siempre diferenciable?	Sea $f:[a,b]\to\mathbb{R}$ integrable en Riemann y $F(x)=\int_a^xf(t)dt$. ¿Esta función $F$ es siempre diferenciable? Debido a que la primitiva se define como $F'=f$ correcto, se podría pensar que siempre se cumple.	real-analysis
A.362	Kuratowski's Theorem using Axiom of Choice	I can't seem to be able to prove Kuratowski's Theorem using the Axiom of Choice, although they are equivalent assertions. Kuratowski's Lemma: Every partial order has a maximal chain. Axiom of Choice: For every set X of disjoint nonempty sets there exists a set$Y $such that for every set $Z \in X, Y \cap Z$ is a singleton. My attempt: Consider any chain $C_0$ of the partial order. If $\exists x \in X \setminus C_0$ which is comparable with some element of $C_0$, let $C_1 := C_0 \cup \{ x \}$. Iterate this process . If at some point we cannot find such an x, then we have found a maximal chain. Suppose we can find such an $x$ infinitely, then the sets $i\geq 1 \Rightarrow X_i := C_{i+1} \setminus C_i$ are disjoint singletons. Hence by axiom of choice there exists $Y$ for which $X_i \subseteq Y$. Inorder to finish the proof, I need to prove something of the form "If a is comparable with some element of $C_0$, then $\exists j$ s.t. $a \in C_j$". I can't seem to prove this. P.S: x is comparable with y iff $x R y \lor y R x$.	order-theory,axiom-of-choice
A.362	Teorema de Kuratowski usando el axioma de la elección	No puedo probar el teorema de Kuratowski usando el axioma de la elección, aunque son afirmaciones equivalentes. La lemma de Kuratowski: Cada orden parcial tiene una cadena máxima. El axioma de la elección: Para cada conjunto X de conjuntos no vacíos desarticulados existe un conjunto$Y $ tal que para cada conjunto $Z \in X, Y \cap Z$ es un singleton. Mi intento: Considere cualquier cadena $C_0$ del orden parcial. Si $\exists x \in X \setminus C_0$ que es comparable con algún elemento de $C_0$, dejar que $C_1 := C_0 \cup \{ x \}$. Iterar este proceso. Si en algún momento no podemos probar tal x, entonces hemos encontrado una cadena máxima. Supongamos que podemos encontrar tal $x$ infinitamente, entonces los conjuntos $i\geq 1 \Rightarrow X_i := C_{i+1} \setminus C_i$ son singletones desarticulados. Por tanto, por axioma de elección existe $Y$ para el cual $X_i \subseteq Y$. Para terminar la prueba, necesito demostrar algo de la forma "Si a es comparable con algún elemento de $C_0$, entonces $\existe j$ s.t. $a \in C_j$". Parece que no puedo probar esto. P.D: x es comparable con y si $x R y \lor y R x$.	order-theory,axiom-of-choice
A.363	Non-negative martingale $X_n \rightarrow 0$ a.s. prove that $P[X^* \geq x | \mathcal{F}_0]= 1 \wedge X_0 / x$	I need to prove the following statement. Let $X$ be a non negative martingale such that $X_n\rightarrow 0$ a.s. when $n\rightarrow \infty$. Define $X^*=supX_n$. Prove that for all $x>0$ $$P[X^*  \geq x | \mathcal{F}_0]= 1 \wedge X_0 / x$$ I think I've got the easy case  if  $x\leq X_0$  Then necessarily $x\leq X^*$ for the sup property. Then it follows that for $1\leq X_0 /x$ we have that $P[X^*  \geq x | \mathcal{F}_0]= 1$. But I can't figure out the other case.	probability,martingales,stopping-times
A.363	Martingales no negativos $X_n \rightarrow 0$ a.s. demuestran que $P[X^* \geq x | \mathcal{F}_0]= 1 \wedge X_0 / x$	Debemos probar la siguiente afirmación. que $X$ sea un martingale no negativo tal que $X_n\rightarrow 0$ a.s. cuando $n\rightarrow \infty$. definir $X^*=supX_n$. demostrar que para todos $x>0$ $$P[X^*  \geq x | \mathcal{F}_0]= 1 \wedge X_0 / x$$ creo que tengo el caso fácil si $x\leq X_0$ entonces necesariamente $x\leq X^*$ para la propiedad sup. entonces se sigue que para $1\leq X_0 /x$ tenemos que $P[X^*  \geq x | \mathcal{F}_0]= 1$. pero no puedo averiguar el otro caso.	probability,martingales,stopping-times
A.364	Inequality in metric space	For a point $x$ and a non-empty subset $A$ of a metric space $(X, d)$, define $\begin{align}\inf\left\{ d(x,a):a\in A\right\}\end{align}$ Prove that if $y$ is another point in $X$ then $$d(x,A)\leqslant d(x,y)+d(y,A)$$	general-topology,analysis,metric-spaces
A.364	Desigualdad en el espacio métrico	Para un punto $x$ y un subconjunto $A$ no vacío de un espacio métrico $(X, d)$, definir $\begin{align}\inf\left\{ d(x,a):a\in A\right\}\end{align}$ Pruebe que si $y$ es otro punto en $X$ entonces $$d(x,A)\leqslant d(x,y)+d(y,A)$$	general-topology,analysis,metric-spaces
A.365	Are Infinite ordinals and their successor equinumerous?	Ordinals in set theory are well-ordered by $\in$ or equivalently $\subset$. If we define all ordinals greater or equal to $\omega$ as infinite ordinals. Is it true that every infinite ordinal is equinumerous to its successors. Basically my question is the proof or refutation of the following statement: Given infinite ordinal $\alpha$. Does there exist an injection from $\alpha^+$ to $\alpha$.	set-theory
A.365	¿Son los ordinarios infinitos y su sucesor equinumeros?	Los ordinarios en la teoría de conjuntos están bien ordenados por $\in$ o equivalentemente $\subset$. Si definimos todos los ordinarios mayores o iguales a $\omega$ como ordinarios infinitos. ¿Es cierto que cada ordinal infinito es equinumero para sus sucesores? Básicamente mi pregunta es la prueba o la refutación de la siguiente afirmación: Dado ordinal infinito $\alpha$. Existe un injeccion de $\alpha^+$ a $\alpha$.	set-theory
A.366	Show that if a normed space $X $ has a linearly independent subset of $n$ elements, so does the dual space $X'$	Show that if a normed space $X $ has a linearly independent subset of $n$ elements, so does the dual space $X'$ My attempt : $\text{Given that  a  normed space  $X$ has  a linearly indepenedent  susbset of  $n-$  element}\tag1$ let the subset be  $S=\{ e_1,e_2,e_3,....,e_n\}$ Define $e_i \in X$ by $f_j(e_i)= \delta_{ij} = \begin{cases} 1 & i=j \\0 , & i \neq j  \end{cases}$ where $1\le i\le n$ and $1\le j\le n$ From $(1)$ we have  $c_1e_1+...+c_ne_n=0\implies c_1f(e_1)+...+c_nf(e_n)=0$ After  that im not able to proceed further	functional-analysis
A.366	Muestre que si un espacio normativo $X $ tiene un subconjunto linealmente independiente de elementos $n$, también lo hace el espacio dual $X'$	Demuestre que si un espacio normado $X $ tiene un subconjunto linealmente independiente de $n$ elementos, también lo tiene el espacio dual $X'$ Mi intento: $\text{Dado que un espacio normado $X$ tiene un subconjunto linealmente independiente de $n-$ elemento}\tag1$ sea el subconjunto $S=\{ e_1,e_2,e_3,....,e_n\}$ Defina $e_i \in X$ por $f_j(e_i)= \delta_{ ij} = \begin{cases} 1 & i=j \\0 , & i \neq j \end{cases}$ donde $1\le i\le n$ y $1\le j\le n$ De $(1 )$ tenemos $c_1e_1+...+c_ne_n=0\implica c_1f(e_1)+...+c_nf(e_n)=0$ Después de eso no puedo continuar	functional-analysis
A.367	Prove that $d(x,M)=\frac{|\langle f,x \rangle|}{||f||}$	I want to show that for $E$ a normed space, $f\in E^*$ and $M=\{x\in E\,:\, f(x)=0\}$:  Write $M^\perp$ Show that $d(x,M)=\frac{|\langle f,x\rangle|}{||f||}$.  This is my attempt: For the second part: We have that $f\in E^*$ and for $x\in E$ and $m\in M$ $$\ |\langle f,x-m|\rangle \leq ||f|| ||x-m|| \Rightarrow \frac{|\langle f,x\rangle|}{||f||} \leq ||x-m||. $$ Therefore, $$ \frac{|\langle f,x\rangle|}{||f||} \leq \inf_{m\in M}||x-m|| =d(x,M). $$ The second inequalyty is that I can't prove, I think that any corollary of Hanh-Banach could help me to prove that  $$d(x,M)\leq \frac{|\langle f,x\rangle|}{||f||} $$ Does anyone have any idea and could check my proof? Update I found the same question in this link Orthogonality Relations Exercise, Brezis' Book Functional Analysis	functional-analysis,normed-spaces
A.367	Prueba que $d(x,M)=\frac{|\langle f,x \rangle|}{||f||}$	Quiero mostrar que para $E$ un espacio normalizado, $f\in E^*$ y $M=\{x\in E\,:\, f(x)=0\}$: Escribir $M^\perp$ Muestre que $d(x,M)=\frac{|\langle f,x\rangle|}{||f||}$. Este es mi intento: Para la segunda parte: Tenemos que $f\in E^*$ y para $x\in E$ y $m\in M$ $$\ |\langle f,x-m|\rangle \leq ||f|| ||x-m|| \Rightarrow \frac{|\langle f,x\rangle|}{||f||} \leq ||x-m||. $$ Por lo tanto, $$ \frac{|\langle f,x\rangle|}{||f||} \leq \inf_{m\in M}||x-m|| =d(x,M). $$ La segunda desigualdad es que no puedo probar, creo que cualquier corollario de Hanh-Banach podría ayudarme a probar que $$d(x,M)\leq \frac{|\langle f,x\rangle|}{||f||} $$ ¿Alguien tiene alguna idea y podría comprobar mi prueba? actualización Encontré la misma pregunta en este enlace Orthogonality Relations Exercise, Libro de Brezis Análisis Funcional	functional-analysis,normed-spaces
A.368	Basel Problem approximation error bounded by $\mathcal O(1/x)$?	In this answer it is stated that $$ \sum_{n\geq1}\frac{1}{n^2}=\sum_{n\leq x}\frac1{n^2}+\mathcal O(1/x). $$ Is this statement true as $x\to\infty$? What I've done: If $x$ is fixed, then I think the answer is almost trivial, because we may set $C=\pi^2x/6$, so $$ \sum_{n=x}^\infty\frac1{n^2}\leq\sum_{n=1}^\infty\frac1{n^2}=\frac{\pi^2}{6}=\frac{C}{x}, $$ therefore $$ \sum_{n\geq1}\frac1{n^2}=\sum_{n\leq x}\frac{1}{n^2}+\sum_{n=x}^\infty\frac{1}{n^2}\leq\sum_{n\leq x}\frac{1}{n^2}+C/x=\sum_{n\leq x}\frac{1}{n^2}+\mathcal O(1/x). $$ But is there a constant independent of $x$ that makes this true?	approximation
A.368	¿El problema de aproximación de Basilea limitado por $\mathcal O(1/x)$?	En esta respuesta se dice que $$ \sum_{n\geq1}\frac{1}{n^2}=\sum_{n\leq x}\frac1{n^2}+\mathcal O(1/x). $$ ¿Es esta afirmación verdadera como $x\to\infty$? Lo que he hecho: Si $x$ es fijo, entonces creo que la respuesta es casi trivial, porque podemos establecer $C=\pi^2x/6$, así que $$ \sum_{n=x}^\infty\frac1{n^2}\leq\sum_{n=1}^\infty\frac1{n^2}=\frac{\pi^2}{6}=\frac{C}{x}, $$ por lo tanto $$ \sum_{n\geq1}\frac1{n^2}=\sum_{n\leq x}\frac{1}{n^2}+\sum_{n=x}^\infty\frac{1}{n^2}\leq\sum_{n\leq x}\frac{1}{n^2}+C/x=\sum_{n\leq x}\frac{1}{n^2}+\mathcal O(1/x). $$ Pero hay una constante independiente de $x$ que hace que esto sea cierto?	approximation
A.369	Recurrent integral	How to calculate integral $$J_n=\int_{-\pi}^\pi \frac{\sin{(nx)}}{(1+2^n) \sin{x}}\,\mathrm{d}x\:?$$ I tried partial integration but did not succeed in finding a recurrent relation? Also, tried Moivre formula for $I_n+iJ_n$, where $I_n=\int_{-\pi}^\pi \frac{\cos{(nx)}}{(1+2^n) \sin{x}} dx$, but also without success. Any help is welcome. Thanks in advance.	integration,definite-integrals,recurrence-relations
A.369	Integral recurrente	¿Cómo calcular la integral $$J_n=\int_{-\pi}^\pi \frac{\sin{(nx)}}{(1+2^n) \sin{x}}\,\mathrm{d}x\:?$$ Intenté la integración parcial pero no pude encontrar una relación recurrente? También probé la fórmula de Moivre para $I_n+iJ_n$, donde $I_n=\int_{-\pi}^\pi \frac{\cos{(nx)}}{(1+2^n) \sin{x}} dx$, pero también sin éxito. Cualquier ayuda es bienvenida. Gracias por adelantado.	integration,definite-integrals,recurrence-relations
A.370	What if we take step functions instead of simple functions in the Lebesgue integral	When we define the Lebesgue integral, we first define it for simple functions $s(x) = \sum\limits_{j=1}^n c_j\chi_{A_j}(x)$ (where $A_j$ are measurable) as $\int sd\mu = \sum\limits_{i=j}^n c_j \mu(A_j)$ and then for $f\ge 0$ as $\int fd\mu = \sup\{\int sd\mu$ : s simple and $0\le s\le f\}$. But I was wondering what could go wrong if instead of taking simple functions in the supremum, we would take step functions, i.e. $s(x)=\sum\limits_{j=1}^nc_i\chi_{I_j}(x)$ where $I_j$ are intervals (any type, like $(a,b), (a,b], [a,b), [a,b])$).	measure-theory,lebesgue-integral,step-function,simple-functions
A.370	¿Qué pasa si tomamos funciones de paso en lugar de funciones simples en la integral de Lebesgue	Cuando definimos la integral de Lebesgue, primero la definimos para funciones simples $s(x) = \sum\limits_{j=1}^n c_j\chi_{A_j}(x)$ (donde $A_j$ son medibles) como $\int sd\mu = \sum\limits_{i=j}^n c_j \mu(A_j)$ y luego para $f\ge 0$ como $\int fd\mu = \sup\{\int sd\mu$ : s simple y $0\le s\le f\}$. Pero me preguntaba qué podría ir mal si en lugar de tomar funciones simples en el supremo, tomáramos funciones de paso, es decir, $s(x)=\sum\limits_{j=1}^nc_i\chi_{I_j}(x)$ donde $I_j$ son intervalos (cualquier tipo, como $(a,b), (a,b], [a,b), [a,b])$).	measure-theory,lebesgue-integral,step-function,simple-functions
A.371	Can I say $|f(x)g(x)|=||fg||$	Let $f,g:[0,1]\to \Bbb{R}$ be continuous functions. Show that $$||fg||\le||f||\space||g||$$ What I have got so far : $|f(x)| \le\max|f(x)|=$ norm of $f$, $||f||$.$\forall x\in[0,1]$. (Note: I have replaced supremum with maximum.) $|f(x)||g(x)| =|f(x)g(x)|\le \max |f(x)|g(x)=||f|| \space\space |g|\le \max|f(x)|\space \max|g(x)|=|f||\space\space||g||$ $|f(x)g(x)|\le||f||\space\space||g||$ As I have to show that $||fg||\le||f||\space||g||$ : Can I say $|f(x) g(x)|=||fg||$? I'm not sure about that  Because $|f(x)g(x)| \le\max|f(x)g(x)|=\max|f(x)| \space \max|g(x)|$ I feel I am missing concept to prove $|f(x) g(x)|=||fg||$, through which I think I finally can prove $||fg||\le||f||\space||g||$ please If you guys could clarify.	functional-analysis,analysis,norm
A.371	¿Puedo decir $|f(x)g(x)|=||fg||$	Que $f,g:[0,1]\to \Bbb{R}$ sean funciones continuas. Muestre que $$||fg||\le||f||\space||g||$$ Lo que tengo hasta ahora: $|f(x)| \le\max|f(x)|=$ norma de $f$, $||f||$.$\forall x\in[0,1]$. (Nota: he reemplazado supremo con máximo.) $|f(x)||g(x)| =|f(x)g(x)|\le \max |f(x)|g(x)=||f|| \space\space |g|\le \max|f(x)|\space \max|g(x)|=|f||\space\space||g||$ $|f(x)g(x)|\le||f||\space\space||g||$ Como tengo que mostrar que $||fg||\le||f||\space||g||$ : ¿Puedo decir $f,g:[0,1]\to \Bbb{R}$0? No estoy seguro de eso porque $f,g:[0,1]\to \Bbb{R}$1 siento que me falta el concepto de probar $f,g:[0,1]\to \Bbb{R}$0, a través del cual creo que finalmente puedo probar $||fg||\le||f||\space||g||$ por favor si ustedes pueden aclarar.	functional-analysis,analysis,norm
A.372	When did we move from $\mathbb{Z}\left[\sqrt{d}\right]$ to the ring of integers $\mathcal{O}_{\mathbb{Q}\left[\sqrt{d}\right]}$ and why?	Gauss made great progress in number theory in $\mathbb{Z}$ by working in $\mathbb{Z}[i]$ (or equivalently $\mathbb{Z}\left[\sqrt{-1}\right]$), so much so that we call $\mathbb{Z}[i]$ the Gaussian integers now. And it was even known to the old mathematicians that solutions to Pell's equation $x^2 - dy^2 = 1$ could be better analysed by working in $\mathbb{Z}\left[\sqrt{d}\right]$. But now in modern number theory we study much more the ring of integers $\mathcal{O}_{\mathbb{Q}\left[\sqrt{d}\right]}$. I find this confusing, as if we want to study Pell's equation with $d = 5$, we have that $\mathcal{O}_{\mathbb{Q}\left[\sqrt{5}\right]} = \mathbb{Z}\left[\frac{1 + \sqrt{5}}{2}\right]$ instead of $\mathbb{Z}\left[\sqrt{5}\right]$, which is not what we need. I was under the assumption that modern number theory usually tries to generalise its techniques but I don't see how this is a sensible generalisation and I don't see why the ring of integers is any more useful than just plain old $\mathbb{Z}\left[\sqrt{d}\right]$. So my question is: Why is the ring of integers defined the way it is?	abstract-algebra,ring-theory,soft-question,definition
A.372	¿Cuándo nos transladamos del $\mathbb{Z}\left[\sqrt{d}\right]$ al anillo de números enteros $\mathcal{O}_{\mathbb{Q}\left[\sqrt{d}\right]}$ y por qué?	Gauss hizo grandes avances en la teoría de números en $\mathbb{Z}$ trabajando en $\mathbb{Z}[i]$ (o equivalentemente $\mathbb{Z}\left[\sqrt{-1}\right]$ ), tanto es así que ahora llamamos $\mathbb{Z}[i]$ a los enteros gaussianos. E incluso los viejos matemáticos sabían que las soluciones a la ecuación de Pell $x^2 - dy^2 = 1$ podían analizarse mejor trabajando en $\mathbb{Z}\left[\sqrt{d}\right]$ . Pero ahora en la teoría de números moderna estudiamos mucho más el anillo de números enteros $\mathcal{O}_{\mathbb{Q}\left[\sqrt{d}\right]}$. Esto me parece confuso, ya que si queremos estudiar la ecuación de Pell con $d = 5$, tenemos que $\mathcal{O}_{\mathbb{Q}\left[\sqrt{5}\right]} = \ mathbb{Z}\left[\frac{1 + \sqrt{5}}{2}\right]$ en lugar de $\mathbb{Z}\left[\sqrt{5}\right]$, que no es lo que nosotros necesitamos. Suponía que la teoría de números moderna generalmente intenta generalizar sus técnicas, pero no veo cómo esto es una generalización sensata y no veo por qué el anillo de números enteros es más útil que el simple $\mathbb{ Z}\left[\sqrt{d}\right]$. Entonces mi pregunta es: ¿Por qué el anillo de números enteros está definido como está?	abstract-algebra,ring-theory,soft-question,definition
A.373	Show that $\sqrt n$ is irrational if $n$ is not a perfect square, using the method of infinite descent.	Show that $\sqrt n$ is irrational if $n$ is not$a $perfect square, using the method of infinite descent. I know how to prove this by doing a contradiction proof and using The Fundamental Theorem of Arithmetic, but now I'm asked to use infinite descent to prove it. Then the very next problem says "Why does the method of the text fail to show that $\sqrt n$ is irrational if $n$ is a perfect square?" I'm confused by this. Any hints or solutions are greatly appreciated. I was thinking of the standard argument, let $\sqrt n = {a\over b}$ where $gcd(a,b)=1$ and then through some algebra arrive at a common factor for both $a$ and $b$ which contradicts the fact that $gcd(a,b)=1$ and so we can apply this over and over again, but then I don't understand how the next problem says to explain why this method fails.	elementary-number-theory
A.373	Muestre que $\sqrt n$ es irracional si $n$ no es un cuadrado perfecto, utilizando el método de descenso infinito.	Muestre que $\sqrt n$ es irracional si $n$ no es $a $ perfectos cuadrado, usando el método de descenso infinito. Sé cómo probar esto haciendo una prueba de contradicción y usando El Teorema Fundamental de la Aritmética, pero ahora me piden que use descenso infinito para probarlo. Entonces el siguiente problema dice "¿Por qué el método del texto no muestra que $\sqrt n$ es irracional si $n$ es un cuadrado perfecto?" Estoy confundido por esto. Cualquier sugerencia o solución son muy apreciadas. Estaba pensando en el argumento estándar, dejamos que $\sqrt n = {a\over b}$ donde $gcd(a,b)=1$ y luego a través de algún álgebra llegar a un factor común para ambos $a$ y $b$ que contradice el hecho de que $gcd(a,b)=1$ y así podemos aplicar esto una y otra vez, pero entonces no entiendo cómo el siguiente problema dice que expliques porque este método falla.	elementary-number-theory
A.374	Finding all monic complex polynomials $P(x)$ such that $P(x)|P(x^2)$	Find all monic complex polynomials $P(x)$ such that $P(x)|P(x^2)$.  My progress so far is that I have find that for degree 1, $P(x)=x, x^2$ are the only ones. For degree 2, they are $P(x)=x^2+x+1, x^2, x^2-1, x^2-x, x^2-2x+1$. I also prove that these are only solutions for degree 1 and 2. However I do not see how this generalizes. Any help please?	algebra-precalculus,polynomials,divisibility
A.374	Encontrar todos los polinomios de complejo monico $P(x)$ tales que $P(x)|P(x^2)$	Encuentra todos los polinomios de complejo monico $P(x)$ tales como $P(x)|P(x^2)$. Mi progreso hasta ahora es que he encontrado que para el grado 1, $P(x)=x, x^2$ son los únicos. Para el grado 2, son $P(x)=x^2+x+1, x^2, x^2-1, x^2-x, x^2-2x+1$. También demuestro que estas son solo soluciones para el grado 1 y 2. Sin embargo, no veo cómo esto generaliza. ¿Alguna ayuda por favor?	algebra-precalculus,polynomials,divisibility
A.375	Can anyone help me solve this diophantine equation?	Find all integer solutions to $x^2 + 7 = 2^n$. I've done the case where n is an even integer but now I'm a little lost. Could anyone walk me through the solution?	elementary-number-theory
A.375	¿Puede alguien ayudarme a resolver esta ecuación diofantina?	Encontrar todas las soluciones de números enteros para $x^2 + 7 = 2^n$. He hecho el caso donde n es un número entero par pero ahora estoy un poco perdido. ¿Alguien podría guiarme a través de la solución?	elementary-number-theory
A.376	Evaluating the limit of a sqrt function using Riemann Sums	$\lim\limits_{n\to\infty}\dfrac{\sqrt1+\sqrt2+\sqrt3+\ldots+\sqrt n}{n\sqrt n}$ I am having trouble doing this problem. I have attempted to take the Riemann Sum but cannot get past the square root. I also tried to upper-bound and lower-bound it, but I got stuck doing this.	calculus,limits,riemann-sum
A.376	Evaluar el límite de una función sqrt utilizando Riemann Sumas	$\lim\limits_{n\to\infty}\dfrac{\sqrt1+\sqrt2+\sqrt3+\ldots+\sqrt n}{n\sqrt n}$ Tengo problemas para hacer este problema. he intentado tomar la suma de Riemann pero no puedo pasar por la raíz cuadrada. también he intentado la límite superior y la limitación inferior, pero me quedé atascado haciendo esto.	calculus,limits,riemann-sum
A.377	What is $\mathbb{R}^{n+1}-\mathbb{R}^n$?	In C.H. Edwards's Advanced Calculus of Several Variables he defines the ordinate set $\mathcal{O}_f$ of a function $f:\mathbb{R}^n\to\mathbb{R}$ as the set of points between $\mathbb{R}^n$ and the graph of $f,$ including the points of evaluation, $\mathbf{x}\in\mathbb{R}^n$ and the points in the graph $\left\{\mathbf{x},f\left(\mathbf{x}\right)\right\}\in\mathbb{R}^{n+1}$.  Later on he defines a set $\hat{\mathcal{G}}=\partial\mathcal{O}_f-\mathbb{R}^n,$ where $\partial\mathcal{O}_f$ is the boundary of $\mathcal{O}_f.$  The intent seems clear.  First $$\mathbb{R}^{n+1}-\mathbb{R}^n=\mathbb{R}^n\times\left(\mathbb{R}-\left\{0\right\}\right)$$ where $\times$ means Cartesian product.  Then $$\hat{\mathcal{G}}=\left(\mathbb{R}^{n+1}-\mathbb{R}^n\right)\cap\partial\mathcal{O}_f.$$ But long ago I learned that $\mathbb{R}^n$ is the set of all real number n-tuples, and $\mathbb{R}^{n+1}$ is the set of all (n+1)-tuples, so elements of $\mathbb{R}^{n}$ are not elements of $\mathbb{R}^{n+1}$ and $\mathbb{R}^{n}$ is not a subset of $\mathbb{R}^{n+1}.$ So am I correct in concluding that $\mathbb{R}^{n+1}-\mathbb{R}^n$ is not really the relative complement of the two sets?	multivariable-calculus,elementary-set-theory,vector-spaces
A.377	¿Qué es $\mathbb{R}^{n+1}-\mathbb{R}^n$?	En el cálculo avanzado de varias variables de C.H. Edwards define el conjunto ordenado $\mathcal{O}_f$ de una función $f:\mathbb{R}^n\to\mathbb{R}$ como el conjunto de puntos entre $\mathbb{R}^n$ y el gráfico de $f,$ incluyendo los puntos de evaluación, $\mathbf{x}\in\mathbb{R}^n$ y los puntos en el gráfico $\left\{\mathbf{x},f\left(\mathbf{x}\right)\right\}\in\mathbb{R}^{n+1}$. Más tarde define un conjunto $\hat{\mathcal{G}}=\partial\mathcal{O}_f-\mathbb{R}^n,$ donde $\partial\mathcal{O}_f$ es el límite de $\mathcal{O}_f.$ La intención parece clara. Primero $$\mathbb{R}^{n+1}-\mathbb{R}^n=\mathbb{R}^n\times\left(\mathbb{R}-\left\{0\right\}\right)$$ donde $\times$ significa producto cartesiano. Entonces $$\hat{\mathcal{G}}=\left(\mathbb{R}^{n+1}-\mathbb{R}^n\right)\cap\partial\mathcal{O}_f.$$ Pero hace mucho tiempo aprendí que $\mathbb{R}^n$ es el conjunto de todos los números reales n-tuples, y $\mathbb{R}^{n+1}$ es el conjunto de todos los (n+1)-tuples, por lo que los elementos de $\mathbb{R}^{n}$ no son elementos de $\mathbb{R}^{n+1}.$ y $\mathbb{R}^{n+1}.$ no es un subconjunto relativo de $\mathbb{R}^{n+1}-\mathbb{R}^n$ Así que ¿estoy correcto en concluir que $\mathbb{R}^{n+1}-\mathbb{R}^n$ no es realmente el complemento de los dos conjuntos?	multivariable-calculus,elementary-set-theory,vector-spaces
A.378	Do exponent rules follow different rules from radicals	Does $\left(-3\right)^\frac{2}{2}$ not equal $\sqrt{\left(-3\right)^2}$?	exponential-function
A.378	¿Las reglas del exponente siguen reglas diferentes de las de los radicales?	¿No es $\left(-3\right)^\frac{2}{2}$ igual a $\sqrt{\left(-3\right)^2}$?	exponential-function
A.379	Properties of a set of all isomorphisms $ f: G \to G $	I'm kinda stuck with this task. Let $G$ be a group and $ S $ the set of all isomorphisms $ f: G \to G$. I first want to show that $ (S, \circ) $ is also a group. I believe I've shown that all the properties of a group is fulfilled with $ (S, \circ) $: i) Assume that $ x \in $ and $ f_1,f_2 \in S$. Then $f_2(x) = y \in G$, since $ f_1 $ is an ismorphism. $ f_2(y) = z \in G $ since $ f_2 $ is an ismorphism. Then $ f_1(f_2(x)) = f_1(y) = z = f_1 \circ f_2(x),$ hence $f_1, f_2 \in S \longrightarrow f_1 \circ f_2 \in S.$ ii) $id$ is an isomorphism $ \longrightarrow id \in S$. iii) $ f $ is an isomorphism $ \longrightarrow \exists f^{-1}$, one can show that $ f^{-1} $ is also an isomorphism, $ \longrightarrow f^{-1} \in S$. Now, assume that $ | S | = 1$, then I want to show that $ G $ is abelian and each element has an order of 1 or 2. Im kinda lost with that $ | S | = 1 $. If $ f \in S $ then $f^{-1} \in S $ due to previous result, should not $ | S | = 1 $ imply that $ f = f^{-1} = id$? And how do I move forward from this? Any hints is muy appreciated!	group-theory,elementary-number-theory,group-isomorphism,automorphism-group
A.379	Propiedades de un conjunto de todos los isomorfismos $ f: G \to G $	Estoy un poco atascado con esta tarea. Deja que $G$ sea un grupo y $ S $ el conjunto de todos los isomorfismos $ f: G \to G$. Primero quiero mostrar que $ (S, \circ) $ es también un grupo. Creo que he demostrado que todas las propiedades de un grupo se cumplen con $ (S, \circ) $: i) Supongamos que $ x \in $ y $ f_1,f_2 \in S$. Entonces $f_2(x) = y \in G$, ya que $ f_1 $ es un ismorfismo. $ f_2(y) = z \in G $ desde $ f_2 $ es un ismorfismo. Entonces $f_1, f_2 \in S \longrightarrow f_1 \circ f_2 \in S.$ ii) $id$ es un isomorfismo $ \longrightarrow id \in S$. iii) $ f $ es un isomorfismo $ \longrightarrow \exists f^{-1}$, se puede mostrar que $ f^{-1} $ es también un isomorfismo, $ \longrightarrow f^{-1} \in S$. Ahora, supongamos que $ | S | = 1$, entonces quiero mostrar que $ G $ es abeliano y cada elemento tiene un orden de 1 o 2. Estoy un poco perdido con ese $ | S | = 1$. Si $ f \in S $ entonces $f^{-1} \in S $ debido al resultado anterior, ¿no debería $ | S | = 1 $ implicar que $ f = f^{-1} = id$? ¿Y cómo salgo de esto? ¡Cualquier sugerencia es muy apreciada!	group-theory,elementary-number-theory,group-isomorphism,automorphism-group
A.380	If $\int_0^\pi f(t) \sin(t)dt =\int_0^\pi f(t) \cos(t)dt = 0$, then $f(x)=0$ admits two solutions	Let $f\colon [0,\pi]\to\mathbb{R}$ be a continuous function.  If $\int^{\pi}_{0}f (t) \sin(t)dt =\int^{\pi}_{0} f (t) \cos(t)dt = 0$, then $f(x)=0$ admits two solutions in $[0,\pi]$  I try to show if $f(x)>0$ and then get the contradiction but I failed to prove that, so maybe can someone help me with that? thanks in advance.	integration,analysis
A.380	Si $\int_0^\pi f(t) \sin(t)dt =\int_0^\pi f(t) \cos(t)dt = 0$, entonces $f(x)=0$ admite dos soluciones	Si $f\colon [0,\pi]\to\mathbb{R}$ es una función continua. si $\int^{\pi}_{0}f (t) \sin(t)dt =\int^{\pi}_{0} f (t) \cos(t)dt = 0$, entonces $f(x)=0$ admite dos soluciones en $[0,\pi]$ intento mostrar si $f(x)>0$ y luego obtener la contradicción pero no pude probar eso, así que tal vez alguien puede ayudarme con eso? gracias por adelantado.	integration,analysis
A.381	$A_1 \times ... \times A_n$ is countable if $A_1, ..., A_n$ are countable	Suppose that $A_1, ..., A_n$ are countable sets. Show that the cartesian product $A := A_1 \times ... \times A_n$ is countable. My attempt: Sets are said to be countable if they are finite or if they have the same cardinality as some subset of $\mathbb{N}$ (i.e. we can find some bijection $f: A \rightarrow S$ or $f: S \rightarrow A$ where $S \subset \mathbb{N}$). Assume that $A_1, ..., A_n$ are countable sets. Then, there exists bijections $fi: \mathbb{N} \rightarrow A_i$ for $i = 1, ..., n$. Define $g: \mathbb{N} \rightarrow A$ as follows My issue arises here in finding such a bijective function without it being too complicated. How would I go about finding one? I am also open to any suggestions. Any assistance is welcomed.	elementary-set-theory,induction
A.381	$A_1 \times ... \times A_n$ es contable si $A_1, ..., A_n$ es contable	Supongamos que $A_1, ..., A_n$ son conjuntos contables. Muestre que el producto cartesiano $A := A_1 \times ... \times A_n$ es contable. Mi intento: Se dice que los conjuntos son contables si son finitos o si tienen la misma cardinalidad que algún subconjunto de $\mathbb{N}$ (es decir, podemos encontrar alguna bijección $f: A \rightarrow S$ o $f: S \rightarrow A$ donde $S \subset \mathbb{N}$). Supongamos que $A_1, ..., A_n$ son conjuntos contables. Luego, existen bijecciones $fi: \mathbb{N} \rightarrow A_i$ para $i = 1, ..., n$. Definir $g: \mathbb{N} \rightarrow A$ de la siguiente manera Mi problema surge aquí en encontrar tal función bijectiva sin que sea demasiado complicada. ¿Cómo voy a encontrar una? También estoy abierto a cualquier sugerencia. Cualquier ayuda es bienvenida.	elementary-set-theory,induction
A.382	Set of functions from $SS = \{ A_{1}, A_{2}, A_{3},...\}$ to $\{ \mathbf{T}, \mathbf{F} \} $ is countable?	Let $SS=\{ A_1,A_2,A_3,\ldots\}$, and let $V = \{ v \mid v: SS \to \{ \mathbf{T}, \mathbf{F} \} \}$ .Is the set  V countable?  Justify your answer. My instinct is to say that $V = \{ v \mid v: SS \to \{ \mathbf{T}, \mathbf{F} \} \}$ is uncountable, and to prove this using a diagonalization argument, i.e. create a table of values of the functions $v_{1}, v_{2},...$ for natural numbers $n_1, n_2,\ldots$ and define a function $v_{m} : SS \to \{ \mathbf{T}, \mathbf{F} \}$ where it takes all values in the diagonals of the table $T, F$ and flips them, and then show that $v_{m}$ cannot appear anywhere in the list. Is my guess correct, and if so would this be a reasonable approach to the problem?	elementary-set-theory
A.382	¿El conjunto de funciones de $SS = \{ A_{1}, A_{2}, A_{3},...\}$ a $\{ \mathbf{T}, \mathbf{F} \} $ es contable?	Deja que $SS=\{ A_1,A_2,A_3,\ldots\}$, y que $V = \{ v \mid v: SS \to \{ \mathbf{T}, \mathbf{F} \} \}$ .¿Es el conjunto V contable? Justifica tu respuesta. Mi instinto es decir que $V = \{ v \mid v: SS \to \{ \mathbf{T}, \mathbf{F} \} \}$ es incontable, y para demostrar esto usando un argumento de diagonalización, es decir, crear una tabla de valores de las funciones $v_{1}, v_{2},...$ para los números naturales $n_1, n_2,\ldots$ y definir una función $v_{m} : SS \to \{ \mathbf{T}, \mathbf{F} \}$ donde toma todos los valores en las diagonales de la tabla $T, F$ y los voltea, y luego mostrar que $v_{m}$ no puede aparecer en ninguna parte de la lista. ¿Es mi suposición correcta, y si es así, este sería un enfoque razonable al problema?	elementary-set-theory
A.383	Automorphisms of the disk without the maximum principle	For pedagogical purposes, I am looking for an elementary proof (i.e. without resorting to the maximum principle) that $f_a(z):=\frac{z-a}{1-\overline{a}z}$ maps the unit disk into itself when $|a|<1$. The usual argument (at least usual for me) is to look at $f_a(e^{it})$ and check that $|f_a(e^{it})|=1$. Then we are done by the maximum principle and the fact that $f_a(a)=0$. However, I cannot help but think that there should be an elementary way to do this, using only basic facts about complex numbers such as the triangle inequality. Unfortunately I am running into circles.	complex-analysis,complex-numbers
A.383	Automorfismos del disco sin el principio máximo	Para fines pedagógicos, estoy buscando una prueba elemental (es decir, sin recurrir al principio máximo) de que $f_a(z):=\frac{z-a}{1-\overline{a}z}$ mapea el disco unitario en sí mismo cuando $|a|<1$. El argumento habitual (al menos habitual para mí) es mirar a $f_a(e^{it})$ y comprobar que $|f_a(e^{it})|=1$. Entonces estamos concretos por el principio máximo y el hecho de que $f_a(a)=0$. Sin embargo, no puedo evitar pensar que debería haber una manera elemental de hacerlo, utilizando solo hechos básicos sobre números complejos como la desigualdad del triángulo. Desafortunadamente estoy corriendo en círculos.	complex-analysis,complex-numbers
A.384	What does this bracket notation mean?	I am currently taking MIT6.006 and I came across this problem on the problem set. Despite the fact I have learned Discrete Mathematics before, I have never seen such notation before, and I would like to know what it means and how it works, Thank you: $$ f_3(n) = \binom n2$$ (Transcribed from screenshot)	discrete-mathematics,algorithms
A.384	¿Qué significa esta notación de corchetes?	Actualmente estoy tomando MIT6.006 y me encontré con este problema en el conjunto de problemas. a pesar del hecho de que he aprendido Matemáticas Discreta antes, nunca había visto tal notación antes, y me gustaría saber lo que significa y cómo funciona, Gracias: $$ f_3(n) = \binom n2$$ (Transcripto de captura de pantalla)	discrete-mathematics,algorithms
A.385	Properties of size function in a general Euclidean domain	In ring theory a given ring $R$ is called a Euclidean domain if there exists a function $\sigma:R -\{0\}\rightarrow \{0,1,2,3...\}   $ which satisfies the division algorithm i.e. $ $ if $a,b \in R$ then there exists $q,r \in R$ such that  $b=aq+r$ and either $r=0$ or $\sigma(r)\lt \sigma(a) $ Now I want to ask if we can prove, using just this definition that an element of larger degree won't divide an element of smaller degree. In specific rings such as (i) the integers we can say$$\sigma(a)=|a|$$ $$\sigma(ab)=\sigma(a)\sigma(b)$$ ii) polynomials where$$\sigma(f(x))=deg(f(x))$$$$\sigma(ab)=\sigma(a)+\sigma(b)$$ So in both the above cases the size of product of two elements will always be greater than or equal to the size of individual elements and hence the larger element can never divide the smaller element. But is this true in general for all Euclidean domains ? And how will we prove that	ring-theory,factoring,euclidean-domain
A.385	Propiedades de la función de tamaño en un dominio euclidiano general	En la teoría de anillos un anillo dado $R$ se llama un dominio euclidiano si existe una función $\sigma:R -\{0\}\rightarrow \{0,1,2,3...\}   $ que satisface el algoritmo de división es decir, $ $ si $a,b \in R$ entonces existe $q,r \in R$ tal que $b=aq+r$ y ya sea $r=0$ o $\sigma(r)\lt \sigma(a) $ Ahora quiero preguntar si podemos probar, usando sólo esta definición que un elemento de mayor grado no dividirá un elemento de menor grado. En anillos específicos como (i) los numeros enteros podemos decir $$\sigma(a)=|a|$$ $$R$0 ii) polinomios donde $R$1$R$2 Así que en ambos casos anteriores el tamaño del producto de dos elementos siempre será mayor o igual al tamaño de los elementos individuales y por lo tanto el elemento más grande nunca puede dividir el elemento más pequeño. Pero ¿esto es cierto en general para todos los dominios euclidianos ? ¿ Y como lo probarias?	ring-theory,factoring,euclidean-domain
A.386	Prove that $\sum _{n=-\infty }^{\infty } e^{-n^2 \pi x}=\frac{1}{\sqrt{x}}\sum _{n=-\infty }^{\infty } e^{-\frac{n^2 \pi }{x}}$	In Wikipedia's proof of Riemann's functional equation for the zeta function (here, and click "Show Proof"), I find the assertion that $$\sum _{n=-\infty }^{\infty } e^{-n^2 \pi x} = \frac{1}{\sqrt{x}}\sum _{n=-\infty }^{\infty } e^{-\frac{n^2 \pi }{x}}$$ I can't work out how this works. Is it to do with Jacobi theta functions? Mathematica (which seems to use Jacobi's original notation) simplifies the expression on the left hand side above to the Jacobi elliptic theta function (Wikipedia here, plus the 'Auxiliary Functions' section that follows) $$ \begin{aligned} \sum _{n=-\infty }^{\infty } e^{-n^2 \pi x} &= \vartheta_{3}(0,e^{-\pi  x}) \\&= \vartheta_{00}(0,e^{-\pi  x}) \\&= \vartheta(0,e^{-\pi  x}) \end{aligned} $$ Wikipedia defines $$\vartheta(z;\tau) := \sum _{n=-\infty }^{\infty } e^{\pi i n^2 \tau+2 \pi i n z}$$ But setting $z = 0$ and $\tau = e^{-\pi  x}$ then gives $$\vartheta(0,e^{-\pi  x}) = \sum _{n=-\infty }^{\infty } e^{\pi i n^2 e^{-\pi  x}}$$ which is clearly not equivalent to the original expression. I suspect that this may have something to do with nomes, which Wikipedia mentions but I cannot get my head around. So, my two questions are:  How do I prove the original equivalence? What am I doing wrong in relation to the Jacobi theta function?	proof-explanation,riemann-zeta,theta-functions
A.386	Prueba que $\sum _{n=-\infty }^{\infty } e^{-n^2 \pi x}=\frac{1}{\sqrt{x}}\sum _{n=-\infty }^{\infty } e^{-\frac{n^2 \pi }{x}}$	En la prueba de Wikipedia de la ecuación funcional de Riemann para la función zeta (aquí, y haga clic en "Mostrar prueba"), encuentro la afirmación de que $$\sum _{n=-\infty }^{\infty } e^{-n^2 \pi x} = \frac{1}{\sqrt{x}}\sum _{n=-\infty }^{\infty } e^{-\frac{n^2 \pi }{x}}$$ no puedo determinar cómo funciona esto. ¿Tiene que ver con las funciones de Jacobi theta? Matemática (que parece usar la notación original de Jacobi) simplifica la expresión en el lado izquierdo superior a la función de Jacobi elíptica theta (aquí, Wikipedia, más la sección de 'Funciones Auxiliares' que sigue) $$ \begin{aligned} \sum _{n=-\infty }^{\infty } e^{-n^2 \pi x} &= \vartheta_{3}(0,e^{-\pi  x}) \\&= \vartheta_{00}(0,e^{-\pi  x}) \\&= \vartheta(0,e^{-\pi  x}) \end{aligned} $$ Wikipedia define $$\vartheta(z;\tau) := \sum _{n=-\infty }^{\infty } e^{\pi i n^2 \tau+2 \pi i n z}$$ Pero fijar $z = 0$ y $\tau = e^{-\pi  x}$ luego da $$\vartheta(0,e^{-\pi  x}) = \sum _{n=-\infty }^{\infty } e^{\pi i n^2 e^{-\pi  x}}$$ que no es equivalente a la expresión original. sospecho que esto puede tener algo que ver con nombres, que Wikipedia menciona pero no puedo obtener mi respuesta. Así que, mis dos preguntas son: ¿Cómo hago la equivalencia original? ¿Qué relación tengo claramente con la función Jacobi theta?	proof-explanation,riemann-zeta,theta-functions
A.387	Any positive integer greater than $11$ is a nonnegative linear combination of $5$ and $4$. My solution	Let $n\in\mathbb{Z}^{+}$, then there exists $k\in\mathbb{Z}_0^+$, such that $n=5k + i, i\in\{0,1,2,3,4\}$. Now analyzing by cases we have:  If $i=0$, then $\begin{align*}         n = 5k \Rightarrow n = 5k + 4(0). \end{align*}$ If $ i = 1 $, then $\begin{align*}        n & = 5k + 1 \\          & = 5k-5(3) +5(3) +1 \\          & = 5(k-3) + 15 + 1 \\          & = 5(k-3) +16 \Rightarrow n = 5(k-3) +4(4). \end{align*}$ If $ i = 2 $, then $\begin{align*}    n & = 5k + 2 \\      & = 5k-5(2) +5(2) +2 \\      & = 5(k-2) + 10 + 2 \\      & = 5(k-2) +12 \Rightarrow n = 5(k-2) +4(3). \end{align*}$ If $i=3$, then $\begin{align*}    n & = 5k + 3 \\      & = 5k-5 + 5 + 3 \\      & = 5(k-1) +8 \Rightarrow n = 5(k-1) +4(2). \end{align*}$ If $i=4$, then $\begin{align*}     n = 5k + 4 \Rightarrow n = 5k + 4(1). \end{align*}$  Thus, every positive number can be expressed as a linear combination of $5$ and $4$. Now using that $n>11$, so we have: $\begin{align*} n       &> 11 \\ 5k + i  &> 5(2) +1 \\ 5k-5(2) &> 1-i \\ 5 (k-2) &> 1-i \\ k-2     &> \frac{1-i}{5} \\ k       &> 2+\frac{1-i}{5}. \end{align*}$ So by increasing over the values ​​that $ i $ takes, we have: $\begin{align*} k &> 2+ \frac{1-i}{5} \geq 2+ \frac{1-0}{5}\\ k &> 2 + 0.2 = 2.2 \end{align*}$ But $k\in\mathbb{Z}_0^+ \Rightarrow k \geq 3 \Rightarrow n \geq 15 $. Thus we have that every positive integer greater than or equal to $15$ is a non-negative linear combination of $5$ and $4$. Finally, let's look at the cases that are still unverified, which are $12$, $13$ and $14$. $\begin{align*} 12 &= 5(0) +4(3) \\ 13 &= 5(1) +4(2) \\ 14 &= 5(2) +4(1). \end{align*}$ Therefore, any positive integer greater than $11$ is a nonnegative linear combination of $5$ and $4$. I think this is the correct solution, I await your comments. If anyone has a different solution or correction of my work I will be grateful.	elementary-number-theory
A.387	Cualquier número entero positivo mayor que $11$ es una combinación lineal no negativa de $5$ y $4$. Mi solución	Sea $n\in\mathbb{Z}^{+}$, entonces existe $k\in\mathbb{Z}_0^+$, tal que $n=5k + i, i\in\{0,1 ,2,3,4\}$. Ahora analizando por casos tenemos: Si $i=0$, entonces $\begin{align*} n = 5k \Rightarrow n = 5k + 4(0). \end{align*}$ Si $ i = 1 $, entonces $\begin{align*} n & = 5k + 1 \\ & = 5k-5(3) +5(3) +1 \\ & = 5 (k-3) + 15 + 1 \\ & = 5(k-3) +16 \Rightarrow n = 5(k-3) +4(4). \end{align*}$ Si $ i = 2 $, entonces $\begin{align*} n & = 5k + 2 \\ & = 5k-5(2) +5(2) +2 \\ & = 5 (k-2) + 10 + 2 \\ & = 5(k-2) +12 \Rightarrow n = 5(k-2) +4(3). \end{align*}$ Si $i=3$, entonces $\begin{align*} n & = 5k + 3 \\ & = 5k-5 + 5 + 3 \\ & = 5(k-1) + 8 \Flecha derecha n = 5(k-1) +4(2). \end{align*}$ Si $i=4$, entonces $\begin{align*} n = 5k + 4 \Rightarrow n = 5k + 4(1). \end{align*}$ Por lo tanto, cada número positivo se puede expresar como una combinación lineal de $5$ y $4$. Ahora usando $n>11$, entonces tenemos: $\begin{align*} n &> 11 \\ 5k + i &> 5(2) +1 \\ 5k-5(2) &> 1-i \\ 5 (k-2) &> 1-i \\ k-2 &> \frac{1-i}{5} \\ k &> 2+\frac{1-i}{5}. \end{align*}$ Entonces, al aumentar sobre los valores que toma $ i $, tenemos: $\begin{align*} k &> 2+ \frac{1-i}{5} \geq 2+ \frac{1-0}{5}\\ k &> 2 + 0.2 = 2.2 \end{align*}$ Pero $k\in\mathbb{Z}_0^+ \Rightarrow k \geq 3 \Rightarrow n \ geq 15 $. Por lo tanto, tenemos que cada entero positivo mayor o igual a $15$ es una combinación lineal no negativa de $5$ y $4$. Finalmente, veamos los casos que aún no están verificados, que son $12$, $13$ y $14$. $\begin{align*} 12 &= 5(0) +4(3) \\ 13 &= 5(1) +4(2) \\ 14 &= 5(2) +4(1). \end{align*}$ Por lo tanto, cualquier entero positivo mayor que $11$ es una combinación lineal no negativa de $5$ y $4$. Creo que esta es la solución correcta, espero sus comentarios. Si alguien tiene una solución diferente o corrección de mi trabajo se lo agradeceré.	elementary-number-theory
A.388	Linear algebra find $k$	Given the linear system: $$\begin{cases}  x_1 + kx_2 − x_3 = 2\\  2x_1 − x_2 + kx_3 = 5 \\ x_1 +10x_2 −6x_3 =1 \end{cases}$$ for which values of $k$ has the system (2): (a) No solutions (b) A unique solution. (c) Infinitely many solutions. I've been trying echelon form where i switched $R_1$ with $R_3$ and then i switched $R_2$ with $R_3$ So I have $\left[\begin{array}{ccc|c}1&10&-6&1\\1&k&-1&2\\2&-1&k&5\end{array}\right]$ but then I'm stuck and don't know how to get any further.	linear-algebra
A.388	Algebra lineal encuentra $k$	Dado el sistema lineal: $$\begin{cases}  x_1 + kx_2 − x_3 = 2\\  2x_1 − x_2 + kx_3 = 5 \\ x_1 +10x_2 −6x_3 =1 \end{cases}$$ para el cual los valores de $k$ tiene el sistema (2): (a) No hay soluciones (b) Una solución única. (c) Infinitamente muchas soluciones. He estado intentando la forma de escala donde cambié $R_1$ con $R_3$ y luego cambié $R_2$ con $R_3$ Así que tengo $\left[\begin{array}{ccc|c}1&10&-6&1\\1&k&-1&2\\2&-1&k&5\end{array}\right]$ pero luego estoy atascado y no sé cómo seguir adelante.	linear-algebra
A.389	Convergence of a Special Series as N is large	I'm trying to find a general formula for the series and x is a constant: $$\sum\limits_{i=1}^N  \frac{i}{(1+r)^i}$$ I have deduced the general formula for the sum. $$\frac{(1+r)^{N+1}-(1+r)-rN}{r^2(1+r)^N}$$ Will this sum converge to some value when N is very large? Could someone explain how to deal with it?	summation
A.389	Convergencia de una serie especial como N es grande	Estoy tratando de encontrar una fórmula general para la serie y x es una constante: $$\sum\limits_{i=1}^N  \frac{i}{(1+r)^i}$$ He deducido la fórmula general para la suma. $$\frac{(1+r)^{N+1}-(1+r)-rN}{r^2(1+r)^N}$$ ¿Convergirá esta suma a algún valor cuando N es muy grande? ¿Puede alguien explicar cómo lidiar con él?	summation
A.390	Prove that if $x_n$ converges to $\omega$, $t_n$ converges to $\omega$ too	By the sequence $(x_n)_{n\in\Bbb{N}}$, define a new sequence $(t_n)_{n\in\Bbb{N}}$ such that $t_n:=\frac {x_1+x_2+...+x_n}{n}$. If $\lim_{n\rightarrow\infty}t_n=\omega$, how can I show that $\lim_{t\rightarrow\infty}x_n=\omega$? Original post had ``If $\lim_{n\rightarrow\infty}x_n=\omega$, ..."	sequences-and-series,convergence-divergence
A.390	Demostrar que si $x_n$ converge a $\omega$, $t_n$ converge a $\omega$ también	Por la secuencia $(x_n)_{n\in\Bbb{N}}$, definir una nueva secuencia $(t_n)_{n\in\Bbb{N}}$ tal que $t_n:=\frac {x_1+x_2+...+x_n}{n}$. Si $\lim_{n\rightarrow\infty}t_n=\omega$, ¿cómo puedo mostrar que $\lim_{t\rightarrow\infty}x_n=\omega$? El post original tenia ``If $\lim_{n\rightarrow\infty}x_n=\omega$, ..."	sequences-and-series,convergence-divergence
A.391	$C_r $ inequality	Show that for each $r> 0$ $$\mathbb{E} |X+Y|^r \leq c_r (\mathbb{E} |X|^r + \mathbb{E} |Y|^r),$$ where $c_r$ is a constant given by $\begin{equation}    c_r = \left\{         \begin{array}{ll}    1 & \mathrm{if\ } 0 < r \le 1 \\    2^{r-1}     & \mathrm{if\ } 1 < r         \end{array}       \right. \end{equation}$ I've tried to use other inequalities for the proof of this one but I still get stuck for the case of $2^{r-1}$.	probability-theory,probability-distributions
A.391	$C_r $ desigualdad	Muestre que para cada $r> 0$ $$\mathbb{E} |X+Y|^r \leq c_r (\mathbb{E} |X|^r + \mathbb{E} |Y|^r),$$ donde $c_r$ es una constante dada por $\begin{equation}    c_r = \left\{         \begin{array}{ll}    1 & \mathrm{if\ } 0 < r \le 1 \\    2^{r-1}     & \mathrm{if\ } 1 < r         \end{array}       \right. \end{equation}$ he intentado usar otras desigualdades para la prueba de este pero todavía me quedo atascado para el caso de $2^{r-1}$.	probability-theory,probability-distributions
A.392	How to compute inverse polynomials modulo an integer	I am working with polynomials in the ring $\mathbb{Z}[X]/(X^n-1)$, so only polynomials with degree at most $n-1$ are allowed, and multiplications must be reduced modulo $X^n-1$. The thing is that I have a polynomial $f(X)$ and I want to compute its inverse modulo an integer $p$, $f_p(X)$, such that $$ f * f_p \equiv 1 \bmod p $$ How could I do that? Any known algorithm? I have heard about the extended Euclidean algorithm, but I'm not quite sure how to use it, for example, given $f(X) = -1+X+X^2-X^4+X^6+X^9-X^{10}$.	polynomials,ring-theory,euclidean-algorithm
A.392	Cómo calcular polinomios inversos modulo un número entero	Estoy trabajando con polinomios en el anillo $\mathbb{Z}[X]/(X^n-1)$, por lo que sólo se permiten polinomios con grado máximo $n-1$, y las multiplicaciones deben reducirse modulo $X^n-1$. La cosa es que tengo un polinomio $f(X)$ y quiero calcular su modulo inverso un número entero $p$, $f_p(X)$, de tal manera que $$ f * f_p \equiv 1 \bmod p $$ ¿Cómo podría hacer eso? Cualquier algoritmo conocido? He oído hablar del algoritmo extendido de Euclides, pero no estoy muy seguro de cómo usarlo, por ejemplo, dado $f(X) = -1+X+X^2-X^4+X^6+X^9-X^{10}$.	polynomials,ring-theory,euclidean-algorithm
A.393	Prove an entire function is a constant under an inequality	f is an entire function, suppose $|f(z^{2})| \leq  2|f(z)|$ for all C, then f is a constant. I 'm trying to use Liouville's theorem, but it seems that it isn't helpful.	complex-analysis
A.393	Prueba que una función entera es una constante bajo una desigualdad	f es una función entera, supongamos que $|f(z^{2})| \leq  2|f(z)|$ para todo C, entonces f es una constante. Estoy tratando de usar el teorema de Liouville, pero parece que no es útil.	complex-analysis
A.394	Trying to find the $\delta$ in epsilon-delta continuity proof.	I am trying to prove the following function is continuous for all irrationals: $f(x) = \begin{cases} 0,  & \text{if $x$ is irrational} \\ 1/n, & \text{if $x = m/n$} \end{cases}$ The question assumes $m/n$ is in lowest terms. I have shown that it is discontinuous for all rationals, and now I believe I have to either use the $\epsilon-\delta$ definition of continuity or sequential continuity to show the function is continuous for when $x$ is irrational. I split my attempt into two cases: Our value of $x$ is irrational, then I want: $$\forall \epsilon > 0, \exists \delta > 0, |x-a| $a$ is irrational here. Using that $x$ is irrational I get that $f(x) = 0$ as does $f(a)$ so no matter the $\delta$ we have our condition for continuity  satisfied as $0 < \epsilon$ for all $\delta$ Our value of $x$ is rational i.e. $x = \frac{m}{n}$ subbing in we want: $$\forall \epsilon > 0, \exists \delta > 0, |\frac{m}{n}-a| I am struggling to find the $\delta$ necessary. I am able to bound $|\frac{m}{n}-a|$ by $1+2|a|$ if I say that $\delta \le 1$. However I do not know how to find a $\delta$ to yield the second inequality. Should I change my approach to sequential continuity?	continuity,epsilon-delta
A.394	Intentando encontrar el $\delta$ en la prueba de continuidad epsilon-delta.	Estoy tratando de demostrar que la siguiente función es continua para todos los irracionales: $f(x) = \begin{cases} 0,  & \text{if $x$ is irrational} \\ 1/n, & \text{if $x = m/n$} \end{cases}$ La pregunta asume que $m/n$ es en términos más bajos. He demostrado que es discontinuo para todos los racionales, y ahora creo que tengo que usar la definición $\epsilon-\delta$ de continuidad o continuidad secuencial para mostrar que la función es continua cuando $x$ es irracional. Divido mi intento en dos casos: Nuestro valor de $x$ es irracional, entonces quiero: $$\forall \epsilon > 0, \exists \delta > 0, |x-a| $a$ es irracional aqui. Usando $x$ como irracional consigo $f(x) = 0$ al igual que $f(a)$ asi que no importa que $\delta$ tenemos una condicion de continuidad satisfacida como $0 < \epsilon$ para toda $\delta$ Nuestro valor de $x$ es racional i.e. $x = \frac{m}{n}$ substituyendo queremos: $$\forall \epsilon > 0, \exists \delta > 0, |\frac{m}{n}-a| Tengo problemas consiguiendo el $\delta$ necesario. Soy capaz de unir $|\frac{m}{n}-a|$ por $1+2|a|$ si digo que $\delta \le 1$. Sin embargo no se como encontre $\delta$ para producir la segunda desigualdad ¿Deberia cambiar mi enfoque hacia la continuidad secuncial.?	continuity,epsilon-delta
A.395	Why we have to check both additivity and homogenity for linearity?	$f: V \to W $ over $K$ with $a,b \in V$ and $k \in K$. Additivity: $f(a+b) = f(a) + f(b)$ Homogenity: $k*f(b) =f(k * b)$ I have a visual understanding that a function is linear if the structure is kept while projecting it with $f$ but why it is not enough to check if the function is additive? I would be glad to have some easy examples and an intuition why we would have to check both conditions.	linear-algebra
A.395	¿Por qué tenemos que comprobar tanto la aditividad como la homogeneidad para la linealidad?	$f: V \to W $ sobre $K$ con $a,b \in V$ y $k \in K$. Adictividad: $f(a+b) = f(a) + f(b)$ Homogeneidad: $k*f(b) =f(k * b)$ Tengo una comprensión visual de que una función es lineal si se mantiene la estructura mientras se proyecta con $f$ pero por qué no es suficiente para comprobar si la función es aditiva? Me encantaría tener algunos ejemplos fáciles e intuición por qué deberíamos comprobar ambas condiciones.	linear-algebra
A.396	What is the MLE $\theta^*$ of $\theta$?	I have that $x_1, x_2,...,x_n$ are from a rv $X$ that has the density function $f_X(x)=\frac{2x}{\theta^2} \quad$ for $0 \le x \le \theta  \quad$ and $f_X(x)=0 \quad$ otherwise. Ihave to determine the MLE of $\theta^*$ of $\theta$  Here is how I have done it:  $L(\theta)= \frac{2}{\theta^{2n}}\prod_{i=1}^nx_i$  $\frac{\partial L(\theta)}{\partial \theta} =...=\frac{-4n}{\theta^{2n+1}}\prod_{i=1}^nx_i + \frac{2}{\theta^{2n}}\frac{\partial(\prod_{i=1}^nx_i)}{\partial \theta}$  Is this correct? and also how do I calculate the CDF $F_{\theta^*}$, the pdf $f_{\theta^*}$ and the expectation $E[\theta^*]$ of the maximum likelihood estimator $\theta^*$?	statistics,statistical-inference,maximum-likelihood,cumulative-distribution-functions,robust-statistics
A.396	¿Cuál es el MLE $\theta^*$ de $\theta$?	Tengo que $x_1, x_2,...,x_n$ son de un rv $X$ que tiene la función de densidad $f_X(x)=\frac{2x}{\theta^2} \quad$ para $0 \le x \le \theta  \quad$ y $f_X(x)=0 \quad$ de lo contrario. Tengo que determinar el MLE de $\theta^*$ de $\theta$ Aquí es como lo he hecho: $L(\theta)= \frac{2}{\theta^{2n}}\prod_{i=1}^nx_i$ $\frac{\partial L(\theta)}{\partial \theta} =...=\frac{-4n}{\theta^{2n+1}}\prod_{i=1}^nx_i + \frac{2}{\theta^{2n}}\frac{\partial(\prod_{i=1}^nx_i)}{\partial \theta}$ ¿Es esto correcto? y también cómo calcular el CDF $x_1, x_2,...,x_n$0, el pdf $x_1, x_2,...,x_n$1 y la expectativa $x_1, x_2,...,x_n$2 del estimador de probabilidad máxima $\theta^*$?	statistics,statistical-inference,maximum-likelihood,cumulative-distribution-functions,robust-statistics
A.397	If the limit does not converge, can the sum? Or, how could the sum converge?	Alright, I thought I had seen everything but last night I saw this identity (`twas attributed to Ramanujan), $$ 1 + 2 + 3 + 4 + \cdots = -\frac{1}{12} $$ Then I saw a proof that was seemingly correct. So alright, I believe it, hey it is no crazier than having infinities of different sizes and I finally have some closure with that fact. But then, I recalled the benchmark induction proof everyone learns, $$ \sum_{i=1}^{n} i = \frac{n(n+1)}{2} $$ Then kicks in the remains of all those calculus courses I once took, making me thing that, hey wait! We have this, $$ \lim_{n\rightarrow\infty} \frac{n(n+1)}{2} = \infty $$ I think in this case we said the limit does not exist or the function diverges (correct me if I am wrong!) But... but... according to the identity above, $$ \sum_{i=1}^{n=\infty} i = -\frac{1}{12} $$ But then shouldn't, $$ \lim_{n\rightarrow\infty} \frac{n(n+1)}{2} \stackrel{?}{=} -\frac{1}{12} $$ So what I am seeing here is that even if the limit does not converge, the sum does. Also, a long time ago I remember being told that the sum of two positive integers is always positive. Furthermore, addition is suppose to be closed under integers right? Here we not only have a negative number as a result of the sum of positive integers but a negative non-integer at that.	limits,summation
A.397	Si el límite no converge, ¿puede la suma? ¿o, como podria la suma converger?	Bueno, pensé que había visto todo pero anoche vi esta identidad (`twas atribuido a Ramanujan), $$ 1 + 2 + 3 + 4 + \cdots = -\frac{1}{12} $$ Entonces vi una prueba que era aparentemente correcta. Así que bien, lo creo, hey no es más loco que tener infinidades de diferentes tamaños y finalmente tengo algún cierre con ese hecho. Pero entonces, recordé la prueba de inducción de referencia que todos aprenden, $$ \sum_{i=1}^{n} i = \frac{n(n+1)}{2} $$ Entonces patea en los restos de todos esos cursos de cálculo que una vez tomé, haciendo que yo suponga que, hey espera! Tenemos esto, $$ \lim_{n\rightarrow\infty} \frac{n(n+1)}{2} = \infty $$ Creo que en este caso dijimos que el límite no existe o la función diverge (corrige si estoy equivocado!) Pero... pero... según la identidad anterior, $$ \sum_{i=1}^{n=\infty} i = -\frac{1}{12} $$ Pero entonces lo que veo aquí es que incluso si el límite no converge, la suma si lo hace. También, si hace mucho tiempo, recuerdo que sólo una suma de dos números integrales negativos es un resultado positivo, pero además, se supone que la suma debe cerrarse bajo números enteros, ¿verdad?. Aquí no solo tenemos un número negativo como resultado de la suma de números enteros positivos, sino también un número no entero negativo.	limits,summation
A.398	A question on differentiability at a point	Is a continuous function differentiable at $x=c$ if the limit of its derivative has a value at that point? That is, if $$\lim_{x \rightarrow c} f'(x) = L = \lim_{x \rightarrow c^+} f'(x) =\lim_{x \rightarrow c^-} f'(x)$$ Intuitively, the slopes of the tangents approach the same value and since the function is continuous a jump-discontinuity isn't possible so it appears the slope at the point should be $L$ too. However, I cannot seem to locate such a theorem, so I suspect my intuition is wrong. Is it?	real-analysis,calculus
A.398	Una pregunta sobre la diferenciabilidad en un punto	¿Es una función continua diferenciable en $x=c$ si el límite de su derivada tiene un valor en ese punto? Es decir, si $$\lim_{x \rightarrow c} f'(x) = L = \lim_{x \rightarrow c^+} f'(x) =\lim_{x \rightarrow c^-} f'(x)$$ Intuitivamente, las pendientes de las tangentes se acercan al mismo valor y ya que la función es continua una descontinuidad de salto no es posible por lo que parece que la pendiente en el punto debe ser $L$ también. Sin embargo, no puedo encontrar tal teorema, así que sospecho que mi intuición está equivocada. ¿Es así?	real-analysis,calculus
A.399	Disjoint axis-aligned rectangles in the plane	Let $A$ be some set of axis-aligned rectangles in the plane, each pair of which has empty intersection. Prove that $A$ is a countable set. (An axis-aligned rectangle is a set of the form $$M = {\{\langle x,y \rangle \in \mathbb{R^2} | a \leq x \leq b , c \leq y \leq d}\}$$ for $a,b,c,d$ such that $a < b$ and $ c < d$.) Attempt: I tried using the density of the $\mathbb{Q}$ in $(\mathbb{R},\leq)$, but without any success.	real-analysis,combinatorics
A.399	Rectángulos desarticulados alineados en el eje del avión	Si el rectángulo $A$ es un conjunto de rectángulos alineados con el eje en el plano, cada par de los cuales tiene una intersección vacía, demuestra que el $A$ es un conjunto contable. (Un rectángulo alineado con el eje es un conjunto de la forma $$M = {\{\langle x,y \rangle \in \mathbb{R^2} | a \leq x \leq b , c \leq y \leq d}\}$$ para el $a,b,c,d$ tal que el $a < b$ y el $ c < d$.) Intento: intenté usar la densidad del $\mathbb{Q}$ en el $(\mathbb{R},\leq)$, pero sin éxito.	real-analysis,combinatorics
A.400	What is the probability hat two particular players verse in Wimbledon if it begins with $16$ players?	Sixteen people play in the quarter-finals at Wimbledon. The winner of the quarter-finals play again in the semi-final to decide who enters the finals. What is the probability that two particular people will play each other if the tournament begins with 16 players? So I have so far (case 1 + case 2) = verse player in quarter OR verse player in semi = $\frac{1}{15} + \frac{14}{15}...$ I'm not sure what else to include in the second case Also the third part of the question asks What is the probability when $2^n$ players begin? The worked solutions show $\frac{1}{2^n-1}+\frac{2^n-2}{2^n-1}\left(\frac{1}{2^{n-2}}\right)^2\:=\:\frac{1}{2^{n-1}}$ which I cannot get. Even symbolab doesn't show the same simplification. As with the previous question, I do not understand the $\left(\frac{1}{2^{n-2}}\right)^2$	probability,combinatorics,algebra-precalculus
A.400	¿Cuál es la probabilidad de que dos jugadores particulares vayan a Wimbledon si comienza con $16$ jugadores?	Seis personas juegan en los cuartos de final en Wimbledon. El ganador de los cuartos de final juega de nuevo en la semifinales para decidir quién entra en las finales. ¿Cuál es la probabilidad de que dos personas particulares jueguen entre sí si el torneo comienza con 16 jugadores? Así que hasta ahora tengo (casos 1 + caso 2) = jugador de verso en el cuartos O jugador de verso en el semifinales = $\frac{1}{15} + \frac{14}{15}...$ No estoy seguro de qué más incluir en el segundo caso También la tercera parte de la pregunta pregunta ¿Cuál es la probabilidad cuando comienzan los jugadores $2^n$? Las soluciones trabajadas muestran $\frac{1}{2^n-1}+\frac{2^n-2}{2^n-1}\left(\frac{1}{2^{n-2}}\right)^2\:=\:\frac{1}{2^{n-1}}$ que no puedo obtener. Incluso simbólab no muestra la misma simplificación. Como con la pregunta anterior, no entiendo el $\left(\frac{1}{2^{n-2}}\right)^2$	probability,combinatorics,algebra-precalculus